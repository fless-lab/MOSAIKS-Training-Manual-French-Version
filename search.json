[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manuel de Formation MOSAIKS",
    "section": "",
    "text": "Bienvenue\nBienvenue dans la première édition du Manuel de Formation MOSAIKS ! Ce manuel constitue une référence complète pour comprendre MOSAIKS, ses capacités et son application pratique. Vous découvrirez ce qu’est MOSAIKS, ses possibilités et comment l’utiliser efficacement dans diverses applications.\nLes compétences et connaissances acquises grâce à ce manuel vous permettront d’exploiter l’imagerie satellite et l’apprentissage automatique pour répondre à des défis socio-économiques et environnementaux complexes. Ce manuel peut être utilisé en auto-formation ou dans le cadre d’une formation encadrée.\nDe nombreux concepts et exemples s’appliquent largement au domaine de la télédétection et de l’apprentissage automatique. Ainsi, même si vous n’utilisez pas MOSAIKS, vous trouverez certainement ce contenu utile.",
    "crumbs": [
      "Bienvenue"
    ]
  },
  {
    "objectID": "index.html#quest-ce-que-mosaiks",
    "href": "index.html#quest-ce-que-mosaiks",
    "title": "Manuel de Formation MOSAIKS",
    "section": "Qu’est-ce que MOSAIKS ?",
    "text": "Qu’est-ce que MOSAIKS ?\nMOSAIKS signifie Multi-task Observation using SAtellite Imagery & Kitchen Sinks. C’est un cadre conçu pour simplifier l’utilisation de l’imagerie satellite et de l’apprentissage automatique dans la prédiction de résultats socio-économiques et environnementaux à travers différents contextes géographiques et périodes. MOSAIKS s’appuie sur des convolutions aléatoires (développées par Rahimi et Recht (2008)) appliquées à l’imagerie satellite, qui extraient des caractéristiques indépendantes des tâches et permettent aux chercheurs et praticiens de prédire facilement et de manière flexible divers résultats à partir d’images brutes.\n\n\n\n\n\n\nFigure 1: MOSAIKS écrit avec des images du catalogue de données de la constellation de satellites Landsat. Créé avec : Your Name in Landsat",
    "crumbs": [
      "Bienvenue"
    ]
  },
  {
    "objectID": "index.html#à-qui-sadresse-cette-formation",
    "href": "index.html#à-qui-sadresse-cette-formation",
    "title": "Manuel de Formation MOSAIKS",
    "section": "À qui s’adresse cette formation ?",
    "text": "À qui s’adresse cette formation ?\nCe programme complet de deux semaines est conçu pour les universitaires, les professionnels et les praticiens souhaitant utiliser MOSAIKS pour mieux comprendre les enjeux socio-économiques et environnementaux. La formation est particulièrement pertinente pour les personnes travaillant dans :\n\nLa télédétection et l’analyse d’images satellites\nLes applications d’apprentissage automatique avec des données géospatiales\nLe suivi et l’évaluation agricole et environnementale\nLa recherche en développement et l’élaboration de politiques",
    "crumbs": [
      "Bienvenue"
    ]
  },
  {
    "objectID": "index.html#que-vais-je-apprendre",
    "href": "index.html#que-vais-je-apprendre",
    "title": "Manuel de Formation MOSAIKS",
    "section": "Que vais-je apprendre ?",
    "text": "Que vais-je apprendre ?\nTout au long de cette formation, vous apprendrez des applications pratiques à travers :\n\nLes fondements théoriques et la compréhension conceptuelle\nDes exercices pratiques avec des données réelles\nLes meilleures pratiques de mise en œuvre\nLes stratégies d’analyse et d’interprétation des résultats\n\nLe programme couvre l’ensemble du flux de travail MOSAIKS, notamment :\n\nL’accès et le traitement des images satellites\nLa compréhension de l’extraction des caractéristiques MOSAIKS\nL’utilisation de l’API MOSAIKS\nL’implémentation des modèles d’apprentissage automatique\nLa quantification et la communication de l’incertitude\nL’application des modèles dans différents contextes\nLe travail avec les données d’enquête\n\nQue vous débutiez avec MOSAIKS ou que vous souhaitiez approfondir votre expertise, cette formation vous fournit les outils et les connaissances nécessaires pour utiliser efficacement ce cadre.",
    "crumbs": [
      "Bienvenue"
    ]
  },
  {
    "objectID": "index.html#structure-du-cours",
    "href": "index.html#structure-du-cours",
    "title": "Manuel de Formation MOSAIKS",
    "section": "Structure du cours",
    "text": "Structure du cours\nCette formation est conçue comme un programme intensif de deux semaines combinant cours magistraux, démonstrations et sessions pratiques. Chaque journée est structurée comme suit :\n\n\n\n\n\n\nHoraire\nActivité\n\n\n\n\n9h00 - 10h30\nSession matinale 1\n\n\n10h30 - 11h00\nPause\n\n\n11h00 - 12h30\nSession matinale 2\n\n\n12h30 - 13h30\nDéjeuner\n\n\n13h30 - 15h00\nSession après-midi 1\n\n\n15h00 - 15h30\nPause\n\n\n15h30 - 16h30\nSession après-midi 2\n\n\n16h30 - 17h00\nRetour d’expérience\n\n\n\n\n\nTable 1: Planning quotidien montrant 2 sessions le matin, 2 sessions l’après-midi, et un créneau final dédié au développement du cours.\n\n\n\nChaque journée se termine par une session de questions-réponses et de retours d’expérience de 16h30 à 17h00, offrant l’opportunité de clarifier les concepts et de partager des idées. Cette première édition du cours devrait faire émerger de nombreuses nouvelles idées et concepts à inclure dans les futures formations. N’oubliez pas de prendre des notes tout au long de la journée, en particulier sur les points qui pourraient être mieux expliqués ou qui nécessiteraient des sujets supplémentaires.\nBien que la version présentielle de ce cours soit conçue selon le planning ci-dessus, les utilisateurs en auto-formation peuvent parcourir le contenu à leur rythme, en notant que les chapitres sont conçus pour un apprentissage séquentiel.",
    "crumbs": [
      "Bienvenue"
    ]
  },
  {
    "objectID": "index.html#programme-du-cours",
    "href": "index.html#programme-du-cours",
    "title": "Manuel de Formation MOSAIKS",
    "section": "Programme du cours",
    "text": "Programme du cours\n\nSemaine 1\n\nJour 1 : Cadre MOSAIKS, accès aux caractéristiques, flux de travail de base\nJour 2 : Compréhension des données de terrain, nettoyage des données, résolution spatiale\nJour 3 : Prédiction des rendements agricoles, téléchargement des caractéristiques via l’API\nJour 4 : Types d’imagerie satellite, considérations de traitement, contrôle qualité\nJour 5 : Calcul des caractéristiques, théorie des RCF, mise en œuvre pratique\n\n\n\nSemaine 2\n\nJour 1 : Sélection de modèles, réglage des hyperparamètres, validation croisée\nJour 2 : Analyse des erreurs, intervalles de confiance, communication de l’incertitude\nJour 3 : Principes de conception d’enquêtes, méthodes de géolocalisation, stratégies d’échantillonnage\nJour 4 : Sujets avancés, applications émergentes, développements futurs\nJour 5 : Présentations des projets et perspectives",
    "crumbs": [
      "Bienvenue"
    ]
  },
  {
    "objectID": "index.html#attentes-de-la-formation",
    "href": "index.html#attentes-de-la-formation",
    "title": "Manuel de Formation MOSAIKS",
    "section": "Attentes de la formation",
    "text": "Attentes de la formation\n\nCe que vous apprendrez\n\nCompréhension du cadre et des capacités de MOSAIKS\nCompétences pratiques en traitement d’images satellites\nExpérience avec les applications d’apprentissage automatique\nPratique avec des jeux de données réels\nConnaissance de l’intégration des données d’enquête\nMeilleures pratiques pour l’implémentation des modèles\n\n\n\nPrérequis\nIl n’y a pas de prérequis explicites, bien que ce cours couvre des sujets avancés en :\n\nLangage de programmation Python\nApprentissage automatique\nDonnées géospatiales\n\n\n\nAttentes des participants\n\nParticipation active aux discussions et sessions pratiques\nRéalisation des devoirs assignés (particulièrement le devoir du vendredi de la semaine 1)\nEngagement dans les sessions de questions-réponses\nContribution aux sessions de retour d’expérience pour l’amélioration du cours\n\n\n\nExigences informatiques\nLe cours comprend des sessions pratiques. Vous aurez besoin de :\n\nUn ordinateur avec accès à Internet\nUn compte Google\nAccès à Google Colaboratory\nAccès aux données nécessaires (détails fournis ultérieurement)\n\n\n\nDevoirs et présentations\nIl y aura un devoir à la fin de la semaine 1, que les participants présenteront le mardi de la semaine 2. Ce devoir est conçu pour renforcer l’apprentissage et fournir une expérience pratique avec les outils MOSAIKS.",
    "crumbs": [
      "Bienvenue"
    ]
  },
  {
    "objectID": "index.html#structure-et-contenu-du-manuel",
    "href": "index.html#structure-et-contenu-du-manuel",
    "title": "Manuel de Formation MOSAIKS",
    "section": "Structure et contenu du manuel",
    "text": "Structure et contenu du manuel\nCe manuel est organisé en six parties principales, chacune se concentrant sur un aspect critique de MOSAIKS. Nous commençons par les concepts fondamentaux et progressons graduellement vers des sujets plus avancés en modélisation et quantification de l’incertitude.\n\n\n\n\n\n\n\n\n\n\nPartie\nDescription\n\n\n\n\nIntroduction\nConfiguration informatique, aperçu de MOSAIKS, accès API, démonstration initiale\n\n\nDonnées d’étiquettes\nCompréhension des étiquettes appropriées, intégration d’enquêtes, préparation des données\n\n\nImagerie satellite\nSélection d’images appropriées, considérations de traitement\n\n\nCaractéristiques\nCaractéristiques convolutives aléatoires, accès API, calcul des caractéristiques\n\n\nModélisation\nSélection de modèles, analyse spatiale, considérations temporelles\n\n\nIncertitude\nQuantification de l’incertitude, considérations éthiques\n\n\n\n\n\nTable 2: Aperçu du contenu du Manuel de Formation MOSAIKS\n\n\n\nLe contenu est conçu pour être à la fois complet et pratique, chaque partie s’appuyant sur les concepts précédents tout en restant relativement autonome. Cette structure permet aux lecteurs soit de progresser séquentiellement à travers le manuel, soit de se concentrer sur des sujets spécifiques d’intérêt.",
    "crumbs": [
      "Bienvenue"
    ]
  },
  {
    "objectID": "index.html#remerciements",
    "href": "index.html#remerciements",
    "title": "Manuel de Formation MOSAIKS",
    "section": "Remerciements",
    "text": "Remerciements\nMOSAIKS a été développé et est soutenu par une grande équipe de chercheurs à travers plusieurs organisations partenaires :\nÉquipe de développement :\nBenjamin Recht, Cullen Molitor, Darin Christensen, Esther Rolf, Eugenio Noda, Grace Lewin, Graeme Blair, Hannah Druckenmiller, Hikari Murayama, Ian Bolliger, Jean Tseng, Jessica Katz, Jonathan Proctor, Juliet Cohen, Karena Yan, Luke Sherman, Miyabi Ishihara, Shopnavo Biswas, Simon Greenhill, Solomon Hsiang, Steven Cognac, Tamma Carleton, Taryn Fransen, Trinetta Chong, Vaishaal Shankar\nÉquipe du Manuel de Formation MOSAIKS : Cullen Molitor, Tamma Carleton, Esther Rolf, Sean Luna McAdams, Heather Lahr\nVersion française : Abdou-Raouf ATARMLA\nOrganisations partenaires :\n\nCenter for Effective Global Action (CEGA; UCB)\nEnvironmental Markets Lab (emLab; UCSB)\nGlobal Policy Lab (GPL; Stanford University)\nProject on Resources and Governance (PRG; UCLA)\nMaster of Environmental Data Science Program (MEDS; UCSB)\n\nSoutien financier :\n\nThe Patrick J. McGovern Foundation\nThe Fund for Innovation in Development (FID)\nThe United States Agency for International Development (USAID)\nUnited Nations Development Programme (UNDP)\n\nNous sommes reconnaissants du soutien et des contributions de tous les membres de l’équipe et des organisations partenaires qui ont fait de MOSAIKS une réalité. Nous espérons continuer à développer le cadre et ses applications pour répondre aux défis mondiaux pressants.\n\n\n\n\n\n\nPerspectives\n\n\n\nDans la première partie de ce manuel, nous aborderons les bases de MOSAIKS, notamment son cadre, ses capacités et ses applications pratiques. Cette section se concentre sur l’exploration de la publication originale de MOSAIKS (Rolf et al. 2021) et la compréhension des concepts fondamentaux du cadre.",
    "crumbs": [
      "Bienvenue"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html",
    "href": "part-00-intro/00-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Aperçu\nCette section présente les concepts fondamentaux de MOSAIKS (Multi-task Observation using Satellite Imagery & Kitchen Sinks) et fournit des conseils pratiques pour démarrer avec le système. Que vous soyez novice en analyse d’imagerie satellite ou praticien expérimenté, la compréhension de ces fondamentaux est cruciale pour utiliser efficacement MOSAIKS dans votre travail.\nMOSAIKS comble le fossé entre le vaste potentiel de l’imagerie satellite et les applications pratiques en fournissant :",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html#aperçu",
    "href": "part-00-intro/00-intro.html#aperçu",
    "title": "Introduction",
    "section": "",
    "text": "Des outils d’apprentissage automatique accessibles aux non-experts\nDes prédictions efficaces en termes de calcul\nDes applications flexibles pour diverses tâches\nDes solutions évolutives pour les défis mondiaux",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html#quand-utiliser-mosaiks",
    "href": "part-00-intro/00-intro.html#quand-utiliser-mosaiks",
    "title": "Introduction",
    "section": "Quand utiliser MOSAIKS",
    "text": "Quand utiliser MOSAIKS\nMOSAIKS est particulièrement utile lorsque vous devez :\n\nGénérer des prédictions sur de vastes zones géographiques\nTravailler avec des ressources informatiques limitées\nAnalyser plusieurs résultats en utilisant la même imagerie\nCréer des prédictions sans expertise en apprentissage profond\nÉtendre l’analyse du niveau local au niveau mondial\n\nCependant, MOSAIKS peut ne pas être le meilleur choix lorsque :\n\nVous avez besoin de prédictions à une résolution inférieure au kilomètre\nVotre objectif nécessite des bandes spectrales spécifiques\nDes prédictions en temps réel sont essentielles\nVotre application nécessite des caractéristiques interprétables\n\n\n\n\n\n\n\nIl existe peut-être un outil existant qui répond à vos besoins. MOSAIKS n’est pas le meilleur choix pour toutes les applications.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/00-intro.html#plan-de-la-section",
    "href": "part-00-intro/00-intro.html#plan-de-la-section",
    "title": "Introduction",
    "section": "Plan de la section",
    "text": "Plan de la section\nLes chapitres suivants vous guideront dans la prise en main de MOSAIKS :\n\n\n\n\n\n\n\n\nChapitre\nSujets clés\n\n\n\n1  Configuration informatique\nGoogle Colab, gestion des données, pratiques de mise en œuvre\n\n\n2  Qu’est-ce que MOSAIKS ?\nConcepts fondamentaux, architecture du système, capacités\n\n\n3  Accès à MOSAIKS\nAccès API, produits de données, authentification\n\n\n4  Essayez MOSAIKS\nFlux de travail de base, exemple d’analyse, pièges courants\n\n\n\n\n\nTable 1: Plan de la section d’introduction\n\n\nCes chapitres fournissent à la fois la compréhension théorique et les compétences pratiques nécessaires pour commencer à travailler avec MOSAIKS. L’accent est mis sur l’accessibilité de la prédiction par satellite tout en maintenant la rigueur scientifique et l’efficacité computationnelle.\n\n\n\n\n\n\nPerspectives\n\n\n\nDans le prochain chapitre, nous configurerons notre environnement de calcul en utilisant Google Colab. Ce cours utilise Colab pour démontrer divers aspects de MOSAIKS, dans un environnement gratuit et accessible.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html",
    "href": "part-00-intro/01-intro-compute.html",
    "title": "1  Configuration informatique",
    "section": "",
    "text": "1.1 Vue d’ensemble\nCe cours utilise principalement Google Colaboratory (Colab) pour nos besoins en calcul. Colab est une plateforme gratuite basée sur le cloud qui permet d’écrire et d’exécuter du code Python via votre navigateur. Il est livré avec de nombreuses bibliothèques préinstallées et offre un accès gratuit aux ressources de calcul, y compris les GPUs.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Configuration informatique</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#prérequis",
    "href": "part-00-intro/01-intro-compute.html#prérequis",
    "title": "1  Configuration informatique",
    "section": "1.2 Prérequis",
    "text": "1.2 Prérequis\nPour participer aux parties de programmation de ce cours, vous aurez besoin de :\n\nUn ordinateur portable ou de bureau\nUne connexion Internet fiable\nUn compte Google (si vous n’en avez pas, créez-en un sur accounts.google.com)\nUn navigateur web (navigateurs basés sur Chromium recommandés)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Configuration informatique</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#débuter-avec-google-colab",
    "href": "part-00-intro/01-intro-compute.html#débuter-avec-google-colab",
    "title": "1  Configuration informatique",
    "section": "1.3 Débuter avec Google Colab",
    "text": "1.3 Débuter avec Google Colab\n\n1.3.1 Accéder à Colab\n\nAllez sur colab.research.google.com\nConnectez-vous avec votre compte Google\nCliquez sur “Nouveau Notebook” pour créer votre premier notebook Colab\n\n\n\n1.3.2 Comprendre l’interface\nL’interface Colab est similaire aux notebooks Jupyter, avec quelques éléments clés :\n\nBarre de menu : Contient les options Fichier, Édition, Affichage, Insertion, Runtime, Outils et Aide.\nBarre d’outils : Accès rapide aux actions courantes comme l’ajout de cellules de code/texte.\nZone des cellules : Où vous écrivez et exécutez le code ou le texte.\nÉtat du runtime : Montre l’état de la connexion de votre notebook aux serveurs Google.\n\n\n\n1.3.3 Opérations de base\n\nCréer des cellules :\n\nCellules de code : Cliquez sur + Code. Supporte Python ou R selon le runtime sélectionné\nCellules de texte : Cliquez sur + Texte. Supporte Markdown et les balises HTML pour la documentation\n\nExécuter des cellules :\n\nCliquez sur le bouton lecture à côté de la cellule ou utilisez Shift+Enter\nVous pouvez aussi sélectionner Runtime &gt; Exécuter la cellule sélectionnée dans le menu\n\n\n\n\n1.3.4 Fonctionnalités importantes\n\nType de Runtime :\n\nCliquez sur Runtime &gt; Modifier le type de runtime\nSélectionnez Python 3 comme runtime\nPour l’accès GPU : Changez l’accélérateur matériel pour un des types GPU proposés si nécessaire\n\nGestion des fichiers :\n\nLes fichiers téléchargés sur Colab sont temporaires et seront perdus à la déconnexion du runtime\nConnectez-vous à Google Drive et sauvegardez les sorties pour un stockage permanent :\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nInstallation des packages :\n\nInstallez des packages supplémentaires en utilisant :\n\ncondapip\n\n\n# Attention : utiliser \"!conda install\" n'est pas recommandé. \n# En règle générale, utilisez la commande magique \"%conda install\"\n%conda install &lt;nom_package&gt;\n\n\n# Attention : utiliser \"!pip install\" n'est pas recommandé. \n# En règle générale, utilisez la commande magique \"%pip install\"\n%pip install &lt;nom_package&gt;\n\n\n\n\n\n1.3.5 Bonnes pratiques\n\nSauvegardez votre travail :\n\nLes liens dans ce livre créeront une nouvelle copie d’un notebook tel qu’il est sauvegardé sur GitHub\nPour sauvegarder vos modifications, cliquez sur Fichier &gt; Enregistrer une copie dans Drive\nTéléchargez localement les notebooks importants comme sauvegardes\n\nGestion des ressources :\n\nFermez les notebooks inutilisés pour libérer des ressources\nSoyez conscient des délais d’inactivité (les notebooks se déconnectent après une période d’inactivité)\n\nUtilisation de la mémoire :\n\nSurveillez l’utilisation de la mémoire via Runtime &gt; Voir les ressources\nLa version gratuite de Colab fournit une mémoire très limitée (12GB) qui peut être insuffisante pour les grands jeux de données ou les modèles complexes\n\n\n\n\n1.3.6 Raccourcis clavier\nVoici quelques raccourcis clavier utiles pour travailler dans Colab :\n\nWindows/LinuxMac\n\n\n\n\n\n\n\n\n\n\n\n\nRaccourci\nAction\n\n\n\n\nCtrl+M H\nVoir les raccourcis clavier\n\n\nCtrl+Enter\nExécuter la cellule courante\n\n\nShift+Enter\nExécuter et passer à la suivante\n\n\nAlt+Enter\nExécuter et insérer dessous\n\n\nCtrl+M A\nInsérer cellule code au-dessus\n\n\nCtrl+M B\nInsérer cellule code en-dessous\n\n\nCtrl+M M\nConvertir en cellule texte\n\n\nCtrl+M Y\nConvertir en cellule code\n\n\nCtrl+M D\nSupprimer la cellule courante\n\n\nCtrl+M L\nAfficher numéros de ligne\n\n\nCtrl+M O\nAfficher/masquer sortie\n\n\nCtrl+M X\nCouper la cellule\n\n\nCtrl+M C\nCopier la cellule\n\n\nCtrl+M V\nColler la cellule en-dessous\n\n\nShift+Up/Down\nSélectionner plusieurs cellules\n\n\nCtrl+F\nRechercher et remplacer\n\n\nCtrl+S\nSauvegarder le notebook\n\n\n\n\n\nTable 1.1: Raccourcis Windows/Linux\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaccourci\nAction\n\n\n\n\n⌘+M H\nVoir les raccourcis clavier\n\n\n⌘+Enter\nExécuter la cellule courante\n\n\nShift+Enter\nExécuter et passer à la suivante\n\n\nOption+Enter\nExécuter et insérer dessous\n\n\n⌘+M A\nInsérer cellule code au-dessus\n\n\n⌘+M B\nInsérer cellule code en-dessous\n\n\n⌘+M M\nConvertir en cellule texte\n\n\n⌘+M Y\nConvertir en cellule code\n\n\n⌘+M D\nSupprimer la cellule courante\n\n\n⌘+M L\nAfficher numéros de ligne\n\n\n⌘+M O\nAfficher/masquer sortie\n\n\n⌘+M X\nCouper la cellule\n\n\n⌘+M C\nCopier la cellule\n\n\n⌘+M V\nColler la cellule en-dessous\n\n\nShift+Up/Down\nSélectionner plusieurs cellules\n\n\n⌘+F\nRechercher et remplacer\n\n\n⌘+S\nSauvegarder le notebook\n\n\n\n\n\nTable 1.2: Raccourcis Mac\n\n\n\n\n\n\n\n\n1.3.7 Problèmes courants et solutions\n\nDéconnexions du Runtime :\n\nCliquez sur “Reconnecter” quand demandé\nVos variables seront réinitialisées, mais le code sauvegardé reste\n\nProblèmes d’installation de packages :\n\nRedémarrez le runtime après l’installation de nouveaux packages\nUtilisez Runtime &gt; Redémarrer le runtime\n\nErreurs de mémoire :\n\nEffacez les variables inutiles au fur et à mesure\nEnvisagez d’utiliser des échantillons de données plus petits pendant le développement\n\n\n\n\n\n\n\n\nLes erreurs de mémoire sont fréquentes lors du travail avec de grands jeux de données ou des modèles complexes sur la version gratuite de Colab. Si vous rencontrez ces problèmes, envisagez d’utiliser une version payante de Colab ou de connecter une machine virtuelle Google Cloud Platform (VM).\n\n\n\n\n\n1.3.8 Obtenir de l’aide\n\nAccédez à la documentation Colab : Aide &gt; Questions fréquemment posées\nEssayez d’utiliser Google Gemini pour l’assistance IA.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Configuration informatique</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#assistance-ia-dans-colab",
    "href": "part-00-intro/01-intro-compute.html#assistance-ia-dans-colab",
    "title": "1  Configuration informatique",
    "section": "1.4 Assistance IA dans Colab",
    "text": "1.4 Assistance IA dans Colab\nGoogle Gemini est un puissant assistant IA intégré à Google Colab. Vous pouvez l’utiliser pour générer du code, des commentaires ou du texte markdown pour améliorer vos notebooks. Gemini est accessible de plusieurs façons dans Colab, toutes commençant par la sélection de l’icône Gemini dans différentes parties de l’éditeur de notebook.\n\n\n\n\n\n\nIcône Gemini\n\n\n\n\nCherchez cette icône pour indiquer où vous pouvez cliquer pour accéder à Gemini dans Colab.\n\n\nVoici quelques façons d’utiliser Google Gemini efficacement dans Colab :\n\n1.4.1 Support par chat\nCliquez sur le bouton Gemini en haut à droite pour ouvrir une interface de chat où vous pouvez poser des questions sur votre code, déboguer des problèmes ou obtenir des explications sur des concepts. Cette option est particulièrement utile pour les débutants ou pour résoudre des problèmes complexes.\n\n\n1.4.2 Génération de code\nUtilisez l’option “Générer du code” (l’icône étincelle) au-dessus de toute cellule de code vide pour générer du nouveau code basé sur votre description. Vous pouvez lui demander de faire différentes choses comme :\n\nCharger un jeu de données appelé my_data.csv\nTracer un histogramme des données\nConstruire un modèle pour prédire y à partir de X\n\n\n\n1.4.3 Explication de code\nUtilisez l’option “Expliquer le code” (l’icône étincelle) au-dessus de toute cellule de code complète pour ouvrir une interface de chat qui expliquera automatiquement le code dans la cellule. C’est utile pour comprendre le code écrit par quelqu’un d’autre, apprendre de nouveaux concepts ou obtenir un second avis sur votre travail.\n\n\n1.4.4 Complétion de code\nColab fournit une autocomplétion intelligente pendant la saisie :\n\nAppuyez sur Tab pour accepter les suggestions\nUtilisez Ctrl+Espace (Cmd+Espace sur Mac) pour déclencher manuellement les suggestions\nObtenez la documentation et les conseils de paramètres en temps réel\n\n\n\n\n\n\n\nBien que ces outils d’IA soient utiles, vérifiez et comprenez toujours le code qu’ils suggèrent avant de l’utiliser dans votre travail.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Configuration informatique</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#accéder-aux-notebooks-du-cours",
    "href": "part-00-intro/01-intro-compute.html#accéder-aux-notebooks-du-cours",
    "title": "1  Configuration informatique",
    "section": "1.5 Accéder aux notebooks du cours",
    "text": "1.5 Accéder aux notebooks du cours\nTous les notebooks du cours sont hébergés sur GitHub et peuvent être accédés directement dans Google Colab. Il y a deux façons de les ouvrir :\n\n1.5.1 Méthode 1 : Liens directs\nChaque section de ce livre inclut des liens directs “Ouvrir dans Colab” pour les notebooks pertinents. Cliquez simplement sur le badge pour ouvrir le notebook :\nExemple \nCette méthode ouvrira une nouvelle copie du notebook tel qu’il est sauvegardé sur GitHub. Si vous avez déjà cliqué sur le badge une fois, fait des modifications et sauvegardé votre notebook, vous devrez naviguer vers votre dossier Drive où il est sauvegardé pour accéder à ces modifications.\n\n\n\n\n\n\nCliquer sur le badge dans ce livre ouvrira toujours une nouvelle copie.\n\n\n\n\n\n1.5.2 Méthode 2 : Cloner le notebook\nPour sélectionner un notebook du dépôt Dépôt de notebooks :\n\nOuvrez Google Colab (colab.research.google.com)\nCliquez sur Fichier &gt; Ouvrir un notebook\nSélectionnez l’onglet GitHub\nEntrez l’URL du dépôt : https://github.com/[username]/[repo] (À METTRE À JOUR AVEC LE DÉPÔT)\nSélectionnez le notebook que vous voulez ouvrir\n\n\n\n1.5.3 Sauvegarder votre travail\nQuand vous ouvrez un notebook depuis GitHub dans Colab, il crée une copie temporaire. Pour sauvegarder votre travail :\n\nCliquez sur Fichier &gt; Enregistrer une copie dans Drive\nCela crée votre propre copie modifiable dans votre Google Drive\nToutes les modifications futures seront sauvegardées dans votre copie\n\n\n\n1.5.4 Organisation des notebooks\nLes notebooks du cours sont organisés en :\n\ndemos/ : Notebooks de démonstration complets\nexercises/ : Notebooks interactifs avec exercices à compléter\nsolutions/ : Versions complètes des notebooks d’exercices\n\nChaque notebook inclut :\n\nInstructions claires et explications en cellules markdown\nCellules de code avec exemples ou exercices\nSections À FAIRE pour les exercices\nCellules de validation pour vérifier votre travail",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Configuration informatique</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#accès-et-gestion-des-données",
    "href": "part-00-intro/01-intro-compute.html#accès-et-gestion-des-données",
    "title": "1  Configuration informatique",
    "section": "1.6 Accès et gestion des données",
    "text": "1.6 Accès et gestion des données\nIl existe plusieurs façons d’accéder aux données dans les notebooks Colab. Voici les principales approches :\n\n1.6.1 Téléchargements directs\nPour les données hébergées sur des dépôts comme Zenodo, vous pouvez télécharger directement en utilisant wget :\n# Télécharger les données\n!wget https://zenodo.org/records/14040658/files/Data.zip\n\n# Décompresser les données\n!unzip Data.zip\n\n\n1.6.2 Intégration Google Drive\n\n1.6.2.1 Monter Google Drive\nPour les données stockées dans Google Drive :\n\nD’abord, montez votre Google Drive :\nfrom google.colab import drive\ndrive.mount('/content/drive')\nAccédez à vos données en utilisant le chemin monté :\ndrive_path = \"/content/drive/MyDrive/&lt;dossier_projet&gt;\"\n\n\n\n1.6.2.2 Copier les données vers la VM (optionnel)\nPour de meilleures performances, faites des copies locales des données sur la machine virtuelle (VM) :\nimport os\nimport shutil\n\n# Créer un répertoire local\nlocal_dir = \"/content/data/\"\nos.makedirs(local_dir, exist_ok=True)\n\n# Copier les données de Drive vers la VM\ndrive_data = os.path.join(drive_path, \"my_data\") \nshutil.copytree(drive_data, local_dir, dirs_exist_ok=True)\n\n\n\n\n\n\nN’oubliez pas que le stockage de la VM est temporaire - les fichiers seront supprimés à la déconnexion du runtime. Gardez toujours une sauvegarde de vos données dans Drive ou un autre emplacement de stockage permanent.\n\n\n\n\n1.6.2.2.1 Pourquoi copier les données vers la VM ?\nLors du travail avec des données dans Colab, copier les fichiers de Google Drive vers la machine virtuelle (VM) peut améliorer significativement les performances :\n\nAccès plus rapide : La lecture directe depuis Google Drive nécessite un transfert de données sur le réseau pour chaque opération. Le stockage local de la VM offre des vitesses de lecture/écriture beaucoup plus rapides.\nLatence réduite : La latence réseau entre Colab et Google Drive peut ralentir les opérations nécessitant plusieurs accès aux données. Les données locales éliminent cette latence.\nPlus fiable : Les problèmes de connectivité réseau ou d’accès à Drive n’interrompront pas votre analyse une fois les données copiées localement.\nMeilleur pour le traitement itératif : Si votre code doit lire les mêmes données plusieurs fois (comme dans les boucles d’entraînement d’apprentissage automatique), l’accès local est beaucoup plus efficace.\n\nPar exemple, lire un jeu de données de 1 Go depuis Drive peut prendre 30 secondes, tandis que la lecture depuis le stockage local de la VM pourrait prendre quelques secondes. Le temps passé à copier les données une fois au début de votre session peut économiser un temps significatif pendant l’analyse. C’est particulièrement vrai dans un environnement notebook où un utilisateur peut développer du code qui accède répétitivement aux mêmes fichiers de données, mais ne peut pas tout stocker en mémoire (par exemple, de nombreux fichiers images).\n\n\n\n1.6.2.3 Sauvegarder les sorties vers Google Drive\nPour sauvegarder les sorties ou les modèles vers Google Drive :\n# Définir le répertoire de sortie\noutput_dir = \"/content/drive/MyDrive/project_folder/output\"\n\n# Sauvegarder les sorties\nshutil.copytree(local_output, output_dir, dirs_exist_ok=True)\nCela garantit que tout travail effectué dans le notebook est sauvegardé dans votre Google Drive pour référence future. Si les fichiers de sortie ne sont pas copiés et restent dans la VM, ils seront perdus à la déconnexion du runtime.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Configuration informatique</span>"
    ]
  },
  {
    "objectID": "part-00-intro/01-intro-compute.html#configuration-de-lenvironnement-local",
    "href": "part-00-intro/01-intro-compute.html#configuration-de-lenvironnement-local",
    "title": "1  Configuration informatique",
    "section": "1.7 Configuration de l’environnement local",
    "text": "1.7 Configuration de l’environnement local\nBien que l’approche principale de ce livre soit d’utiliser Google Colab, certains apprenants peuvent préférer ou avoir besoin d’exécuter le code localement. Le livre est largement configuré pour cela, bien que l’utilisateur doive gérer son propre environnement informatique. Pour cela, nous fournissons un fichier environment.yml (situé dans le répertoire environment de ce livre). Voici les étapes pour vous installer Miniconda et créer un environnement local.\n\n\n\n\n\n\nBien que les environnements locaux puissent offrir plus de contrôle, nous recommandons fortement Google Colab pour la cohérence et les ressources cloud gratuites. Cette configuration locale est purement optionnelle et pourrait être plus adaptée pour ceux ayant des dépendances particulières ou des configurations avancées.\n\n\n\n\n1.7.1 Télécharger et installer Miniconda\nMiniconda est un installateur minimal pour conda. Choisissez l’installateur pour votre système d’exploitation parmi les liens ci-dessous et suivez les invites.\n\nWindowsmacOSLinux\n\n\n\nAllez sur l’Installateur Windows Miniconda.\nTéléchargez l’installateur .exe pour votre système Windows (64-bit recommandé).\nDouble-cliquez sur l’installateur et suivez les instructions à l’écran.\nQuand demandé, cochez l’option Ajouter Miniconda au PATH ou sélectionnez “Installer pour tous les utilisateurs” qui ajoute généralement conda au PATH automatiquement.\n\n\n\n\nAllez sur l’Installateur macOS Miniconda.\nTéléchargez l’installateur .pkg (ou .sh si vous préférez) pour macOS (64-bit).\nDouble-cliquez sur l’installateur et suivez les instructions à l’écran.\nQuand demandé, cochez l’option Ajouter Miniconda au PATH ou ajoutez manuellement les lignes de chemin appropriées à votre fichier ~/.zshrc ou ~/.bash_profile.\n\n\n\n\nAllez sur l’Installateur Linux Miniconda.\nTéléchargez l’installateur .sh pour votre distribution Linux (64-bit recommandé).\nOuvrez un terminal et exécutez bash Miniconda3-latest-Linux-x86_64.sh.\nSuivez les invites ; envisagez de permettre à l’installateur d’initialiser Miniconda pour votre shell (ajoutant conda à votre PATH).\n\n\n\n\n\n\n1.7.2 Ajouter conda à votre PATH\nSi vous n’avez pas ajouté conda à votre PATH pendant l’installation, vous pouvez le faire manuellement en ajoutant une ligne à votre fichier de configuration shell (~/.bashrc, ~/.zshrc, ou similaire) :\n# Exemple pour les utilisateurs Linux/macOS\nexport PATH=\"$HOME/miniconda3/bin:$PATH\"\nPour Windows, assurez-vous d’avoir sélectionné l’option d’ajouter conda au PATH pendant l’installation, ou exécutez l’Invite Anaconda (qui a automatiquement conda disponible) pour gérer votre environnement.\n\n\n1.7.3 Créer un environnement local depuis environment.yml\nDans le répertoire environment du dépôt du cours, vous trouverez un fichier nommé environment.yml. Ce fichier liste tous les packages nécessaires pour la configuration locale.\n\nClonez ou téléchargez le dépôt du livre sur votre machine locale.\nOuvrez un terminal (ou Invite Anaconda sur Windows).\nNaviguez vers le dossier contenant environment.yml.\ncd chemin/vers/MOSAIKS-Training-Manual/environment\nCréez l’environnement :\nconda env create -f environment.yml\nActivez l’environnement :\nconda activate &lt;nom_environnement&gt;\nOù &lt;nom_environnement&gt; est le nom spécifié dans environment.yml (vérifiez le champ name: dans le fichier). Dans ce cas, le nom est mosaiks.\n\n\n\n1.7.4 Utiliser le nouvel environnement dans VS Code\nVisual Studio Code (VS Code) peut détecter et utiliser votre nouvel environnement conda pour le développement Python.\n\nOuvrez VS Code.\nInstallez l’extension Python (si pas déjà installée).\nAppuyez sur Ctrl+Shift+P (ou Cmd+Shift+P sur macOS) et tapez “Python : Sélectionner l’interpréteur”.\nSélectionnez l’interpréteur associé à votre environnement nouvellement créé (il devrait être listé par nom ou chemin).\nOuvrez ou créez un nouveau fichier Python ou notebook, et vérifiez que VS Code utilise le bon environnement (vous pouvez voir l’environnement choisi en bas à droite de VS Code).\n\n\n\n1.7.5 Autres gestionnaires d’environnement\nBien que conda soit un outil commun pour gérer les environnements Python, il existe d’autres options populaires comme :\n\nPoetry\n\npipenv\n\nvirtualenv\n\nChacun a ses propres fichiers de configuration et instructions d’installation. Si vous préférez ces outils ou les utilisez déjà, vous pouvez généralement reproduire les packages listés dans environment.yml. Consultez la documentation respective de l’outil pour des instructions spécifiques sur la façon de traduire les dépendances.\n\n\n\n\n\n\n\nPerspectives\n\n\n\nDans le prochain chapitre, nous examinerons de plus près le framework MOSAIKS, ses concepts fondamentaux et comment il peut être appliqué pour résoudre des problèmes du monde réel.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Configuration informatique</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html",
    "href": "part-00-intro/02-intro-mosaiks.html",
    "title": "2  Qu’est-ce que MOSAIKS ?",
    "section": "",
    "text": "2.1 Le défi\nActuellement, de nombreux systèmes satellites publics collectent quotidiennement d’énormes quantités de données sur le monde. Mais il y a tellement d’images (des téraoctets par jour) qu’il est impossible de les trier manuellement ; et elles sont trop complexes et non structurées pour être utilisables dans leur forme brute pour la plupart des applications.\nC’est pourquoi l’association de l’imagerie satellite au machine learning (parfois appelé SIML ou SatML) est incroyablement puissante. Elle permet de transformer de vastes quantités de données d’images non structurées en informations structurées qui peuvent être utilisées pour la planification, la recherche et la prise de décision.\nNotre espoir est que les personnes du monde entier puissent accéder et utiliser les technologies SIML, mais nous reconnaissons que beaucoup de ceux qui bénéficieraient de ces outils n’ont pas le temps ni les ressources pour gérer d’énormes datasets d’imagerie satellite et apprendre à leur appliquer du machine learning.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Qu'est-ce que MOSAIKS ?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#le-défi",
    "href": "part-00-intro/02-intro-mosaiks.html#le-défi",
    "title": "2  Qu’est-ce que MOSAIKS ?",
    "section": "",
    "text": "Figure 2.1: Représentation visuelle des satellites en orbite autour de la Terre.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Qu'est-ce que MOSAIKS ?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#la-solution",
    "href": "part-00-intro/02-intro-mosaiks.html#la-solution",
    "title": "2  Qu’est-ce que MOSAIKS ?",
    "section": "2.2 La solution",
    "text": "2.2 La solution\nC’est pourquoi nous avons développé MOSAIKS. MOSAIKS vise à réduire les barrières à l’entrée dans le SIML, en diversifiant les utilisateurs de cette technologie puissante et les problèmes que nous résolvons avec elle.\nMOSAIKS est conçu pour fonctionner “prêt à l’emploi” pour une large gamme d’applications SIML, pour les personnes sans expertise en SIML qui travaillent sur des ordinateurs de bureau ou des ordinateurs portables normaux. Pour de nombreuses applications, les utilisateurs de MOSAIKS n’ont jamais besoin de toucher à l’imagerie satellite eux-mêmes et n’ont besoin que d’une formation statistique de base.\n\nSi vous pouvez exécuter une régression, vous pouvez utiliser MOSAIKS !\n\nMOSAIKS permet aux utilisateurs de créer leurs propres nouveaux datasets à partir d’images satellites. Nous ne contrôlons pas les variables que les utilisateurs examinent, et nous n’avons jamais besoin de le savoir. MOSAIKS est un système qui permet aux utilisateurs de transformer rapidement de vastes quantités d’images en cartes de nouvelles variables, en utilisant leurs propres données d’apprentissage.\nSi vous avez déjà été curieux d’essayer le machine learning avec des images satellites, mais que vous ne savez rien sur le machine learning ou les images satellites, MOSAIKS est pour vous.\nEt si vous connaissez beaucoup de choses sur le machine learning et les images satellites, MOSAIKS pourrait encore être pour vous, car il performe de manière compétitive avec les méthodes d’apprentissage profond mais est beaucoup plus simple et moins coûteux à utiliser.\n\n\n\n\n\n\n\n\nFigure 2.2: Traditional framework of deep learning models (i.e., machine learning with artificial neural networks) applied to imagery. In this example, the model is attempting to classify what vehicle the image contains.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Qu'est-ce que MOSAIKS ?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#comment-mosaiks-fonctionne",
    "href": "part-00-intro/02-intro-mosaiks.html#comment-mosaiks-fonctionne",
    "title": "2  Qu’est-ce que MOSAIKS ?",
    "section": "2.3 Comment MOSAIKS fonctionne",
    "text": "2.3 Comment MOSAIKS fonctionne\n\n\n\n\n\n\nLecture recommandée\n\n\n\nA generalizable and accessible approach to machine learning with global satellite imagery (Rolf et al. 2021)\n\n\n\n2.3.1 Séparation des utilisateurs de l’imagerie\nL’idée de base de MOSAIKS est de séparer les utilisateurs du processus coûteux et difficile de transformation des images en entrées (appelées “features”) pour un algorithme d’apprentissage automatique en aval (images → X). L’équipe MOSAIKS a calculé ces fonctionnalités à l’échelle mondiale, donc dans de nombreux cas, les utilisateurs n’ont jamais besoin de télécharger ou de gérer les images eux-mêmes. Au lieu de cela, les utilisateurs téléchargent un tableau de fonctionnalités MOSAIKS (X), les relient à leurs propres données géocodées sur le résultat (Y) qu’ils souhaitent prédire à partir d’images satellites (nous appelons ces données “étiquettes”), puis exécutent une régression linéaire (ou quelque chose de plus complexe si désiré !) pour prédire leurs étiquettes en utilisant les fonctionnalités MOSAIKS (Y = Xβ). Il est important de noter que cette prédiction peut être effectuée dans des lieux, des périodes et à des résolutions spatiales pour lesquelles les étiquettes ne sont pas disponibles.\n\n\n\n  \n\n\nFigure 2.3: Cette image en fausses couleurs montre les sommets enneigés et les crêtes de l’Himalaya oriental entre les grands fleuves du sud-ouest de la Chine. L’Himalaya est constitué de trois chaînes de montagnes parallèles qui s’étendent sur plus de 2 900 kilomètres. Cette image a été prise par l’instrument ASTER (Advanced Spaceborne Thermal Emission and Reflection Radiometer) de la NASA, à bord du satellite Terra, le 27 février 2002. L’image est une composition réalisée en combinant des longueurs d’onde proches de l’infrarouge, rouges et vertes. (Source : NASA)\n\n\n\n\n\n2.3.2 Généralisabilité de MOSAIKS\nPuisque les fonctionnalités MOSAIKS synthétisent les informations contenues dans les images brutes qui ne sont pas adaptées à un résultat spécifique (par exemple, biodiversité, revenu des ménages, utilisation des terres), de nombreux utilisateurs peuvent utiliser les mêmes fonctionnalités MOSAIKS et simplement les faire correspondre à leurs propres étiquettes en fonction de l’emplacement. Les utilisateurs peuvent exécuter leur analyse sur n’importe quel logiciel statistique avec lequel ils sont à l’aise. Pour la plupart des applications, les exigences de calcul ne nécessiteront pas que les utilisateurs travaillent avec des machines spécialisées, car les ordinateurs de bureau et les ordinateurs portables fonctionnent.\n\n\n\n\n\n\nFigure 2.4: MOSAIKS est conçu pour résoudre un nombre illimité de tâches à l’échelle planétaire rapidement. Après une featurisation d’image non supervisée unique à l’aide de fonctionnalités convolutionnelles aléatoires, MOSAIKS stocke et distribue des fonctionnalités agnostiques à la tâche aux utilisateurs, chacun desquels génère des prédictions dans un nouveau contexte. A Les images satellites sont partagées entre plusieurs tâches potentielles. B Schéma du processus MOSAIKS. N images sont transformées en utilisant des fonctionnalités convolutionnelles aléatoires en un vecteur de fonctionnalités compressé et hautement descriptif K-dimensionnel avant que les étiquettes ne soient connues. Une fois les fonctionnalités calculées, elles peuvent être stockées sous forme de tableau (matrice X) et utilisées pour un nombre illimité de tâches sans nouvelle calcul. Les utilisateurs intéressés par une nouvelle tâche (s) fusionnent leurs propres étiquettes (ys) avec les fonctionnalités pour la formation. Ici, l’utilisateur 1 a des étiquettes de couverture forestière pour les emplacements p + 1 à N et l’utilisateur 2 a des étiquettes de densité de population pour les emplacements 1 à q. Chaque utilisateur résout ensuite une régression linéaire unique pour βs. La prédiction linéaire à l’aide de βs et de l’ensemble des fonctionnalités MOSAIKS X génère ensuite des estimations SIML pour les valeurs des étiquettes à tous les emplacements. La généralisabilité permet à différents utilisateurs de résoudre différentes tâches en utilisant une procédure et un tableau de fonctionnalités identiques - ne différant que dans les données d’étiquetage fournies par l’utilisateur pour la formation. Chaque tâche peut être résolue par un utilisateur sur un ordinateur de bureau en quelques minutes sans que l’utilisateur n’ait jamais à manipuler les images. C Illustration du calcul unique non supervisé des fonctionnalités convolutionnelles aléatoires. K patchs sont échantillonnés aléatoirement à partir de l’ensemble des N images. Chaque patch est convolué sur chaque image, générant une carte d’activation non linéaire pour chaque patch. Les cartes d’activation sont moyennées sur les pixels pour générer un vecteur de fonctionnalités unique K-dimensionnel pour chaque image. (Source : Rolf et al. 2021 Figure 1)\n\n\n\n\n\n2.3.3 Pourquoi cela fonctionne\nMOSAIKS fonctionne car les fonctionnalités MOSAIKS capturent une énorme quantité d’informations sur les couleurs, les modèles et les textures qui apparaissent dans les images satellites. Nous ne savons pas quels modèles/couleurs/textures seront importants pour l’application que les utilisateurs ont (puisque nous ne savons pas quelles applications les utilisateurs essayeront), nous essayons donc de capturer tous. Le but de l’étape de régression est d’enseigner au modèle quelles combinaisons de couleurs, de motifs et de textures prédisent les étiquettes, puis d’utiliser cette compréhension pour faire des prédictions dans les emplacements où les utilisateurs n’ont pas d’étiquettes. De plus, MOSAIKS encode les informations d’image d’une manière qui permet des relations non linéaires entre les étiquettes et les données d’image, même si la régression que les utilisateurs mettent généralement en œuvre est une régression linéaire.\n\n\n\n\n\n\nFigure 2.5: Exemple de 4 (sur 4 000) cartes de fonctionnalités MOSAIKS (à droite) calculées à partir d’images satellites (à gauche). Ces fonctionnalités ont été choisies au hasard parmi celles disponibles sur l’API MOSAIKS.\n\n\n\nPour en savoir plus sur ces fonctionnalités, voir Chapter 12 où nous essayons de fournir une intuition sur ce qu’est une fonctionnalité et comment elle est faite.\n\n\n2.3.4 Cinq étapes pour utiliser MOSAIKS\n\n\n\n\n\n\nCette section est une vue d’ensemble très large des étapes pour utiliser MOSAIKS. Les chapitres suivants fourniront des conseils plus détaillés sur chaque étape.\n\n\n\nDans de nombreux cas, les utilisateurs qui visent à prédire un résultat à partir d’images satellites peuvent le faire en utilisant des fonctionnalités d’images précalculées (X) dans un cadre de régression linéaire simple. Plus tard dans ce cours de formation, nous détaillerons des flux de travail plus personnalisés qui restent réalisables mais permettent plus de flexibilité. Dans le cas standard, cependant, la procédure pour utiliser MOSAIKS comporte cinq étapes (la figure correspondante de Rolf et al. est ci-dessous) :\n\nTélécharger les fonctionnalités MOSAIKS précalculées (X) correspondant aux emplacements où vous avez des étiquettes (Chapter 3).\nFusionner les fonctionnalités avec vos étiquettes (Y) en fonction de l’emplacement (de sorte que les fonctionnalités à la position P soient liées aux étiquettes à la position P) (Chapter 7).\nExécuter une régression linéaire avec validation croisée de vos étiquettes sur les fonctionnalités MOSAIKS (Y = Xβ + e ; ou tout autre modèle que vous choisissez ! Voir ?sec-sec-model-choice).\nÉvaluer les performances.\nUtiliser les résultats de la régression (β) pour faire des prédictions (Xβ) dans une nouvelle région d’intérêt où vous n’avez pas d’étiquettes, en utilisant uniquement les fonctionnalités MOSAIKS (X) qui correspondent à ces nouveaux emplacements.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Qu'est-ce que MOSAIKS ?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#que-peut-prédire-mosaiks",
    "href": "part-00-intro/02-intro-mosaiks.html#que-peut-prédire-mosaiks",
    "title": "2  Qu’est-ce que MOSAIKS ?",
    "section": "2.4 Que peut prédire MOSAIKS ?",
    "text": "2.4 Que peut prédire MOSAIKS ?\n\n\n\n\n\n\nCette question est abordée plus en détail dans Chapter 5\n\n\n\nMOSAIKS a été utilisé avec succès pour prédire une large gamme de résultats, notamment :\n\nLes conditions environnementales (couverture forestière, élévation)\nLes modèles de population (densité, lumières nocturnes)\nLes indicateurs économiques (revenu, prix des maisons)\nLes infrastructures (réseaux routiers)\n\nLa figure ci-dessous est tirée de la publication originale MOSAIKS (Rolf et al. 2021). Les cartes de gauche montrent les étiquettes d’entrée. La carte de droite montre les prédictions modélisées. Le graphique en éventail montre les prédictions modélisées par rapport aux étiquettes réelles et rapporte le coefficient de détermination (R²) comme mesure de performance.\n\n\n\n\n\n\nFigure 2.6: (100 000 images diurnes ont été converties en 8 192 fonctionnalités et stockées. Sept tâches ont été sélectionnées en fonction de la couverture et de la diversité. Des prédictions ont été générées pour chaque tâche en utilisant la même procédure. Cartes de gauche : 80 000 observations utilisées pour la formation et la validation, agrégées à 20 km × 20 km pour l’affichage. Cartes de droite : estimations de validation concaténées à partir de la validation croisée à 5 plis pour les mêmes 80 000 cellules de grille (les observations ne sont jamais utilisées pour générer leur propre prédiction), identiquement agrégées pour l’affichage. Éventails : estimations de validation (axe vertical) vs. étiquettes réelles (axe horizontal) ; chaque point est une cellule de grille d’environ 1 km × 1 km. La ligne noire est à 45∘. Les performances du jeu de test et de validation sont essentiellement identiques ; les valeurs de validation sont affichées à des fins de présentation uniquement, car il y a plus d’observations. Les tâches des trois premières rangées sont échantillonnées uniformément dans l’espace, les tâches des quatre dernières rangées sont échantillonnées à l’aide de poids de population ; les zones grises n’ont pas été échantillonnées dans l’expérience. Source : Rolf et al. 2021 Figure 2)\n\n\n\nIl est important de noter que toutes ces prédictions utilisent le même ensemble de fonctionnalités d’images satellites - il n’est pas nécessaire de retraiter les images pour différentes tâches. MOSAIKS atteint une précision comparable aux méthodes d’apprentissage profond plus complexes, mais à une fraction du coût de calcul. C’est le pouvoir de MOSAIKS, il élimine la nécessité de retraiter les images après l’encodage initial.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Qu'est-ce que MOSAIKS ?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#mosaiks-est-il-toujours-le-meilleur-choix",
    "href": "part-00-intro/02-intro-mosaiks.html#mosaiks-est-il-toujours-le-meilleur-choix",
    "title": "2  Qu’est-ce que MOSAIKS ?",
    "section": "2.5 MOSAIKS est-il toujours le meilleur choix ?",
    "text": "2.5 MOSAIKS est-il toujours le meilleur choix ?\nNon ! MOSAIKS est un outil puissant, mais il n’est pas toujours le meilleur choix pour chaque application. En fait, il n’est généralement pas le “meilleur” choix pour aucune application. Nous visons à être compétitifs avec les meilleurs modèles, donc le véritable avantage de MOSAIKS réside dans sa simplicité, son accessibilité et sa scalabilité pour l’utilisateur moyen.\nNous vous recommandons de commencer par rechercher des méthodes existantes développées pour votre application, avant d’investir du temps et des ressources dans MOSAIKS. Un excellent endroit pour commencer cette recherche est à satellite-image-deep-learning où vous pouvez trouver une liste de méthodes d’apprentissage profond qui ont été développées pour les images satellites, ainsi que des datasets existants, des outils et des tutoriels.\nLe monde du SIML est vaste et évolue rapidement. Cela signifie qu’il y a un bon choix que vous n’avez pas à faire des prédictions à l’échelle mondiale vous-même. Au lieu de cela, vous pourriez être en mesure d’utiliser ou de vous appuyer sur le travail acharné de nombreux autres dans le domaine.\nSi vous avez un contexte spécifique dans lequel vous souhaitez des informations personnalisées ou une variable/résultat que personne d’autre n’a prédit auparavant, alors vous voulez MOSAIKS. Non seulement MOSAIKS vous permettra-t-il de faire des prédictions dans un nouveau contexte, mais il vous permettra également de le faire rapidement et avec un minimum de ressources de calcul.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Qu'est-ce que MOSAIKS ?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#matériel-de-cours",
    "href": "part-00-intro/02-intro-mosaiks.html#matériel-de-cours",
    "title": "2  Qu’est-ce que MOSAIKS ?",
    "section": "2.6 Matériel de cours",
    "text": "2.6 Matériel de cours\nTODO: Ajouter une conférence enregistrée ici.\nLecture slides.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Qu'est-ce que MOSAIKS ?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/02-intro-mosaiks.html#résumé",
    "href": "part-00-intro/02-intro-mosaiks.html#résumé",
    "title": "2  Qu’est-ce que MOSAIKS ?",
    "section": "2.7 Résumé",
    "text": "2.7 Résumé\nMOSAIKS est un outil puissant qui permet aux utilisateurs de prédire une large gamme de résultats à partir d’images satellites en utilisant des fonctionnalités précalculées. Le système est conçu pour être accessible aux utilisateurs sans expérience préalable en apprentissage automatique ou en images satellites. Le cadre MOSAIKS implique cinq étapes simples\n\nTélécharger les fonctionnalités\nFusionner avec les étiquettes\nexécuter une régression\nÉvaluer les performances\nFaire des prédictions\n\nDans ce livre, nous explorerons toutes les manières dont cela est une simplification excessive. Vous apprendrez à adapter ce cadre à vos besoins et à comprendre les limites et les hypothèses du système MOSAIKS. De nombreuses compétences présentées dans ce manuel de formation seront applicables à d’autres flux de travail d’imagerie satellite et d’apprentissage automatique.\n\n\n\n\n\n\nRegard vers l’avenir\n\n\n\nDans le prochain chapitre, vous serez présenté à l’API MOSAIKS qui est une ressource gratuite et ouverte pour accéder aux fonctionnalités MOSAIKS précalculées.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Qu'est-ce que MOSAIKS ?</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html",
    "href": "part-00-intro/03-intro-api.html",
    "title": "3  Accès à MOSAIKS",
    "section": "",
    "text": "3.1 Introduction\nAu cœur de MOSAIKS, deux entrées principales sont requises : les caractéristiques satellites et les données de vérité terrain. Notre objectif est de rendre ces caractéristiques aussi accessibles que possible pour que la majorité des utilisateurs n’ait pas à se soucier des détails techniques du traitement d’imagerie satellite.\nPour cela, nous avons développé plusieurs moyens d’accéder aux caractéristiques MOSAIKS :\nL’API MOSAIKS doit être considérée comme le moyen principal d’accéder aux caractéristiques. C’est une interface conviviale qui permet de télécharger des caractéristiques pour n’importe quel endroit sur Terre. L’API est conçue pour être accessible aux utilisateurs de différents niveaux techniques, des débutants aux experts. Pour plus de détails sur les caractéristiques disponibles sur l’API, voir Chapter 13.\nCependant, dans de nombreux cas, les utilisateurs voudront ou devront calculer leurs propres caractéristiques personnalisées. Chapter 14 guide les lecteurs à travers ce processus.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Accès à MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#introduction",
    "href": "part-00-intro/03-intro-api.html#introduction",
    "title": "3  Accès à MOSAIKS",
    "section": "",
    "text": "Option\nSource d’imagerie\nCouverture spatiale\nRésolution spatiale\nRésolution temporelle\nPondération\n\n\n\nAPI MOSAIKS\nPlanet Labs Visual Basemap\nZones terrestres globales\n0.01°\n2019 Q3\nNon pondéré\n\n\nAPI MOSAIKS\nPlanet Labs Visual Basemap\nZones terrestres globales\n0.1°, 1°\n2019 Q3\nSurface & population\n\n\nAPI MOSAIKS\nPlanet Labs Visual Basemap\nZones terrestres globales\nADM0, ADM1, ADM2\n2019 Q3\nSurface & population\n\n\nRolf et al 2021\nGoogle Static Maps\nÉtats-Unis continentaux (~100k localisations)\n0.01°\n2019\nNon pondéré\n\n\nChapter 14\nToute source - voir Chapter 9\n\nDéfini par l’utilisateur\nDéfini par l’utilisateur\nDéfini par l’utilisateur\nDéfini par l’utilisateur\n\n\n\n\n\nTable 3.1: Résumé des sources de caractéristiques MOSAIKS. La colonne de pondération indique que les caractéristiques ont été générées à une résolution de 0.01 degré et sont pondérées lors de leur agrégation à des résolutions inférieures. Les schémas de pondération disponibles sont basés sur la surface du polygone d’agrégation ou sur la population du polygone d’agrégation.\n\n\n\n\n\n\n\n\n\nLes chapitres 1 à 8 se concentrent sur les caractéristiques publiquement disponibles\nLes chapitres 9 à 15 couvrent le calcul de caractéristiques personnalisées à partir d’imagerie",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Accès à MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#api-mosaiks",
    "href": "part-00-intro/03-intro-api.html#api-mosaiks",
    "title": "3  Accès à MOSAIKS",
    "section": "\n3.2 API MOSAIKS",
    "text": "3.2 API MOSAIKS\n\n\n\n\n\n\nLien API MOSAIKS\n\n\n\napi.mosaiks.org\n\n\nL’API MOSAIKS est une interface conviviale qui permet de télécharger des caractéristiques pour n’importe quelle localisation terrestre sur Terre. L’API est conçue pour être accessible aux utilisateurs de différents niveaux techniques, des débutants aux experts. Pour profiter de l’API, vous devrez créer un compte.\n\n3.2.1 Créer un compte\nVisitez api.mosaiks.org.\n\n\n\n\n\nFigure 3.1: Page de connexion de l’API MOSAIKS.\n\n\nSélectionnez Register pour créer un compte. Vous devrez fournir un nom d’utilisateur, un email et un mot de passe.\n\n\n\n\n\nFigure 3.2: Page d’inscription de l’API MOSAIKS.\n\n\nUne fois inscrit, vous pouvez vous connecter pour commencer à télécharger les caractéristiques MOSAIKS.\n\n\n\n\n\nFigure 3.3: Page d’accueil de l’API MOSAIKS.\n\n\n\n3.2.2 Ressources de l’API\nDepuis la page d’accueil, vous pouvez lire des informations supplémentaires sur MOSAIKS et accéder aux ressources pour vous aider à démarrer.\n\n\n\n\n\n\nCe livre est développé pour vous fournir toutes les informations nécessaires pour utiliser MOSAIKS.\n\n\n\nL’API contient les pages suivantes :\n\n\n\n\n\n\n\n\nPage\nDescription\n\n\n\nAccueil\nPage d’accueil de l’API. Contient des informations générales sur l’utilisation de MOSAIKS et l’API\n\n\nFichiers précalculés\nCaractéristiques précalculées aux échelles des limites administratives\n\n\nHDI\nEstimations globales de l’Indice de Développement Humain (IDH) aux niveaux municipalité et grille\n\n\nGrilles globales\nCaractéristiques précalculées et pondérées par surface ou population à résolution 0.1° et 1°\n\n\nRequête carte\nCaractéristiques précalculées à résolution 0.01 degré, l’utilisateur définit une zone d’intérêt\n\n\nRequête fichier\nCaractéristiques précalculées à résolution 0.01 degré, l’utilisateur télécharge un fichier avec coordonnées\n\n\nMes fichiers\nFichiers que vous avez demandés à l’API, disponibles au téléchargement\n\n\nRessources\nExemples de notebooks Python et R pour utiliser le framework MOSAIKS\n\n\n\n\n\nTable 3.2: Pages et descriptions de l’API MOSAIKS.\n\n\n\n3.2.3 Caractéristiques de l’API\nActuellement, l’API MOSAIKS dispose d’un seul ensemble de caractéristiques globales (avec plusieurs niveaux d’agrégation). Les caractéristiques sont disponibles gratuitement au public pour le téléchargement ; c’est la façon la plus rapide et la plus facile de commencer à utiliser MOSAIKS.\nLes caractéristiques de l’API utilisent des images d’entrée de Planet Labs, Inc. Visual Basemap Global Quarterly 2019 (trimestre 3). La qualité des images, et donc la qualité des caractéristiques, peut être affectée par les conditions locales. Par exemple, une zone en saison des pluies au troisième trimestre (juillet à septembre) est susceptible de contenir des artefacts d’image dus à la couverture nuageuse. Cela affectera à son tour les calculs des caractéristiques. Pour plus de détails sur l’imagerie d’entrée Chapter 9.\nÉtant donné la nature statique de l’API, la façon la plus simple de commencer avec MOSAIKS est d’avoir des données d’étiquettes pour une période récente (idéalement de 2019 pour les étiquettes changeant rapidement, ou une année proche pour les étiquettes plus stables).\n\n\n\n\n\nFigure 3.4: Imagerie de base Planet Labs (gauche) et 4 des 4 000 caractéristiques MOSAIKS téléchargées depuis l’API (droite).\n\n\n\n\n\n\n\n\nL’utilisation de MOSAIKS pour les données temporelles est possible et peut bien fonctionner, cependant cela nécessite actuellement de calculer vos propres caractéristiques personnalisées. Voir Chapter 14 et Chapter 18 pour plus d’informations.\n\n\n\nLes caractéristiques MOSAIKS sont créées en utilisant une grille terrestre globale de 0.01 x 0.01 degré de latitude-longitude. Ce sont les caractéristiques natives disponibles au téléchargement depuis l’API, mais elle offre également des caractéristiques pré-agrégées à résolution 0.1 et 1 degré, ainsi que des limites administratives (ADM0, ADM1 et ADM2).\n\n3.2.4 Caractéristiques haute résolution\nLa requête fichier et la requête carte sont les deux méthodes pour obtenir les caractéristiques haute résolution (0.01 degré) via l’API. Pour plus de simplicité, nous stockons ces caractéristiques dans un format tabulaire avec des coordonnées de latitude et longitude. Ces coordonnées sont le centre de chaque cellule de la grille.\nLorsque vous téléchargez les caractéristiques haute résolution (0.01 degré), vous les recevrez dans un format tabulaire .csv où :\n\nChaque ligne (N) représente une cellule de grille unique\nLes deux premières colonnes contiennent les coordonnées de latitude et longitude (centroïdes de la grille)\nLes colonnes restantes représentent K caractéristiques (actuellement K = 4000 caractéristiques)\n\n\nNote : Il y a une limite de N = 100 000 enregistrements par requête\n\n\n3.2.4.1 Requête carte\n\nCréez des boîtes rectangulaires en spécifiant les coordonnées de latitude et longitude\nPlusieurs boîtes peuvent être créées\nLe système affiche un nombre estimé d’enregistrements pour chaque boîte\nNotez que les estimations sont basées sur la surface de la boîte et peuvent ne pas refléter les nombres réels d’enregistrements, en particulier pour les zones contenant des mers et des océans\n\n\n\n\n\n\nFigure 3.5: Page de requête carte de l’API MOSAIKS.\n\n\n\n\n\n\n\n\nUtilisez geojson.io pour trouver les coordonnées de la boîte englobante pour votre zone d’intérêt.\n\n\n\n\n3.2.4.2 Requête fichier\n\nSoumettez un fichier avec des coordonnées de latitude et longitude personnalisées\nL’API renvoie les caractéristiques pour les cellules de grille les plus proches de vos coordonnées d’entrée\nLes points sont alloués au point de grille le plus proche s’ils ne correspondent pas exactement\nLe fichier de sortie peut avoir un nombre de lignes différent de votre entrée\nL’ordre des points peut changer dans la sortie\n\n\n\n\n\n\nFigure 3.6: Requête fichier API\n\n\n\n3.2.5 Caractéristiques agrégées\nDe nombreux utilisateurs peuvent trouver plus facile de travailler avec des caractéristiques agrégées à un certain niveau. L’API MOSAIKS offre des caractéristiques pré-agrégées pour répondre à ces besoins. L’API propose plusieurs niveaux, incluant des cellules de grille plus grandes (0.1 et 1 degré) ou résumées aux limites administratives (ADM0, ADM1 et ADM2). Ces fichiers sont disponibles au téléchargement en fichiers uniques ou fractionnés selon la résolution.\n\n\n\n\n\nFigure 3.7: Exemple montrant 3 caractéristiques convolutives aléatoires représentatives (lignes). Les caractéristiques sont téléchargées depuis l’API MOSAIKS à résolution 0.01° (la résolution native) et agrégées à 3 niveaux, incluant (A) des cellules de grille plus grandes (0.1°), (B) comtés, et (C) états.\n\n\n\n3.2.5.1 Schémas de pondération\nÀ chaque niveau d’agrégation, nous proposons des caractéristiques pondérées par surface et par population. Les poids de population proviennent du jeu de données de densité de population Gridded Population of the World (GPWv4). Le schéma de pondération par surface est basé sur la surface des cellules de grille haute résolution.\n\n3.2.5.2 Quand utiliser les caractéristiques agrégées\nLes caractéristiques agrégées sont particulièrement utiles dans quelques scénarios :\n\nVous avez des données à une échelle plus grande que les cellules de grille de 0.01 degré. De nombreux jeux de données sont au niveau pays, état ou comté.\nVos données ont beaucoup de bruit qui peut être lissé en agrégeant à une échelle plus grande. Un exemple courant pourrait être des données d’enquête ménage qui sont bruyantes au niveau individuel mais se lissent lorsqu’agrégées au niveau village ou district.\nVous voulez faire une analyse globale et n’avez pas les ressources informatiques pour travailler avec toutes les cellules de grille à 0.01 degré.\n\nDans tous les cas, l’utilisation des caractéristiques pré-agrégées peut vous faire gagner du temps et des ressources informatiques.\n\nScénario : Vous travaillez avec une Étude sur la Mesure des Niveaux de Vie - Enquêtes Intégrées sur l’Agriculture (LSMS-ISA). Ce jeu de données contient des données d’enquête avec des coordonnées géographiques au niveau du ménage. Pour protéger la confidentialité des répondants, les données sont perturbées dans un rayon de 5 km mais restent toujours dans les limites administratives locales. Vous pouvez donc résumer vos étiquettes aux unités administratives et construire un modèle avec les caractéristiques agrégées.\n\n\n\n\n\n\n\nSi vous voulez faire des prédictions haute résolution, avec des données d’étiquettes basse résolution, vous pouvez construire votre modèle avec des caractéristiques agrégées et utiliser les caractéristiques haute résolution pour faire des prédictions. Cela sera couvert dans Chapter 17.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Accès à MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#utilisation-des-fonctionnalités-mosaiks-pour-la-prédiction",
    "href": "part-00-intro/03-intro-api.html#utilisation-des-fonctionnalités-mosaiks-pour-la-prédiction",
    "title": "3  Accès à MOSAIKS",
    "section": "\n3.3 Utilisation des fonctionnalités MOSAIKS pour la prédiction",
    "text": "3.3 Utilisation des fonctionnalités MOSAIKS pour la prédiction\n\n\n\n\n\n\nCeci est un aperçu rapide. Des instructions détaillées sont disponibles ultérieurement dans le manuel (Chapter 16).\n\n\n\nFlux de travail de base :\n\nObtenir les mesures de référence (“labels” ; voir Chapter 5)\n\nTélécharger les caractéristiques correspondantes (voir Chapter 13 pour plus de détails).\n\nFusionner spatialement les labels et les caractéristiques (voir Chapter 7)\n\nUtiliser la régression pour modéliser la relation entre les images satellitaires et les résultats (voir Chapter 16)\n\nExploiter les résultats de la régression pour prédire les résultats dans de nouveaux emplacements (voir Chapter 16)\n\nVous pouvez expérimenter différentes approches d’apprentissage automatique lors de l’étape de régression. Pour les débutants, nous recommandons de commencer par notre notebook Jupyter d’exemple (Chapter 4), qui illustre une approche simple de régression ridge (convient aux utilisateurs de R et Python).\n\n\n\n\n\nFigure 3.8: Utilisation de MOSAIKS, un schéma simplifié du flux de travail.\n\n\nCe sujet sera exploré plus en détail dans les chapitres suivants (voir Chapter 16). Dans le prochain chapitre, vous découvrirez un flux de travail MOSAIKS simple qui reproduit les résultats de Rolf et al. 2021.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Accès à MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/03-intro-api.html#exigences-de-citation",
    "href": "part-00-intro/03-intro-api.html#exigences-de-citation",
    "title": "3  Accès à MOSAIKS",
    "section": "\n3.4 Exigences de citation",
    "text": "3.4 Exigences de citation\nLors de la référence à la méthodologie MOSAIKS ou lors de la génération de caractéristiques MOSAIKS, veuillez citer : Rolf et al. “A generalizable and accessible approach to machine learning with global satellite imagery.” Nature Communications (2021).\nVous pouvez utiliser la citation BibTeX suivante :\n@article{article,  \n    author = {Rolf, Esther and Proctor, Jonathan et Carleton, Tamma et Bolliger, Ian et Shankar, Vaishaal et Ishihara, Miyabi et Recht, Benjamin et Hsiang, Solomon},  \n    year = {2021},  \n    month = {07},  \n    pages = {},  \n    title = {A generalizable and accessible approach to machine learning with global satellite imagery},  \n    volume = {12},  \n    journal = {Nature Communications},  \n    doi = {10.1038/s41467-021-24638-z}  \n}  \nSi vous utilisez des caractéristiques téléchargées depuis l’API, veuillez citer, en plus de la publication ci-dessus, l’API MOSAIKS.\nVous pouvez citer l’API en utilisant la référence BibTeX suivante :\n @misc{MOSAIKS API,  \n    author = {{Carleton, Tamma et Chong, Trinetta et Druckenmiller, Hannah et Noda, Eugenio et Proctor, Jonathan et Rolf, Esther et Hsiang, Solomon}},  \n    title = {{Multi-Task Observation Using Satellite Imagery and Kitchen Sinks (MOSAIKS) API}},  \n    howpublished = \"\\url{ https://api.mosaiks.org }\",  \n    version = {1.0},  \n    year = {2022},  \n}  \n\n\n\n\n\n\nÀ venir\n\n\n\nDans le prochain chapitre, vous aurez l’opportunité de tester MOSAIKS sur Google Colab avec les données de Rolf et al. 2021.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Accès à MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html",
    "href": "part-00-intro/04-intro-demo.html",
    "title": "4  Essayez MOSAIKS",
    "section": "",
    "text": "4.1 Aperçu\nCette démonstration reproduit les résultats clés de la publication originale MOSAIKS (Rolf et al. 2021). Bien que MOSAIKS ait un grand potentiel pour améliorer l’accès aux prédictions par satellite dans les environnements pauvres en données, l’article original s’est concentré sur la démonstration des performances aux États-Unis où des données d’entraînement de haute qualité étaient facilement disponibles.\nLes États-Unis ont servi de terrain d’essai idéal pour plusieurs raisons :\nCette validation dans un environnement riche en données était cruciale pour établir MOSAIKS comme un outil fiable avant son déploiement dans des contextes où les données de référence sont rares ou peu fiables.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Essayez MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#aperçu",
    "href": "part-00-intro/04-intro-demo.html#aperçu",
    "title": "4  Essayez MOSAIKS",
    "section": "",
    "text": "Données de référence étendues disponibles pour plusieurs variables\nRéférencement spatial fiable des données\n\nPaysages et environnements bâtis diversifiés\nPossibilité de comparaison avec les méthodes existantes\nValidation systématique des prédictions",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Essayez MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#code-de-démonstration",
    "href": "part-00-intro/04-intro-demo.html#code-de-démonstration",
    "title": "4  Essayez MOSAIKS",
    "section": "\n4.2 Code de démonstration",
    "text": "4.2 Code de démonstration\n\n4.2.1 Flux de travail\nCi-dessous se trouve un lien vers un notebook Jupyter destiné à démontrer l’utilisation pratique de MOSAIKS avec des données réelles. Ce notebook utilise les données d’entrée et les caractéristiques originales de Rolf et al. 2021. Le code démontre :\n\nLe chargement des caractéristiques MOSAIKS pré-calculées et des étiquettes\nLa fusion des caractéristiques et des étiquettes\nL’entraînement d’un modèle de régression ridge\nL’évaluation des prédictions\nLa visualisation des résultats\n\n4.2.2 Données d’étiquettes\nLa démo présente MOSAIKS prédisant plusieurs variables, avec un sous-ensemble des données utilisées dans l’article original. Les variables incluent :\n\n\nCouverture forestière\nÉlévation\nDensité de population\nLumières nocturnes\nRevenus\nLongueur des routes\n\n\n\n\n\n\n\n\nFigure 4.1: Données d’entrée de couverture forestière (gauche) de Global Land Analysis & Discover (GLAD) Global 2010 Tree Cover (30 m)\n\n\n\n\n\n\n\n\n\nFigure 4.2: Données d’élévation (gauche) fournies par Mapzen et accessibles via le service AWS Terrain Tile. Le code de téléchargement est disponible ici.\n\n\n\n\n\n\n\n\n\nFigure 4.3: Données de densité de population (gauche) du jeu de données Gridded Population of the World (GPW). Ces données sont accessibles ici.\n\n\n\n\n\n\n\n\n\nFigure 4.4: Données de luminosité nocturne (gauche) générées à partir d’imagerie satellite nocturne, fournies par l’Earth Observations Group de la NOAA et le NGDC. Ces données sont accessibles ici.\n\n\n\n\n\n\n\n\n\nFigure 4.5: Données de revenus (gauche) de l’American Community Survey (ACS), estimations sur 5 ans du revenu médian des ménages en 2015. Ces données sont accessibles via le package acs dans R (48), table B19013\n\n\n\n\n\n\n\n\n\nFigure 4.6: Données de longueur des routes (gauche) de l’USGS National Transportation Dataset, basé sur les données TIGER/Line fournies par le US Census Bureau en 2016. Ces données sont accessibles ici.\n\n\n\n\n\nL’utilisateur doit simplement sélectionner la variable qu’il souhaite prédire, sans autre modification du code nécessaire. Toutes les données ont été prétraitées et le code téléchargera les fichiers nécessaires depuis Zenodo.\n\n4.2.3 Contraintes\nPour rester dans les limites de mémoire de la version gratuite de Colab, nous avons réduit les données. Nous prenons un échantillon aléatoire de 50% des caractéristiques (K=4 000 au lieu de 8 192) et des observations (N=50 000 au lieu de 100 000) par rapport à l’article original. Malgré l’utilisation de ce jeu de données réduit, la démo obtient toujours de bonnes performances prédictives, soulignant l’efficacité de MOSAIKS.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Essayez MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#exécutez-le-code",
    "href": "part-00-intro/04-intro-demo.html#exécutez-le-code",
    "title": "4  Essayez MOSAIKS",
    "section": "\n4.3 Exécutez le code !",
    "text": "4.3 Exécutez le code !\n\n\n\n\n\n\nCliquez sur le badge pour lancer la démonstration !\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\nN’oubliez pas de cliquer sur File -&gt; Save a copy in Drive pour sauvegarder vos modifications.\n\nOu pour voir une version statique du code sur GitHub, cliquez sur le badge ci-dessous.\n\n\nPour des instructions et conseils sur l’utilisation de Google Colab, veuillez consulter Chapter 1.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Essayez MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#vous-ne-voulez-pas-exécuter-le-code",
    "href": "part-00-intro/04-intro-demo.html#vous-ne-voulez-pas-exécuter-le-code",
    "title": "4  Essayez MOSAIKS",
    "section": "\n4.4 Vous ne voulez pas exécuter le code ?",
    "text": "4.4 Vous ne voulez pas exécuter le code ?\nRegardez plutôt cette démonstration !\n\n\n\n\n\nFigure 4.7: Un aperçu de MOSAIKS et une démonstration en direct de la génération de nouvelles prédictions avec le système. Vidéo enregistrée lors de la session CIGAR Generalized Planetary Remote Sensing - Convention 2020. Présentée par Esther Rolf et Tamma Carleton.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Essayez MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-00-intro/04-intro-demo.html#et-ensuite",
    "href": "part-00-intro/04-intro-demo.html#et-ensuite",
    "title": "4  Essayez MOSAIKS",
    "section": "\n4.5 Et ensuite ?",
    "text": "4.5 Et ensuite ?\nAprès avoir établi les capacités de MOSAIKS dans le contexte américain, l’équipe de développement a démontré avec succès le système dans de nombreux autres contextes. Cela inclut l’échelle mondiale ou des environnements avec peu ou de faibles données. Dans les chapitres suivants, nous explorerons certaines de ces applications, montrant comment MOSAIKS peut aider à combler les lacunes de données dans les régions où la collecte traditionnelle est difficile ou coûteuse.\n\n\n\n\n\n\nPour la suite\n\n\n\nDans la prochaine section, nous examinerons plus en détail les données d’étiquettes qui peuvent être utilisées avec MOSAIKS.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Essayez MOSAIKS</span>"
    ]
  },
  {
    "objectID": "part-01-labels/00-labels.html",
    "href": "part-01-labels/00-labels.html",
    "title": "Données d’étiquettes",
    "section": "",
    "text": "Vue d’ensemble\nCette section explore les données de vérité terrain (étiquettes) qui peuvent être utilisées pour entraîner un modèle prédictif avec MOSAIKS. Bien que le système soit conçu pour être flexible en ce qui concerne les types de résultats qu’il peut prédire, comprendre ce qui fait de bonnes données d’étiquettes et comment les préparer correctement est crucial pour la réussite.\nLes données d’étiquettes représentent la “vérité” que MOSAIKS tente de prédire - qu’il s’agisse de rendements agricoles, de densité de population, d’indicateurs économiques ou de toute autre variable qui pourrait être visible (directement ou indirectement) dans l’imagerie satellitaire. La qualité et les caractéristiques de ces données d’étiquettes influencent significativement les performances du modèle.",
    "crumbs": [
      "Données d'étiquettes"
    ]
  },
  {
    "objectID": "part-01-labels/00-labels.html#quest-ce-qui-fait-de-bonnes-données-détiquettes",
    "href": "part-01-labels/00-labels.html#quest-ce-qui-fait-de-bonnes-données-détiquettes",
    "title": "Données d’étiquettes",
    "section": "Qu’est-ce qui fait de bonnes données d’étiquettes ?",
    "text": "Qu’est-ce qui fait de bonnes données d’étiquettes ?\nPour des performances optimales avec MOSAIKS, les données d’étiquettes doivent avoir plusieurs caractéristiques clés :\n\nDes informations précises sur la localisation géographique\nUne résolution spatiale appropriée (typiquement ≥1km²)\nUn alignement temporel raisonnable avec les caractéristiques de l’imagerie\nUne taille d’échantillon suffisante (généralement ≥300 observations)\nUne connexion observable avec les caractéristiques de surface",
    "crumbs": [
      "Données d'étiquettes"
    ]
  },
  {
    "objectID": "part-01-labels/00-labels.html#plan-de-la-section",
    "href": "part-01-labels/00-labels.html#plan-de-la-section",
    "title": "Données d’étiquettes",
    "section": "Plan de la section",
    "text": "Plan de la section\nLes chapitres suivants vous guideront à travers les considérations clés pour travailler avec les données d’étiquettes dans MOSAIKS :\n\n\n\n\n\n\n\n\nChapitre\nSujets clés\n\n\n\n5  What labels work?\nApplications exemples, analyse des performances, validation\n\n\n6  Survey data\nIntégration d’enquêtes, conception d’échantillonnage, référencement géographique\n\n\n7  Preparing labels\nNettoyage des données, jointure spatiale, contrôle qualité\n\n\n8  Label data demo\nExemple pratique, flux de travail pratique, dépannage\n\n\n\n\n\nTable 1: Plan de la section sur les données d’étiquettes\n\n\nCes chapitres fournissent à la fois des conseils pratiques pour préparer vos propres données d’étiquettes et une compréhension plus approfondie des types de résultats que MOSAIKS peut efficacement prédire.\n\n\n\n\n\n\nPerspectives\n\n\n\nDans le prochain chapitre, nous explorerons plus de 100 résultats différents qui ont été testés avec MOSAIKS, en examinant ce qui fonctionne bien et ce qui ne fonctionne pas.",
    "crumbs": [
      "Données d'étiquettes"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html",
    "href": "part-01-labels/01-labels-100-maps.html",
    "title": "5  What labels work?",
    "section": "",
    "text": "5.1 Overview\nMOSAIKS is a designed to be useful for predicting anything that may be visible in satellite imagery. Some things are easier to predict than others, but the system is designed to be flexible. For instance, some variables can be seen directly in the imagery itself, such as forest cover, which clearly emits a visible green signal. Other outcomes can only be predicted through indirect relationships between imagery data and labels. For example, income and housing price are not themselves directly visible in raw imagery, but their values can be reliably estimated from objects (e.g., cars), textures (e.g., roofing material), and colors (e.g., grey road infrastructure) contained within the imagery.\nIn this chapter we will discuss a new working paper (Proctor et al., in prep.) that explores the question of what can and cannot be reliably predicted from satellite imagery using MOSAIKS. This paper investigates over 100 different outcomes, reporting predictive performance for each using MOSAIKS. We will discuss the results of a selection of these outcomes in detail, and provide a brief overview of the rest. Of course, this list of outcomes is not exhaustive and reported performance can differ substantially across contexts and in particular with varying quality of ground truth data. We show these results as an initial investigation of what might be possible with SIML using MOSAIKS, but encourage users to conduct their own experiments as resulst are likely to differ in new settings. The pre-print of Proctor et al. will be posted here when publicly available.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#overview",
    "href": "part-01-labels/01-labels-100-maps.html#overview",
    "title": "5  What labels work?",
    "section": "",
    "text": "Figure 5.1: MOSAIKS versatility makes it the perfect tool for a wide range of applications.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#one-hundred-maps",
    "href": "part-01-labels/01-labels-100-maps.html#one-hundred-maps",
    "title": "5  What labels work?",
    "section": "\n5.2 One hundred maps",
    "text": "5.2 One hundred maps\n\n5.2.1 Original publication\nIn Rolf et al. 2021, the authors tested MOSAIKS on 7 outcomes: forest cover, income, housing price, population density, nighttime luminosity, and elevation. The study area is focused in the continental United States. This is an excellent starting point for understanding the capabilities of MOSAIKS as the data quality is high for a diverse set of outcomes. While the results showed significant promise and demonstrated the potential of MOSAIKS, the true test is in the application to new outcomes and new geographies.\n\n5.2.2 Going global\nTo test the global applicability of MOSAIKS, across a diverse set of outcomes, there were 2 primary things that needed to happen:\n1. The creation of a global set of features\n\n\n\n\n\nFigure 5.2: Planet Labs visual basemap imagery from quarter 3 of 2019 (left) and 4 of 4,000 MOSAIKS features downloaded from the API (right).\n\n\n2. Collecting and curating a large set of outcomes with diversity in spatial structures and categories\n\n\n\n\n\n\n\n\n\nCategory\nNumber of Labels\nExample Label\n\n\n\nAgricultural Assets\n5\nAgricultural land ownership\n\n\nAgriculture\n16\nMaize yield\n\n\nBuilt Infrastructure\n9\nBuildings\n\n\nDemographics\n5\nMedian age\n\n\nEducation\n10\nExpected years of schooling\n\n\nHealth\n15\nMalaria in children\n\n\nHousehold Assets\n21\nMobile phones\n\n\nIncome\n9\nHuman development index\n\n\nNatural Systems\n8\nTree cover\n\n\nOccupation\n17\nUnemployment\n\n\n\n\n\nTable 5.1: The authors selected 115 variables across 10 categories and set to work testing each in the MOSAIKS system.\n\n\nWith this data in hand, they were able to devise a few simple questions to test:\n\nWhich variables can be effectively measured?\nWhat are the most compelling applications?\nWhat are the modes of failure?",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#results",
    "href": "part-01-labels/01-labels-100-maps.html#results",
    "title": "5  What labels work?",
    "section": "\n5.3 Results",
    "text": "5.3 Results\n\n5.3.1 Overall performance\nThe results of the 100 maps experiment are shown in the scatter plot below (Figure 5.3). Each point in each scatter sub-plot represents a location in the study for a given label. The x-axis is the observed value of the label, and the y-axis is the MOSAIKS-predicted value. The diagonal 45° line in each sub-plot represents perfect prediction. The coefficient of determination (R²) is used here as the primary measure of accuracy.\nA few broad insights stand out:\n\nSubstantial variation: Even within the same category, we see varying degrees of predictive power. For instance, in the “Agriculture” category, some labels (such as high-level yield averages) are predicted quite accurately, while others (like certain niche crops or management practices) remain more elusive.\nCategory differences: Some categories have consistently higher R² scores. For instance, “Natural Systems” (e.g., tree cover) often score better because the patterns are more directly visible from above—think of large, contiguous forest areas contrasted with open fields or urban centers. On the other hand, “Occupation” or “Demographics” include variables (like unemployment rates) that are largely socio-economic in nature, requiring more indirect and subtle cues.\nFailure cases: A few outcomes show near-random performance, suggesting that the satellite imagery features alone are insufficient to capture their spatial patterns, or that the signals are drowned out by noise (see Failures below).\n\n\n\n\n\n\nFigure 5.3: The results of the 100 maps experiment with the x-axis shows the observed values of the outcome, while the y-axis shows the predicted values. Each point in each scatter is a location from the study. The diagonal line (45°) represents perfect prediction. Performance is measured with the coefficient of determination (R²).\n\n\nIn the box plot (Figure 5.4) we see the distribution of R² values for each category across all 115 labels. This confirms the wide range of performance. Categories such as “Agriculture,” “Income,” and “Natural Systems” tend to have higher median R² values; categories such as “Health” and “Occupation” show more varied or lower overall performance.\n\n\n\n\n\nFigure 5.4: The results of the 100 maps experiment.\n\n\nThis heterogeneity underscores an important takeaway: MOSAIKS is not a one-size-fits-all solution. Some phenomena lend themselves to easier detection via satellite data than others. Still, the ability to simultaneously handle over 100 different outcomes from a single feature set is itself a testament to MOSAIKS’ flexibility and global applicability.\n\n5.3.2 Successes\n\n5.3.2.1 Maize yield\n\n5.3.2.1.1 Maize data\nThe maize yield data comes from crop yield data collected at the second administrative level (e.g., counties in the US) from the United States, China, Brazil and the European Union. Yields are calculated as harvested grain divided by the planted area, though in some cases harvested area is used instead of planted area. The data covers years from 1983-2009, with the most recent available year used for each location.\n\n5.3.2.1.2 Maize yield results\nA standout example of a high-performing label is Maize yield (Figure 5.5). This outcome is naturally suited to detection by satellite imagery:\n\n\nDirect visual signal: Agricultural fields have characteristic features, including crop texture, canopy cover, and phenological (growth stage) patterns, all of which can be captured in the spectral and spatial signals from satellite images.\n\n\n\nSpatial contiguity: Large, contiguous fields of maize reduce noise and enable easier extraction of relevant features.\n\nIn the left-hand scatter plot of Figure 5.5, the predicted yield values match well with the observed values, often clustering along the 45° line. On the right, we see that visually identifiable patterns in maize-growing regions are clearly reflected in the predicted maps. This strong alignment highlights how MOSAIKS can quickly yield robust predictions for outcomes that are clearly manifested in the satellite imagery.\n\n\n\n\n\nFigure 5.5: Performance of MOSAIKS on Maize yield, showing the observed values plotted against the model predictions (left). Observed label data is shown in the upper right, while the corresponding predictions are shown bottom right.\n\n\n\n5.3.2.1.3 Why it works\nCrop yields are a classic use case for remote sensing because farmland is often large, geographically dispersed, and subject to rapid changes from weather and management practices—conditions that satellite imagery can routinely monitor at scale. By measuring vegetation indices (e.g., NDVI, EVI), researchers gain insight into plant health, canopy density, and photosynthetic activity, all of which correlate strongly with agricultural productivity. This non-invasive, timely, and spatially comprehensive approach makes it invaluable for crop forecasting, detecting stress, and guiding resource allocation. Consequently, remote sensing has become a cornerstone in modern yield estimation methods for staple crops around the world. MOSAIKS is a natural extension of this trend, leveraging the latest in machine learning to extract actionable insights from satellite imagery.\n\n\n\n\n\nFigure 5.6: Agricultural fields in the United States Midwest region. This examples shows the clear delineation of fields with varying color intensities, making for easily detectable features in the satellite imagery. Source: NASA\n\n\n\n5.3.2.2 International wealth index (IWI)\n\n5.3.2.2.1 IWI data\nThe International Wealth Index data comes from the Demographic and Health Surveys (DHS) program. The index is expressed as a value between 0 and 100, with higher values indicating greater wealth. It is computed from household data on ownership of consumer durables, housing characteristics, and access to basic services like water and electricity. The data is processed and provided by the Global Data Lab with permission from DHS, with values averaged across households within each survey cluster. Survey clusters are displaced by up to 2km for urban areas and up to 5km for rural areas to protect privacy, while remaining within the same administrative boundaries.\n\n5.3.2.2.2 IWI results\nAnother notable success is the International Wealth Index (IWI; Figure 5.7). This composite measure of household wealth is derived from a variety of indicators, such as housing quality, access to services, and ownership of durable goods. While wealth itself is not directly visible from space, the underlying factors that contribute to it often are. For example, wealthier areas tend to have more developed infrastructure, larger homes, and more vehicles—all of which leave distinct signatures in satellite imagery.\n\n\n\n\n\nFigure 5.7: Performance of MOSAIKS on the International Wealth Index (IWI), showing the observed values plotted against the model predictions (left). Observed label data is shown in the upper right, while the corresponding predictions are shown bottom right.\n\n\n\n5.3.2.2.3 Why it works\nDespite being a composite measure of socioeconomic status, the IWI’s underlying indicators—housing conditions, access to utilities, and asset ownership—often manifest in the built environment as features that satellites capture well. For instance, wealthier neighborhoods typically exhibit a higher density of substantial buildings, paved roads, formal layouts, and visible amenities (e.g., swimming pools, parked vehicles). These cues translate into distinctive patterns in the spectral and spatial data extracted by MOSAIKS’ features. Furthermore, infrastructure development and housing materials (like metal roofs versus thatch) can produce detectable differences in reflectance, making it easier for the algorithm to discern socioeconomic gradients.\n\n\n\n\n\n\nThis highlights one of MOSAIKS’ core advantages: even when the target variable isn’t directly “visible,” the system can tease out its proxies from wide-ranging visual cues, leading to robust predictions of wealth indices around the globe.\n\n\n\n\n5.3.3 Failures\n\n5.3.3.1 Pipeline infrastructure\n\n5.3.3.1.1 Pipeline data\nThe pipeline data comes from the Energy Information Association (EIA) and includes interstate trunk lines and selected intrastate lines across three types: crude oil pipelines, hydrocarbon gas liquids (HGL) pipelines, and petroleum product pipelines. The data represents pipeline infrastructure as of January 2020 and covers the lower 48 United States. The variable measures the length in kilometers of each pipeline type within each grid cell.\n\n5.3.3.1.2 Pipeline results\nCertain labels show extremely low R² values, effectively indicating no predictive power under this approach. One notable example is the presence of underground pipelines Figure 5.8.\n\n\n\n\n\nFigure 5.8: Where it fails\n\n\n\n5.3.3.1.3 Why it fails\nUnlike forests or agricultural fields, pipeline infrastructure is typically hidden from direct visual inspection—often located entirely underground or obscured in ways that do not provide surface indicators distinguishable in imagery (Figure 5.9).\n\nLack of visible features: There is no spectral or structural cue (e.g., coloration, texture, shape) that reliably indicates the presence of a pipeline.\nIndirect clues Are unreliable: One might speculate that pipelines could follow roads or distinct corridors, but these correlations vary widely by region and do not consistently appear in the imagery.\nSignal-to-noise ratio: In many areas, the pipeline corridor may appear visually indistinguishable from farmland or other vegetation, leaving little to no unique satellite signature.\n\nAs a result, MOSAIKS has little chance to identify and learn features predictive of such hidden infrastructure. This stands in stark contrast to more visually prominent targets like maize fields or tree cover.\n\n\n\n\n\nFigure 5.9: Why it fails - buried\n\n\n\n5.3.3.2 Bee diversity\n\n5.3.3.2.1 Bee data\nThe bee diversity data comes from the US Geological Survey Eastern Ecological Science Center Native Bee Laboratory, which provides species occurrence records for native and non-native bees, wasps, and other insects collected through various trapping methods. Each record includes taxonomic identification and geographic coordinates. Using point data from 2009-2019, bee diversity is computed as a count of unique species documented within each 0.01° × 0.01° grid cell in North America (including U.S. territories and Minor Outlying islands). For cells with multiple sampling events, species are counted only once. Importantly, the database only includes records of species presence - absences are not recorded - which can lead to sampling bias in the diversity measures.\n\n5.3.3.2.2 Bee results\nAnother notable failure case is bee diversity. While bees play a crucial role in ecosystems and agriculture, their presence and diversity cannot be directly observed from satellite imagery. Several factors contribute to this limitation:\n\n\nScale mismatch: Bees operate at a much finer spatial scale than the resolution of typical satellite imagery\n\nIndirect relationships: While bees rely on vegetation, the link between plant cover visible from space and bee populations is complex and varies by context\n\nTemporal dynamics: Bee populations fluctuate seasonally and can change rapidly, while imagery captures only static snapshots\n\nHidden factors: Critical habitat features like nesting sites are often concealed under canopy or in small spaces invisible from above\n\nThis case illustrates an important principle: MOSAIKS works best when predicting outcomes that have consistent, visible relationships with surface features captured in satellite imagery. When those relationships become too indirect or complex, performance typically suffers.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#types-of-variables",
    "href": "part-01-labels/01-labels-100-maps.html#types-of-variables",
    "title": "5  What labels work?",
    "section": "\n5.4 Types of variables",
    "text": "5.4 Types of variables\nMOSAIKS can work with both continuous and categorical outcome variables, though the approach and evaluation metrics differ between these types.\n\n\n\n\n\n\nThis section provides a brief overview of the two types of variables and some of the metrics used to evaluate them. This will be covered in greater detail in Chapter 16.\n\n\n\n\n5.4.1 Continuous variables\nContinuous variables take numeric values along a spectrum, such as:\n\nCrop yields (e.g., tons per hectare)\nPopulation density (people per square kilometer)\nForest cover percentage (0-100%)\nIncome levels (dollars)\nBuilding height (meters)\n\nFor continuous variables, MOSAIKS typically uses regression approaches and evaluates performance using metrics like R² (coefficient of determination) or RMSE (root mean square error). The R² values reported throughout this chapter indicate the proportion of variance in the outcome that is explained by the MOSAIKS predictions.\n\n\n\n\n\nFigure 5.10: Continuous variable example showing the breeding bird species diversity over the continental United States\n\n\n\n5.4.2 Categorical variables\nCategorical variables group observations into distinct classes or categories, such as:\n\nLand use types (urban/agriculture/forest)\nBuilding types (residential/commercial/industrial)\nCrop types (maize/wheat/rice)\nPresence/absence of features (roads, buildings)\nDevelopment categories (low/medium/high)\n\nFor categorical variables, MOSAIKS can be used in two ways:\n\nBinary classification: For variables with two categories (e.g., presence/absence), MOSAIKS can output probability predictions between 0 and 1. Performance is typically evaluated using metrics like accuracy, precision, recall, or area under the receiver operating characteristic curve (AUC-ROC).\n\nMulti-class classification: For variables with multiple categories, MOSAIKS can either:\n\nUse a one-vs-all approach, treating each category as a separate binary classification problem\nOutput probabilities for each possible category that sum to 1\nConvert categories to numeric values if they have a natural ordering\n\n\n\n\n\n\n\n\nFigure 5.11: Classifier example showing Ecoregions of the world.\n\n\n\n5.4.3 Choosing appropriate metrics\nThe choice of evaluation metric should match the type of variable:\n\n\nVariable Type\nCommon Metrics\nInterpretation\n\n\n\nContinuous\nR², RMSE, MAE\nR² ranges 0-1, higher is better\n\n\nBinary\nAccuracy, AUC-ROC\nRange 0-1, higher is better\n\n\nMulti-class\nAccuracy, F1-score\nRange 0-1, higher is better",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/01-labels-100-maps.html#summary",
    "href": "part-01-labels/01-labels-100-maps.html#summary",
    "title": "5  What labels work?",
    "section": "\n5.5 Summary",
    "text": "5.5 Summary\nThis exploration of over 100 different outcomes reveals several key insights about MOSAIKS:\n\nPerformance varies significantly: Predictive power ranges from very strong (R² &gt; 0.8) to essentially random guessing, depending on the outcome\nDirect visibility matters: Outcomes that are directly observable in imagery (like forest cover) or have strong visible proxies (like wealth indices) tend to perform better\nCategory patterns: Some categories like Natural Systems and Agriculture show consistently stronger performance than others like Health or Demographics\n\nPractical implications: Understanding these patterns helps users:\n\nSet realistic expectations for new applications\nIdentify which types of outcomes are most suitable\nRecognize when alternative approaches might be needed\n\n\n\nThese results demonstrate both the power and limitations of MOSAIKS. While it excels at predicting many important outcomes globally, it is not a universal solution. Success depends largely on whether the outcome of interest has a meaningful relationship with features visible in satellite imagery.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll explore survey label data in more detail.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>What labels work?</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html",
    "href": "part-01-labels/02-labels-survey.html",
    "title": "6  Survey data",
    "section": "",
    "text": "6.1 Why does survey data needs its own chapter?\nNotes:\nSurvey data presents unique challenges and opportunities when working with MOSAIKS. Unlike many other data sources that may be consistently gathered through automated systems or administrative records, survey data:",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#why-does-survey-data-needs-its-own-chapter",
    "href": "part-01-labels/02-labels-survey.html#why-does-survey-data-needs-its-own-chapter",
    "title": "6  Survey data",
    "section": "",
    "text": "Captures detailed household and individual-level information that’s otherwise unobservable\nOften follows complex sampling designs that need special handling\nMay have inconsistent geographic referencing across different surveys\nRequires careful consideration of privacy and ethical concerns\nCan be expensive and time-consuming to collect, making validation of MOSAIKS predictions particularly valuable",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#types-of-survey-data",
    "href": "part-01-labels/02-labels-survey.html#types-of-survey-data",
    "title": "6  Survey data",
    "section": "\n6.2 Types of survey data",
    "text": "6.2 Types of survey data\nSeveral major categories of surveys are commonly used with MOSAIKS:\n\n6.2.1 Household surveys\n\nLiving Standards Measurement Study (LSMS)\nDemographic and Health Surveys (DHS)\nMultiple Indicator Cluster Surveys (MICS)\nLabor force surveys\nNational census data\n\n6.2.2 Agricultural surveys\n\nAgricultural censuses\nCrop cutting surveys\nFarm management surveys\nAgricultural household surveys\n\n6.2.3 Economic surveys\n\nEnterprise surveys\nMarket price surveys\nConsumer expenditure surveys",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#accessing-survey-data",
    "href": "part-01-labels/02-labels-survey.html#accessing-survey-data",
    "title": "6  Survey data",
    "section": "\n6.3 Accessing survey data",
    "text": "6.3 Accessing survey data\nSurvey data access varies by source and type:\n\n6.3.1 Public repositories\n\nWorld Bank Microdata Library\nIPUMS International\nDHS Program website\nFAO statistical databases\n\n6.3.2 National statistical offices\n\nCensus bureaus\nAgricultural ministries\nEconomic agencies\n\n6.3.3 Research institutions\n\nUniversities\nThink tanks\nResearch organizations",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#working-with-survey-data",
    "href": "part-01-labels/02-labels-survey.html#working-with-survey-data",
    "title": "6  Survey data",
    "section": "\n6.4 Working with survey data",
    "text": "6.4 Working with survey data\n\n6.4.1 LSMS data\nThe Living Standards Measurement Study (LSMS) requires specific considerations:\n\nComplex multi-topic household surveys\nDetailed geographic information\nPanel structure in some countries\nIntegration with agricultural data\nVarying spatial referencing methods\n\n6.4.2 DHS data\nThe Demographic and Health Surveys (DHS) present unique characteristics:\n\nStandardized across countries\nCluster-based sampling\nDisplaced GPS coordinates\nRich health and demographic indicators\nRegular update cycle",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/02-labels-survey.html#remote-sensing-informed-survey-design",
    "href": "part-01-labels/02-labels-survey.html#remote-sensing-informed-survey-design",
    "title": "6  Survey data",
    "section": "\n6.5 Remote sensing informed survey design",
    "text": "6.5 Remote sensing informed survey design\nMOSAIKS can enhance survey design in several ways:\n\n6.5.1 Pre-survey planning\n\nOptimize sampling frame using satellite-derived information\nIdentify areas of interest based on physical characteristics\nStratify sampling based on predicted characteristics\n\n6.5.2 During survey implementation\n\nValidate location information\nGuide field teams with up-to-date imagery\nMonitor survey progress\n\n6.5.3 Post-survey analysis\n\nValidate survey responses against satellite indicators\nFill data gaps in hard-to-reach areas\nCreate high-resolution predictions from survey samples\n\nExample of using MOSAIKS features for survey planning:\nThis integration of MOSAIKS with survey data represents a powerful approach for both enhancing traditional survey methods and extending their reach through satellite-based prediction.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll look at practical guidance for preparing label data for use with MOSAIKS, including data cleaning, aggregation, and joining to satellite features.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Survey data</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html",
    "href": "part-01-labels/03-labels-data-prep.html",
    "title": "7  Preparing labels",
    "section": "",
    "text": "7.1 Overview\nIn this chapter, we discuss the process of preparing labels for use with MOSAIKS features. Labels are the observed values we aim to predict with our model—such as crop yields, forest cover, or any variable of interest. They can come in many spatial formats (e.g., points, polygons, or gridded rasters), but they must include a spatial component. We use this spatial information to join the labels with MOSAIKS features, which are also spatially explicit.\nAlthough MOSAIKS offers many optional steps, two components are essential:\nBoth datasets must align in spatial resolution and contain appropriate geographic data for merging.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#overview",
    "href": "part-01-labels/03-labels-data-prep.html#overview",
    "title": "7  Preparing labels",
    "section": "",
    "text": "Ground observations (labels)\n\nSatellite features",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#mosaiks-grid",
    "href": "part-01-labels/03-labels-data-prep.html#mosaiks-grid",
    "title": "7  Preparing labels",
    "section": "\n7.2 MOSAIKS Grid",
    "text": "7.2 MOSAIKS Grid\nBefore we talk about labels, we must first understand the resolution that the MOSAIKS features are offered in. The resolution you choose will serve as the target resolution for your summarizing your labels.\n\n7.2.1 Resolution\nThe standard resolution of MOSAIKS is a global grid at 0.01° resolution. Each grid cell is approximately 1 km² at the equator. The grid is often represented as a point grid, where each point is the center of a grid cell. This means that standard MOSAIKS features come with a latitude and longitude coordinate, which is the center of the grid cell.\n\n\n\n\n\n\nMOSAIKS grid cell centroids are rounded to 0.005 degrees and are spaced by 0.01 degree (e.g., 10.005, 10.015, 10.025,…).\n\n\n\n\n\n\n\n\nFigure 7.1: Visual representation of a standardized grids at varying resolutions (δ) with the highest resolution on the left, and lower resolutions moving right. Source: Rolf et al. 2021 Figure 3 c.\n\n\n\n7.2.2 Advantages of the MOSAIKS grid\nThe MOSAIKS grid has several advantages. The primary advantage is that it helps avoid overlapping labels. Often label data come with coordinates which are unevenly spaced. If you are forced to align your labels to a grid and summarize within cells, you can avoid bleed over from one label to another. This is especially important when you are working with data that has a high degree of spatial autocorrelation.\n\n\n\n\n\n\nThe MOSAIKS API is designed to predict outcomes at scales of 1 km² or larger. Custom solutions are possible for higher-resolution applications (see Chapter 14).\n\n\n\n\n7.2.3 Disadvantages of the MOSAIKS grid\nThe MOSAIKS grid is defined in degrees, therefore the area of a given cell varies with latitude. Near the equator, each cell is approximately 1 km², while at higher latitudes the area decreases as the distance between meridians (longitude lines) converges.\n\n\n\n\n\nFigure 7.2: The area of a grid cell at different latitudes.\n\n\n\n7.2.4 Choosing your resolution\nWe know from Chapter 3 that the MOSAIKS API offers features at 0.01°, 0.1°, 1°, ADM2, ADM1, and ADM0 resolutions. In all cases, features are computed at 0.01° resolution and then aggregated to the desired resolution. If you plan on using the API to download features, you need to make sure your labels are at the same resolution as the features you plan to download.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#ground-observations",
    "href": "part-01-labels/03-labels-data-prep.html#ground-observations",
    "title": "7  Preparing labels",
    "section": "\n7.3 Ground Observations",
    "text": "7.3 Ground Observations\n\n7.3.1 Resolution\nPreparing label data for MOSAIKS depends on location, extent, time, and resolution. When observations have a resolution finer than 1 km², you must select or aggregate them to match the features you plan to use. MOSAIKS can incorporate labels from raster, point, polygon, or vector data. In Chapter 8, we will demonstrate how to prepare labels for each input type.\n\n\n\n\n\nFigure 7.3: Examples of label data formats that can be easily integrated into the MOSAIKS pipeline. Label data of any spatial format that can be aggregated to at least the scale of 1km² (or larger) can be used directly in combination with MOSAIKS imagery features for downstream prediction tasks. Examples shown here are from Rolf et al. (2021) and include: forest cover, elevation, population, and nighttime lights datasets (all raster format); income data (polygon format); road length (vector format); and housing price data (point format). Source: Rolf et al. 2021 Supplementary Figure 4.\n\n\n\n7.3.2 Administrative level data\nIn some cases, you will find labels for administrative regions (states, districts, etc.). In this case, it might not come with any geographic information other than a place name. You can usually find the relevant geographic information online. A good resource for this is the Global Administrative Areas (GADM) database, which provides the boundaries for administrative division at various levels for completely free.\n\n\n\n\n\nFigure 7.4: Global Human Development Index (HDI) data at the first sub-national level of administrative division (ADM1). Source: Smits & Permanyer 2019.\n\n\n\n7.3.3 Challenges of administrative data\nData can be messy. There are two main challenges with administrative data.\n\nName matching: The names in your dataset might not match perfectly with the names in the GADM database or in the MOSAIKS API. Finding a way to comprehensively match these can be time consuming and difficult.\nBoundary changes: Administrative boundaries are not always static. Some regions undergo frequent changes and you need to ensure your data matches the boundaries of the features you use.\n\n7.3.4 Sample Size\nIncreasing sample size (N) often yields higher model performance. MOSAIKS has demonstrated effectiveness across a wide range of sample N. The sample size depends on the spatial (and potentially temporal) resolution of your label data. For instance, if each record is aggregated at the county level, then N equals the number of counties. Incorporating multiple time periods can increase N but also requires more imagery to be featurized (see Chapter 14).\n\n\n\n\n\n\nAs a general rule, a minimum of 300 observations for model training is recommended, though every application is unique and may require more or fewer observations.\n\n\n\nThe original MOSAIKS publication (Rolf et al., 2021) evaluated models with sample sizes from 60,000 to 100,000. In most cases there were only modest performance declines with a few hundred observations (Figure 7.5). In recent crop yield experiments, high performance (R² = 0.80) was achieved with around 400 observations, provided the data were cleaned and aggregated to a district level. It is important to note that the original crop yield dataset included interview records from thousands of farmers across the study country. While this data has a large sample size, it is messy. In this case, a clean dataset with a low number of observations was preferred to a large but noisy dataset.\n\n\n\n\n\nFigure 7.5: Model performance for all seven tasks while varying the number of random convolutional features K and holding N = 64,000 (left) and while varying N and holding K = 8,192 (right). Shaded bands indicate the range of predictive skill across five folds. Lines indicate average accuracy across validation folds. Source: Rolf et al. 2021 Figure 3 b.\n\n\n\n7.3.5 Data Types\nMOSAIKS can accommodate both continuous labels (e.g., fraction of area forested) and discrete labels (e.g., presence/absence of mine). Data type informs model development, performance evaluation, and choice of metric (see Chapter 16). Continuous variables often use coefficient of determination (R²), while discrete variables often use Receiver Operating Characteristic Area Under the Curve (ROC AUC).\n\n\n\n\n\nFigure 7.6: A The coefficient of determination (R²) is a measure of how well the model fits the data. It ranges from 0 to 1, where 1 indicates a perfect fit. The closer each point is to the 45 degree line, the higher the better the model fit and the higher the score will be. B The Receiver Operating Characteristic (ROC) curve is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1-specificity). The area under the curve (AUC) ranges from 0 to 1, where 1 indicates a perfect model. Diagonal line is equivalent to random guessing.\n\n\nThe data type may also effect how the data is cleaned and prepared. For example you may have a dataset of mining locations across a country. If you are interested in predicting the presence of mining, you may want to convert this to a binary variable where a 0 indicates no mining and a 1 indicates mining. Alternatively, you may be interested in predicting the area of mining in each location, in which case you might need to calculate the area of the mining polygons to make the variable continuous.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#joining-data",
    "href": "part-01-labels/03-labels-data-prep.html#joining-data",
    "title": "7  Preparing labels",
    "section": "\n7.4 Joining Data",
    "text": "7.4 Joining Data\n\n7.4.1 Merging Labels with MOSAIKS Features\nTo merge labels with features, you must align the geographic location of both datasets. The native resolution MOSAIKS feature files have rows for each location and columns with longitude, latitude, and features. Your label dataset must also have columns for longitude, latitude, and label values (at minimum). Alternatively, aggregated features will come with the name of the administrative region (e.g., district) and the features. You can join this with your label data by matching the district names.\n\n7.4.2 Example Data Structure\nA simple example: district-level crop yield labels might look like:\n\n\n\n\nObservation\nDistrict\nYear\nCrop Yield\n\n\n\n1\nChibombo\n2019\n1.520\n\n\n2\nKabwe\n2019\n1.878\n\n\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n2.383\n\n\n\n\n\nTable 7.1: Fictional crop yield data for districts in Zambia.\n\n\nMOSAIKS features at this same resolution might look like:\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nDistrict\nYear\nFeature 1\nFeature 2\n…\nFeature K\n\n\n\n\n1\nChibombo\n2019\n4.2\n11.6\n…\n12.7\n\n\n2\nKabwe\n2019\n2.9\n5.3\n…\n11.2\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n10.6\n1.1\n…\n2.2\n\n\n\n\n\nTable 7.2: Fictional MOSAIKS feature data for the same districts.\n\n\nAfter a spatial join, you end up with a merged dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nDistrict\nYear\nCrop Yield\nFeature 1\nFeature 2\n…\nFeature K\n\n\n\n\n1\nChibombo\n2019\n1.520\n4.2\n11.6\n…\n12.7\n\n\n2\nKabwe\n2019\n1.878\n2.9\n5.3\n…\n11.2\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nN\nKitwe\n2019\n2.383\n10.6\n1.1\n…\n2.2\n\n\n\n\n\nTable 7.3: Example of joined data with both labels and features.\n\n\nIn the above example, our geographic location is the district and our label is the crop yield (mt/ha). We then have K columns containing the features and N observations.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#data-cleaning-considerations",
    "href": "part-01-labels/03-labels-data-prep.html#data-cleaning-considerations",
    "title": "7  Preparing labels",
    "section": "\n7.5 Data Cleaning Considerations",
    "text": "7.5 Data Cleaning Considerations\n\n7.5.1 Coordinate Reference Systems (CRS)\nThe default coordinate reference system used by MOSAIKS is World Geodetic System 84 (WGS 84). The standardized code that defines WGS 84 is “EPSG:4326”. EPSG stands for the European Petroleum Survey Group, which maintains a database of coordinate systems and projections. The WGS 84 coordinate system is the most commonly used coordinate system for GPS data.\n\n\n\n\n\nFigure 7.7: Nine small-scale map projections.\n\n\nIf your data is not in WGS 84, you will need to reproject it to this coordinate system before joining it with MOSAIKS features.\n\n7.5.2 Label Quality\nConfirm that label values are within expected ranges, deal with any outliers or missing data, and ensure units are consistent and numeric fields are properly formatted. This book does not go into a great deal of detail on cleaning messy data. This topic is covered in exhaustive detail in the book R for Data Science.\n\n7.5.3 Temporal Alignment\nIf you have time series labels, you will need to compute custom features for each time period. See Chapter 14 for more information on how to do this and ?sec-model-temporal for guidance on modeling time series data with MOSAIKS features.\n\n7.5.4 Data Formats\nMOSAIKS can work with several common spatial data formats:\n\n\n\n\n\n\n\n\n\n\n\nData Type\nCommon File Formats\nDescription\nExample\nGeographic information\n\n\n\nPoint Data\nCSV, GeoJSON, Shapefile\nCoordinate pairs\nPlaces of interest\ngeometry\n\n\nLine Data\nGeoJSON, Shapefile\nGeographic lines\nRoads, rivers\ngeometry\n\n\nPolygon\nGeoJSON, Shapefile\nGeographic areas\nBuildings, fields\ngeometry\n\n\nRaster\nGeoTIFF, NetCDF\nGridded data\nForest cover, elevation\ngrid\n\n\nAdministrative\nCSV\nAdministrative boundaries\nDistricts, states\nplace names\n\n\n\n\n\nTable 7.4: Common spatial data formats with file types, descriptions, examples, and indication of how the spatial information is stored.\n\n\n\n\n\n\n\n\nWhen working with large datasets, converting to efficient data formats such as Parquet, Feather, GeoTIFF, or Zarr can reduce memory usage and improve processing speed.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/03-labels-data-prep.html#summary",
    "href": "part-01-labels/03-labels-data-prep.html#summary",
    "title": "7  Preparing labels",
    "section": "\n7.6 Summary",
    "text": "7.6 Summary\nThe most important considerations when preparing labels for MOSAIKS are spatial resolution, sample size, data type, and data quality. Once you have prepared your labels, you can join them with MOSAIKS features to create a dataset ready for modeling.\nA demonstration of how to process data for the data types in Table 7.4 is provided in the next chapter. This notebook will cover the process of creating a MOSAIKS grid, downloading data, and creating a label dataset at the grid level (0.01 degree) and at the second level of administrative division (ADM2).\n\n7.6.1 Labels data checklist\nFor optimal use with MOSAIKS, label data should be:\n\nConsistently geolocated as point, polygon, vector, or raster data\nAggregable to ≥ 1km² resolution\nObservable in daytime satellite imagery (directly or indirectly)\nRecent or slow-changing if using current API features\nSample size N≥300 (larger samples generally perform better)\nCleaned and formatted for modeling\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll look at practical guidance for preparing label data for use with MOSAIKS including data cleaning and aggregation.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Preparing labels</span>"
    ]
  },
  {
    "objectID": "part-01-labels/04-labels-demo.html",
    "href": "part-01-labels/04-labels-demo.html",
    "title": "8  Label data demo",
    "section": "",
    "text": "8.1 Overview\nThis demonstration will show you a few key concepts about label data in MOSAIKS.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Label data demo</span>"
    ]
  },
  {
    "objectID": "part-01-labels/04-labels-demo.html#demonstration-code",
    "href": "part-01-labels/04-labels-demo.html#demonstration-code",
    "title": "8  Label data demo",
    "section": "\n8.2 Demonstration code",
    "text": "8.2 Demonstration code\nBelow is a link to a Jupyter notebook intended to demonstrate practical preparation of label data for use in MOSAIKS. The notebook will guide you through the process of preparing a dataset for use in MOSAIKS, including:\n\nBuilding and use a MOSAIKS standardized grid\nPreparing geographic point labels (latitude and longitude coordinates)\nPreparing geographic line labels (e.g. shapefiles)\nPreparing geographic polygon labels (e.g. shapefiles)\nPreparing raster labels (e.g. GeoTIFFs)\n\n\n\n\n\n\n\nClick the badge to run the demonstration!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Remember to click File -&gt; Save a copy in Drive to save any changes you make.\n\nOr to view a static version of the code on GitHub, click the badge below.\n\n\nFor instructions and tips on using Google Colab, please refer to Chapter 1.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Label data demo</span>"
    ]
  },
  {
    "objectID": "part-01-labels/04-labels-demo.html#whats-next",
    "href": "part-01-labels/04-labels-demo.html#whats-next",
    "title": "8  Label data demo",
    "section": "\n8.3 What’s next?",
    "text": "8.3 What’s next?\nNow that we have covered the basics of label data, we will move on to the next section, where we will discuss considerations for choosing and processing satellite imagery. This section is not necessary for users who are either a) okay with using the MOSAIKS API to access features, or b) already have experience working with satellite imagery.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next part, we will take a look at considerations for choosing and processing satellite imagery.",
    "crumbs": [
      "Données d'étiquettes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Label data demo</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html",
    "href": "part-02-satellite/00-satellite.html",
    "title": "Satellite imagery",
    "section": "",
    "text": "Overview\nThis section of the book aims to help navigate the complex world of satellite imagery, with a focus on applying these data within the MOSAIKS framework. While MOSAIKS API features are sufficient for many applications, some use cases require working directly with satellite imagery. Understanding how to select and process appropriate imagery becomes crucial in these situations.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#overview",
    "href": "part-02-satellite/00-satellite.html#overview",
    "title": "Satellite imagery",
    "section": "",
    "text": "The skills covered in these chapters are primarily relevant for users who need to generate custom MOSAIKS features. If you plan to use the MOSAIKS API features, you can skip this section.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#when-to-look-beyond-the-mosaiks-api",
    "href": "part-02-satellite/00-satellite.html#when-to-look-beyond-the-mosaiks-api",
    "title": "Satellite imagery",
    "section": "When to look beyond the MOSAIKS API",
    "text": "When to look beyond the MOSAIKS API\nThe MOSAIKS API provides pre-computed features from 2019 Planet Labs imagery. While these features enable a wide range of applications, you may need to work directly with satellite imagery when:\n\nYour analysis requires data from a different time period\nYou need higher spatial or temporal resolution\nYour application requires specific spectral bands\nYou want to validate or compare MOSAIKS features\nYou’re developing new methodologies",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#earth-observation-satellites-past-present-and-future",
    "href": "part-02-satellite/00-satellite.html#earth-observation-satellites-past-present-and-future",
    "title": "Satellite imagery",
    "section": "Earth observation satellites: Past, present and future",
    "text": "Earth observation satellites: Past, present and future\nSatellite-based Earth observation has revolutionized our understanding of the planet since the launch of the first Landsat satellite in 1972. Today, hundreds of Earth observation satellites collect terabytes of imagery daily, supporting applications from weather forecasting to precision agriculture.\nThe variety of available satellite data has grown dramatically, with options including:\n\nFree public satellites (Landsat, Sentinel)\nCommercial providers (Planet Labs, Maxar)\nSpecialized sensors (radar, hyperspectral)\nSatellite constellations providing frequent coverage\nVery high resolution imagery (&lt;1m pixels)\n\nThis wealth of options creates both opportunities and challenges in selecting appropriate imagery for your specific needs.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/00-satellite.html#section-outline",
    "href": "part-02-satellite/00-satellite.html#section-outline",
    "title": "Satellite imagery",
    "section": "Section outline",
    "text": "Section outline\nThe following chapters will guide you through key considerations when working with satellite imagery:\n\n\n\n\n\n\n\n\nChapter\nKey Topics\n\n\n\n9  Choosing imagery\nResolution types, data sources, selection criteria\n\n\n10  Processing imagery\nPre-processing steps, quality control, computing needs\n\n\n\n\n\nTable 1: Outline of the satellite imagery section\n\n\nThese chapters provide practical guidance for incorporating satellite imagery into your MOSAIKS workflow while highlighting important technical and logistical considerations.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we will take a look at how to choose the right satellite imagery for your application.",
    "crumbs": [
      "Satellite imagery"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html",
    "href": "part-02-satellite/01-satellite-imagery.html",
    "title": "9  Choosing imagery",
    "section": "",
    "text": "9.1 Background\nThe number of Earth observation satellites has grown exponentially in recent years, driven by advancements in technology, reduced launch costs, and an increasing demand for environmental, economic, and societal insights.\nAlongside this growth, the quality of satellite imagery has improved significantly, with higher spatial, temporal, and spectral resolutions becoming the norm. The proliferation of satellites, coupled with enhanced imaging capabilities, has resulted in an unprecedented volume of data, offering researchers and practitioners a wealth of opportunities—but also presenting challenges in navigating the diversity of data products and selecting those most suited to specific applications.\nMOSAIKS leverages satellite imagery to generate high-quality training data for machine learning models. In this chapter, we explore the key factors to consider when choosing satellite imagery for use with MOSAIKS, including spatial and spectral resolution, temporal frequency, cloud cover, and processing levels.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#background",
    "href": "part-02-satellite/01-satellite-imagery.html#background",
    "title": "9  Choosing imagery",
    "section": "",
    "text": "Figure 9.1: Growth in number of satellites over recent years. Source: Union of Concerned Scientists\n\n\n\n\n\n\n\n\nFigure 9.2: Growth in satellite iamge resolution over time. Source: FlyPix AI\n\n\n\n\n\n\n\n\n\nAccess the UCS Satellite Database for a comprehensive list of Earth observation satellites, including key specifications and operational details.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#overview",
    "href": "part-02-satellite/01-satellite-imagery.html#overview",
    "title": "9  Choosing imagery",
    "section": "\n9.2 Overview",
    "text": "9.2 Overview\nWhen selecting satellite imagery for use with MOSAIKS, several key factors need to be considered. The choice of imagery should be guided by your specific research needs, label characteristics, and practical constraints like cost and processing requirements. Different use cases, such as monitoring vegetation, mapping urban areas, or tracking climatic variables, may require specific imaging capabilities, making it essential to align satellite features with your intended application. Additionally, trade-offs between resolution, temporal frequency, and accessibility can significantly influence the feasibility of your analysis.\nFor example, high-resolution data from commercial satellites might offer detailed insights but could be prohibitively expensive for large-scale applications. On the other hand, publicly available datasets like Sentinel-2 provide cost-effective options, albeit with potentially coarser resolutions or limited spectral coverage. Understanding these trade-offs is essential to ensure the optimal use of resources.\n\n\n\n\n\n\n\n\nConsideration\nDescription\n\n\n\nAvailability\nThe availability and cost of imagery from public and private sources.\n\n\nTime range\nThe time period for which satellite images are available.\n\n\nTemporal resolution\nThe frequency at which satellite images are captured (e.g., daily, weekly).\n\n\nSpatial resolution\nThe spatial resolution of the satellite sensor (e.g., 10m, 30m, 250m).\n\n\nSpectral resolution\nThe specific wavelengths captured by the sensor (e.g., RGB, NIR, SWIR).\n\n\nSensor type\nThe type of sensor used (e.g., optical, radar, thermal).\n\n\nCloud cover\nCloud cover or other atmospheric interference.\n\n\nProcessing level\nThe preprocessing applied (e.g., orthorectification, normalization).\n\n\n\n\n\nTable 9.1: Based on your labels, a user needs to decide on these key factors. Each consideration will be covered in detail in the following sections.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#availability",
    "href": "part-02-satellite/01-satellite-imagery.html#availability",
    "title": "9  Choosing imagery",
    "section": "\n9.3 Availability",
    "text": "9.3 Availability\n\n9.3.1 Public Satellites\nPublic satellite programs offer free or low-cost imagery with global coverage. These programs, funded by government agencies, provide extensive data for scientific research and environmental monitoring. They are particularly valuable for long-term studies, thanks to consistent data archives spanning decades, and for projects with limited budgets. Publicly funded satellites often balance moderate resolutions with frequent revisit times, ensuring data accessibility and temporal consistency.\n\n\n\n\n\n\n\n\n\n\n\nSatellite\nSensor Type\nResolution\nRevisit Time\nSpecial Features\n\n\n\nSentinel-2\nOptical\n10-60m\n5 days\n13 spectral bands\n\n\nLandsat 8/9\nOptical\n15-100m\n16 days\n11 spectral bands\n\n\nMODIS\nOptical\n250m-1km\nDaily\n36 spectral bands\n\n\nSentinel-1\nRadar (C-band)\n5-40m\n6-12 days\nSynthetic Aperture Radar (SAR)\n\n\nNISAR\nRadar (L- and S-band)\nTBD\nLaunching 2024\nDual-band SAR\n\n\nVIIRS\nOther\n375m-750m\nDaily\nDaily global coverage\n\n\nASTER\nOther\n15-90m\n16 days\n14 spectral bands\n\n\nNICFI\nOptical\n4.77 m\nmonthly composite\nTropical forest monitoring\n\n\n\n\n\nTable 9.2: A selection of public satellite programs and their key specifications.\n\n\n\n9.3.2 Private Satellites\nPrivate satellite programs often provide higher resolution imagery, greater specialization, and more frequent revisits than public programs, but typically at a higher cost. These satellites are particularly useful for applications requiring fine detail, such as urban mapping, infrastructure monitoring, or precision agriculture. Private providers often offer custom data products, tailored delivery timelines, and advanced analytics, which can be invaluable for time-sensitive or commercially driven projects. However, the higher costs associated with private imagery may limit their use for large-scale or long-term studies without significant funding.\n\n\n\n\n\n\n\n\n\n\n\nSatellite\nSensor Type\nResolution\nRevisit Time\nSpecial Features\n\n\n\nMaxar WorldView\nOptical\n31cm (panchromatic), 1.24m (multispectral)\nVaries\nVery high resolution\n\n\nPlanet SkySat\nOptical\n50cm (panchromatic), 1m (multispectral)\nVaries\nCompact, agile satellites\n\n\nAirbus Pleiades\nOptical\n50cm (panchromatic), 2m (multispectral)\nVaries\nVery high resolution\n\n\nPlanet PlanetScope\nOptical\n3-5m\nDaily\nLarge constellation\n\n\nPlanet RapidEye\nOptical\n5m\n5.5 days\nDesigned for agriculture\n\n\nSPOT\nOptical\n1.5-6m\n26 days\nLong-standing program\n\n\nICEYE\nRadar (X-band)\n&lt;1m\nFrequent\nHigh revisit SAR\n\n\nCapella Space\nRadar (X-band)\n50cm\nFrequent\nHigh resolution SAR\n\n\nGHGSat\nSpecialized\nTBD\nVaries\nGreenhouse gas monitoring\n\n\n\n\n\nTable 9.3: A selection of private satellite programs and their key specifications.\n\n\n\n9.3.3 Cost considerations\n\n\n\n\n\n\nRemember that the most expensive or highest resolution imagery isn’t always the best choice. Balancing your needs with practical constraints—like budget, storage capacity, and processing expertise—can often yield better long-term outcomes than simply opting for the highest-specification option.\n\n\n\nSelecting satellite imagery involves not only the direct expense of acquiring data but also a range of indirect or “hidden” costs. When factoring these into your overall budget, consider how frequently you need updates, the extent of pre- or post-processing required, and whether your team has the technical skills to work with complex datasets. Some satellites and data portals also bundle analytics and cloud storage into subscription services, which can alleviate infrastructure demands at an additional cost.\nBelow is an example of how imagery costs might break down, offering a quick reference for typical price points and considerations across different data sources:\n\n\n\n\n\n\n\n\n\n\nCost Tier\nApproximate Range\nExample Missions/Programs\nKey Considerations\n\n\n\nFree Public Data\n$0\nLandsat (30m), Sentinel (10m), MODIS (250m–1km), VIIRS (375m–750m)\nGovernment-funded; widely used for large-scale or long-term projects; moderate resolution.\n\n\nMedium-Resolution\n$1–5 per km²\nSPOT (1.5–6m)\nRelatively affordable for moderate detail; suitable for regional analyses.\n\n\nHigh-Resolution\n$5–15 per km²\nPlanet (3–5m), RapidEye (5m)\nUseful for more detailed studies; frequent revisit can drive up cost if coverage is large.\n\n\nVery High Resolution\n$15–25 per km²\nWorldView (sub-meter), SkySat (0.5–1m)\nOffers fine-grained detail but at a premium; cost can escalate quickly for large areas.\n\n\nSubscription Services\nVariable\nSome commercial data providers bundle storage & analytics\nOften convenient “one-stop shop,” but pricing may be unpredictable or scale poorly.\n\n\n\n\n\nTable 9.4: Cost tiers for satellite imagery and key considerations.\n\n\nIn addition to these basic acquisition costs, you should also consider:\n\n\nData Storage and Computing: Large volumes of high-resolution imagery require greater storage space and more computational power for processing.\n\n\nExpertise and Training: Interpreting complex datasets (such as radar or hyperspectral imagery) often demands specialized skills.\n\n\nSoftware Licenses: Proprietary GIS or image processing software can add to your operational budget.\n\n\nCloud Infrastructure: Commercial cloud solutions can handle large-scale processing but may entail ongoing subscription fees.\n\nCarefully weighing these direct and indirect expenses will help you choose imagery that optimizes both scientific value and cost-effectiveness.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#time-range",
    "href": "part-02-satellite/01-satellite-imagery.html#time-range",
    "title": "9  Choosing imagery",
    "section": "\n9.4 Time range",
    "text": "9.4 Time range\nThe operational lifespan and launch history of a satellite mission are essential considerations when selecting imagery for your project. Older programs such as Landsat offer data archives spanning several decades, making them valuable for long-term change detection or historical analyses. In contrast, newer satellites like Sentinel, launched in 2014, provide data with more advanced sensor technologies but cover a shorter temporal window.\nWhen deciding on a time range, it is important to assess the historical depth your study demands, as well as the consistency of sensor technology over that period. Projects that require comparisons across many years benefit from missions with well-documented calibration procedures and stable sensor performance over time. You may also need to factor in whether future data continuity is guaranteed by planned satellite launches or extended operational budgets. For instance, Landsat’s multi-generational program ensures a relatively consistent data record, while newer constellations might offer superior capabilities but face funding uncertainties that could impact long-term availability.\n\n\n\n\n\nFigure 9.3: The Landsat satellite constellation service timeline. Source: NASA",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#temporal-resolution",
    "href": "part-02-satellite/01-satellite-imagery.html#temporal-resolution",
    "title": "9  Choosing imagery",
    "section": "\n9.5 Temporal resolution",
    "text": "9.5 Temporal resolution\nTemporal resolution, or revisit frequency, dictates how often a satellite images the same location. Higher revisit rates allow you to monitor rapidly changing phenomena, such as vegetation growth cycles or flood dynamics, and respond more quickly to evolving events. However, satellites that pass over more frequently often trade off spatial or spectral detail.\nWhen selecting temporal resolution, consider the pace at which your target variables change. Monitoring daily fluctuations in crop conditions requires denser time series than an annual assessment of deforestation. Cloud conditions in your study area can also influence the effective revisit rate: if clouds frequently obscure the land surface, you may need a satellite constellation with multiple imaging opportunities or a sensor that can “see” through clouds, like radar (SAR). Budget and data processing capacities further shape your decision, since frequent imaging can lead to increased data storage needs and processing overhead.\n\n\n\n\n\nFigure 9.4: Comparison of temporal resolution from different satellite systems. Source: Radiant Earth",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#spatial-resolution",
    "href": "part-02-satellite/01-satellite-imagery.html#spatial-resolution",
    "title": "9  Choosing imagery",
    "section": "\n9.6 Spatial resolution",
    "text": "9.6 Spatial resolution\nSpatial resolution of the satellite sensor determines the level of detail captured in satellite imagery. At higher resolutions, features such as individual trees, vehicles, or small buildings are discernible. Lower resolutions provide broader overviews of landscapes—suitable for regional-scale studies like climate analysis or large-area land cover mapping—but may miss fine-grained changes in smaller features.\nAlthough very high resolution imagery (below one meter) can yield rich information, it often carries higher costs and storage requirements. Meanwhile, moderate resolutions (10–30 m) balance meaningful detail with affordability and manageable data volumes. Your choice should account for the physical size of the features relevant to your labels, the scale of your study area, and the computational resources at your disposal. In many cases, public missions like Sentinel-2 or Landsat offer practical resolutions (10–30 m) that suffice for broad-scale analyses without incurring the expense associated with commercial high-resolution imagery.\n\n\n\n\n\nFigure 9.5: The spatial resolution of satellite imagery from 3 public satellites (top row) and 3 commercial satellites (bottom row). Source: Radiant Earth",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#spectral-resolution",
    "href": "part-02-satellite/01-satellite-imagery.html#spectral-resolution",
    "title": "9  Choosing imagery",
    "section": "\n9.7 Spectral resolution",
    "text": "9.7 Spectral resolution\nSpectral resolution refers to the sensor’s ability to measure discrete wavelengths across the electromagnetic spectrum. Different materials—such as vegetation, water, and urban infrastructure—exhibit distinct spectral signatures, so capturing multiple bands can enhance classification accuracy and thematic mapping. For instance, near-infrared (NIR) bands are extremely valuable for quantifying vegetation health, while shortwave infrared (SWIR) can help distinguish between minerals and moisture content.\nChoosing an appropriate spectral resolution involves balancing the number and width of bands with the specific thematic information you need. Sensors that capture only visible light (RGB) may suffice for broad land cover discrimination, whereas applications such as agricultural health monitoring often benefit from additional NIR or SWIR coverage. Thermal or microwave bands can also be critical in specialized domains—like thermal anomaly detection or soil moisture estimation—and thus influence the utility of certain satellite platforms for specific scientific or operational goals.\n\n\nSpectral resolution and its applications. Source: Radiant Earth",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#sensor-type",
    "href": "part-02-satellite/01-satellite-imagery.html#sensor-type",
    "title": "9  Choosing imagery",
    "section": "\n9.8 Sensor type",
    "text": "9.8 Sensor type\nSatellite sensors can generally be characterized as passive or active instruments. Both categories have distinct advantages and limitations that influence their suitability for particular applications.\nPassive sensors measure natural radiation, typically reflected sunlight (optical) or emitted heat (thermal). Because they rely on external energy sources, data collection can be constrained by sunlight availability and atmospheric conditions. However, optical imagery from passive sensors often offers intuitive color representations, making it easier to interpret. Thermal bands can reveal heat signatures useful in land surface temperature studies.\nActive sensors emit their own energy and record the backscatter signal. Radar systems (e.g., SAR) and LiDAR instruments fall under this category. They can penetrate clouds, operate at night, and measure surface characteristics such as roughness or elevation. On the downside, the data are often more complex to process and interpret, requiring specialized expertise and software.\n\n\n\n\n\nFigure 9.6: Graphic depicting the difference between active and passive sensors. Source: Radiant Earth\n\n\nBelow is a simple comparison of these sensor types:\n\n\n\n\n\n\n\n\n\n\n\nSensor Type\nExamples\nOperating Principle\nPros\nCons\n\n\n\nPassive\nOptical, Thermal\nMeasures naturally available reflected energy (sunlight or emitted heat)\nEasier to interpret; often intuitive (e.g., RGB images)\nLimited by cloud cover and sensitive to atmospheric conditions\n\n\nActive\nRadar (SAR), LiDAR\nEmits energy and measures return signals\nCan penetrate clouds, capture data day or night\nData may require more complex processing\n\n\n\n\n\nTable 9.5: Sensor types and their key characteristics.\n\n\nUnderstanding which sensor type best aligns with your research needs is crucial. For instance, if your study area is frequently cloudy, a radar-based approach can offer more reliable coverage. Alternatively, optical sensors might be sufficient for regions with seasonal cloud-free windows or for general land-cover classification when consistent sunlight is available.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#cloud-cover",
    "href": "part-02-satellite/01-satellite-imagery.html#cloud-cover",
    "title": "9  Choosing imagery",
    "section": "\n9.9 Cloud cover",
    "text": "9.9 Cloud cover\nCloud cover is one of the most persistent challenges in working with optical satellite imagery, as clouds and shadows can degrade the quality of the collected data. In regions with frequent cloudiness, images may require additional processing or longer acquisition windows to achieve acceptable coverage. Strategies to address this issue include applying cloud masking algorithms, merging multiple images to create a cloud-free composite, or using radar-based satellites that penetrate clouds.\nOptical sensors, such as those on Sentinel-2 or Landsat, often include quality assurance layers that flag pixels contaminated by clouds. Filtering out these pixels improves data reliability but reduces the amount of usable imagery, potentially limiting temporal coverage in cloudy regions. On the other hand, radar missions like Sentinel-1 or ICEYE remain largely unaffected by cloud conditions, offering consistent coverage at the cost of different data characteristics and additional expertise required for radar data interpretation. Weighing these trade-offs helps in choosing imagery that meets both your accuracy requirements and your practical constraints.\n\n\n\n\n\nFigure 9.7: Cloud cover obscuring the view over over North America. Source: NASA Earth Observatory",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#processing-levels",
    "href": "part-02-satellite/01-satellite-imagery.html#processing-levels",
    "title": "9  Choosing imagery",
    "section": "\n9.10 Processing levels",
    "text": "9.10 Processing levels\nSatellite data are distributed at various processing levels, ranging from unprocessed instrument measurements to derived products that are nearly ready for analysis. Selecting a processing level that aligns with your project’s technical requirements can streamline workflows and ensure consistent results.\nBelow is a general outline of commonly encountered processing levels:\n\n\n\n\n\n\n\n\n\nLevel\nDescription\nTypical Use Case\n\n\n\nLevel 0\nRaw instrument data with minimal or no preprocessing. Often not orthorectified or radiometrically calibrated.\nRarely used for general analysis due to complexity.\n\n\nLevel 1\nRadiometric calibration and basic geometric corrections. Sometimes subdivided (e.g., 1A, 1B, 1C).\nSuitable if you need control over final atmospheric corrections or custom workflows.\n\n\nLevel 2\nSurface reflectance products with atmospheric corrections applied, often including cloud/shadow masks.\nMost common for analyses requiring accurate reflectance values and multi-temporal comparisons.\n\n\nLevel 3\nDerived or composite products (e.g., mosaics, NDVI layers, gap-filled data).\nIdeal for thematic applications that benefit from aggregated or pre-processed data.\n\n\n\n\n\nTable 9.6: Common processing levels for satellite imagery and their typical use cases.\n\n\nAdditional processes often applied during or after these levels include:\n\n\nOrthorectification: Removal of geometric distortions caused by sensor tilt, terrain relief, and Earth curvature. Ensures accurate spatial representation for mapping and analysis.\n\nAtmospheric correction: Adjustment for atmospheric effects (aerosols, water vapor) to standardize reflectance values across different times or sensors.\n\nRadiometric calibration: Conversion of raw sensor counts to physical units like reflectance or brightness temperature, enabling consistent comparisons among images from different sensors and acquisition dates.\n\n\n\n\n\n\nFigure 9.8: NICFI satellite imagery raw pixel values showing each band (R, G, B, NIR) for each statistic (min, max, mean, median). These data come with atmospheric correction, cloud correction, and normalized pixel values. After Products created after April 2022 also apply a sharpening filter to the images.\n\n\nKnowing which processing steps your data provider already performs helps you avoid redundant work and ensures you acquire data that meet your accuracy requirements. For instance, if you need consistent multi-temporal comparisons, opt for Level 2 or higher to minimize atmospheric variations. Conversely, if you prefer to perform your own corrections and calibrations, Level 1 might offer greater flexibility at the cost of added complexity.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#choosing-the-right-combination",
    "href": "part-02-satellite/01-satellite-imagery.html#choosing-the-right-combination",
    "title": "9  Choosing imagery",
    "section": "\n9.11 Choosing the right combination",
    "text": "9.11 Choosing the right combination\nSelecting the right combination of spatial, spectral, temporal, and sensor characteristics can feel overwhelming, given the numerous satellites and data products available. This choice often comes down to the trade-offs you are willing to make between cost, resolution, revisit frequency, spectral coverage, and data availability.\n\n\n\n\n\n\n\n\n\n\n\nApplication\nSpatial resolution\nTemporal frequency\nKey bands\nExample sensors\n\n\n\nAgricultural\n5–30m\nWeekly–monthly\nNIR, Red, SWIR\nPlanet, Sentinel-2, Landsat 8/9\n\n\nUrban\n0.5–10m\nMonthly–annual\nRGB, NIR\nWorldView, Planet, Sentinel-2\n\n\nForest Monitoring\n10–30m\nMonthly–annual\nNIR, SWIR, Red\nLandsat, Sentinel-2\n\n\nWater Resources\n10–30m\nWeekly–monthly\nNIR, SWIR, Blue\nSentinel-2, Landsat 8/9\n\n\n\n\n\nTable 9.7: Sample satellite characteristics for different applications.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/01-satellite-imagery.html#making-the-final-decision",
    "href": "part-02-satellite/01-satellite-imagery.html#making-the-final-decision",
    "title": "9  Choosing imagery",
    "section": "\n9.12 Making the final decision",
    "text": "9.12 Making the final decision\nConsider these questions when making your final imagery selection:\n\nWhat is the minimum spatial resolution needed?\nHow frequent do observations need to be?\nWhat spectral bands are required?\nWhat is the time period of interest?\nWhat is your budget?\nWhat processing level is needed?\nHow will you handle clouds and data gaps?\nWhat are your storage and computing resources?\n\nThe answers to these questions will guide you toward the most appropriate imagery source for your specific application.\n\n\n\n\n\nFigure 9.9: DALL-E generated image of finding the perfect satellite.\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we will explore how to access and process satellite imagery for use with MOSAIKS.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Choosing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html",
    "href": "part-02-satellite/02-satellite-processing.html",
    "title": "10  Processing imagery",
    "section": "",
    "text": "10.1 Overview\nAfter selecting appropriate satellite imagery (as discussed in Chapter 9), the next step is accessing and processing that imagery for use with MOSAIKS. This chapter covers key considerations and practical approaches for working with satellite data.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#overview",
    "href": "part-02-satellite/02-satellite-processing.html#overview",
    "title": "10  Processing imagery",
    "section": "",
    "text": "Source: Microsoft Planetary Computer",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#accessing-satellite-imagery",
    "href": "part-02-satellite/02-satellite-processing.html#accessing-satellite-imagery",
    "title": "10  Processing imagery",
    "section": "\n10.2 Accessing satellite imagery",
    "text": "10.2 Accessing satellite imagery\n\n10.2.1 Cloud vs local\nThere are several cloud-based platforms that provide access to a wide range of satellite imagery and geospatial datasets. These platforms offer scalable computing resources, pre-configured environments, and often include common datasets. Some popular cloud platforms for satellite imagery processing include:\nThese platforms offer several advantages, including:\n\nNo need to download raw imagery\nScalable computing resources\nPre-configured environments\nOften include common datasets\nCan download to local or cloud storage\n\nNASA Earthdata Cloud Cookbook\n\n10.2.1.1 Google Earth Engine\nGoogle Earth Engine provides a vast catalog of satellite imagery and geospatial datasets that can be accessed through a Python API. The GEE API is based on the Earth Engine Python API, which allows you to interact with the Earth Engine servers and run geospatial analyses in the cloud.\n\n10.2.1.2 Microsoft Planetary Computer\nMicrosoft Planetary Computer provides a multi-petabyte catalog of global environmental data that can easily be accessed through APIs. The MPC API is based on the STAC (SpatioTemporal Asset Catalog) specification, which is an emerging open standard for geospatial data that aims to increase the interoperability of geospatial data, particularly satellite imagery.\nFor more information on using the Microsoft Planetary Computer API, see Reading Data from the STAC API.\n\n\n\n\n\n\nFor practical guidance on accessing the Microsoft Planetary Computer Data Catalog through their API,see the Master of Environmental Data Science (MEDS) course EDS 220 section on the STAC specification\n\n\n\n\n10.2.2 Planet API\n\nNICFI is free\n\nSign up at NICFI\n\nAccess to Visual (RGB) and Analytic (RGB+NIR) basemaps\n\n\nPlanet is paid\n\nSign up at Planet\n\nAPI access to PlanetScope, RapidEye, SkySat, Dove, and basemap imagery\nInstitutional licensing available\n\n\n\n\n10.2.2.1 Sentinel Hub\nSentinel Hub is a cloud-based platform that provides access to a wide range of satellite imagery, including data from the Sentinel-1, Sentinel-2, and Landsat missions. The Sentinel Hub API allows you to access and process satellite imagery using a simple Python interface.\n\n10.2.3 Sentinelsat python API\nThe sentinelsat Python API allows you to search and download satellite imagery from the Copernicus Open Access Hub. The API provides a simple interface for searching and downloading Sentinel-1 and Sentinel-2 imagery, as well as other Copernicus data products.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#understanding-the-data",
    "href": "part-02-satellite/02-satellite-processing.html#understanding-the-data",
    "title": "10  Processing imagery",
    "section": "\n10.3 Understanding the data",
    "text": "10.3 Understanding the data\nOnce you have decided how to acquire satellite imagery, the next step is to make sure that you understand the data so you can make sure to process it correctly. Often the best resource is to consult the data provider’s documentation. Aside from that, the best way to understand the data is to open it up and explore it.\n\n\n\n\n\n\nFor hands on experience with satellite imagery, skip to the next chapter where you can access NICFI imagery in a Google Colab notebook (Chapter 11).\n\n\n\n\n10.3.1 Storage formats\nSatellite imagery can be stored in various formats, including:\n\nGeoTIFF\nNetCDF\nHDF5\nZarr\n\nThere are several python libraries that can be used to read and write these formats, such as rasterio, xarray, rioxarray, and h5py.\n\n10.3.2 Metadata\n\nData types (INT8, UINT16, FLOAT32)\nCoordinate reference system (CRS)\nSpectral bands (Red, Green, Blue, Near-Infrared, etc.)\nSpatial resolution (scene size, pixel size)\nAcquisition date (single or multiple dates)\nCloud cover\nData quality\n\n10.3.3 Visualization\nThere are many considerations for visualizing satellite imagery. We will cover some of the most common ones here.\n\n10.3.3.1 Normalization\n\n10.3.3.2 True color\n\n10.3.3.3 False color\n\n10.3.3.4 Sharpening\n\n10.3.4 Calculate Indices\nDepending on your application, a good place to start is calculating vegetation or other indices. There are many to choose from and it takes some domain knowledge to know if one would be useful for your application. In agriculture, there are several indices that are commonly used to monitor crop health, such as the Normalized Difference Vegetation Index (NDVI), the Enhanced Vegetation Index (EVI), and the Soil Adjusted Vegetation Index (SAVI).\nNormalized Difference Vegetation Index (NDVI) is a simple and common vegetation index that uses the difference between near-infrared (NIR) and red bands to quantify vegetation health.\n\n\n\n\n\nFigure 10.1: NDVI values animated over Africa. Made with Google Earth Engine Python API.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#processing-satellite-imagery",
    "href": "part-02-satellite/02-satellite-processing.html#processing-satellite-imagery",
    "title": "10  Processing imagery",
    "section": "\n10.4 Processing satellite imagery",
    "text": "10.4 Processing satellite imagery\nThere are two main approaches to accessing and processing satellite imagery:\n\n10.4.1 Cloud-based platforms\nModern cloud platforms offer several advantages for satellite image processing:\n\nNo need to download raw imagery\nScalable computing resources\nPre-configured environments\nOften include common datasets\nPay only for what you use\n\nPopular platforms include:\n\nGoogle Earth Engine\nMicrosoft Planetary Computer\nAmazon Web Services\nPlanet Platform\nEuro Data Cube\n\n10.4.2 Local processing\nTraditional local processing may be preferred when:\n\nInternet connectivity is limited\nData privacy is paramount\nWorking with proprietary algorithms\nComputing needs are modest\nFrequent reuse of same imagery\n\nConsider these factors when choosing:\n\nData volume\nProcessing complexity\nBudget constraints\nTime requirements\nSecurity needs",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#processing-steps",
    "href": "part-02-satellite/02-satellite-processing.html#processing-steps",
    "title": "10  Processing imagery",
    "section": "\n10.5 Processing steps",
    "text": "10.5 Processing steps\n\n10.5.1 Quality assessment\n\nCloud detection\nShadow masking\nBad pixel identification\nSensor artifacts removal\n\n10.5.2 Atmospheric correction\n\nConvert to surface reflectance\nAccount for atmospheric effects\nStandardize across images\n\n10.5.3 Geometric correction\n\nOrthorectification\nCo-registration\nProjection alignment\n\n10.5.4 Mosaicking\n\nImage stitching\nFeathering/blending\nGap filling\nColor balancing\n\n10.5.5 Temporal compositing\n\nBest-pixel selection\nWeighted averaging\nGap filling\nSmoothing",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#best-practices",
    "href": "part-02-satellite/02-satellite-processing.html#best-practices",
    "title": "10  Processing imagery",
    "section": "\n10.6 Best practices",
    "text": "10.6 Best practices\n\n\nDocument everything\n\nProcessing steps\nParameter choices\nQuality control decisions\nSoftware versions\n\n\n\nValidate outputs\n\nVisual inspection\nStatistical checks\nGround truth comparison\nCross-platform verification\n\n\n\nOptimize resources\n\nBatch processing\nParallel computing\nMemory management\nStorage efficiency\n\n\n\nVersion control\n\nTrack code changes\nArchive key datasets\nDocument dependencies\nMaintain reproducibility",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#common-challenges",
    "href": "part-02-satellite/02-satellite-processing.html#common-challenges",
    "title": "10  Processing imagery",
    "section": "\n10.7 Common challenges",
    "text": "10.7 Common challenges\n\n10.7.1 Storage requirements\n\nRaw imagery can be massive\nMultiple processing steps multiply storage needs\nIntermediate products management\nBackup considerations\n\n10.7.2 Computing resources\n\nProcessing can be CPU/GPU intensive\nMemory limitations\nI/O bottlenecks\nNetwork bandwidth\n\n10.7.3 Quality issues\n\nCloud contamination\nAtmospheric effects\nSensor artifacts\nGeometric distortions\n\n10.7.4 Time management\n\nProcessing can be slow\nDownload times\nQuality checking\nIteration cycles",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/02-satellite-processing.html#future-considerations",
    "href": "part-02-satellite/02-satellite-processing.html#future-considerations",
    "title": "10  Processing imagery",
    "section": "\n10.8 Future considerations",
    "text": "10.8 Future considerations\nAs you develop your processing pipeline, consider:\n\nScalability needs\nAutomation opportunities\nQuality control requirements\nResource constraints\nTime limitations\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section, we will load, visualize, and explore NICFI satellite imagery in Google Colab.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Processing imagery</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/03-satellite-demo.html",
    "href": "part-02-satellite/03-satellite-demo.html",
    "title": "11  Imagery demo",
    "section": "",
    "text": "11.1 Overview\nThis demonstration will show you a few key concepts about satellite data and how it can be prepared for featurization (featurization covered in Chapter 14).",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imagery demo</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/03-satellite-demo.html#demonstration-data",
    "href": "part-02-satellite/03-satellite-demo.html#demonstration-data",
    "title": "11  Imagery demo",
    "section": "\n11.2 Demonstration data",
    "text": "11.2 Demonstration data\nFor this demonstration, we will use satellite imagery from the NICFI Basemaps. The NICFI Basemaps are a collection of high-resolution satellite images that are freely available for research purposes. These data come with atmospheric correction, cloud correction, and normalized pixel values. After Products created after April 2022 also apply a sharpening filter to the images.\nMore details about NICFI Basemap processing can be found here.\nTo learn how to access NICFI Basemaps, follow the sign up instructions here.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imagery demo</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/03-satellite-demo.html#demonstration-code",
    "href": "part-02-satellite/03-satellite-demo.html#demonstration-code",
    "title": "11  Imagery demo",
    "section": "\n11.3 Demonstration code",
    "text": "11.3 Demonstration code\nBelow is a link to a Jupyter notebook intended to demonstrate practical preparation of satellite data for use in MOSAIKS. The notebook will guide you through the process of preparing imagery, including:\n\nLoading satellite imagery\nInspecting image properties\nImage normalization\nImage visualization\n\n\n\n\n\n\n\nClick the badge to run the demonstration!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Remember to click File -&gt; Save a copy in Drive to save any changes you make.\n\nOr to view a static version of the code on GitHub, click the badge below.\n\n\nFor instructions and tips on using Google Colab, please refer to Chapter 1.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imagery demo</span>"
    ]
  },
  {
    "objectID": "part-02-satellite/03-satellite-demo.html#whats-next",
    "href": "part-02-satellite/03-satellite-demo.html#whats-next",
    "title": "11  Imagery demo",
    "section": "\n11.4 What’s next?",
    "text": "11.4 What’s next?\nNow that we have covered the basics of working with satellite data, we will move on to the next section, where we will discuss image featurization.\n\n\n\n\n\n\nLooking forward\n\n\n\nThis is the end of the satellite data section. Next, we will move on to the featurization of satellite data.",
    "crumbs": [
      "Satellite imagery",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imagery demo</span>"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html",
    "href": "part-03-features/00-features.html",
    "title": "Satellite features",
    "section": "",
    "text": "Overview\nThis section explores MOSAIKS features - the compressed representations of satellite imagery that enable efficient prediction across diverse tasks. While many users can rely on pre-computed API features, understanding how these features work and how to generate them provides valuable context and enables customization when needed.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#overview",
    "href": "part-03-features/00-features.html#overview",
    "title": "Satellite features",
    "section": "",
    "text": "The technical details covered in these chapters are primarily relevant for users who need to understand or generate custom MOSAIKS features. If you plan to use the MOSAIKS API features, you can focus on 13  API features.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#when-to-look-beyond-the-api-features",
    "href": "part-03-features/00-features.html#when-to-look-beyond-the-api-features",
    "title": "Satellite features",
    "section": "When to look beyond the API features",
    "text": "When to look beyond the API features\nThe MOSAIKS API provides pre-computed features derived from 2019 Planet Labs imagery. While these features enable many applications, you may need to work directly with feature generation when:\n\nYour analysis requires data from a different time period\nYou need features at a different spatial resolution\nYou want to experiment with different feature parameters\nYou’re developing new methodological approaches\nYou need to validate or compare feature types",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#feature-types-and-computation",
    "href": "part-03-features/00-features.html#feature-types-and-computation",
    "title": "Satellite features",
    "section": "Feature types and computation",
    "text": "Feature types and computation\nMOSAIKS features transform raw satellite imagery into a concise tabular format that captures essential patterns while dramatically reducing data volume. The system currently supports:\n\nRandom convolutional features (RCFs)\nGaussian random features\nEmpirical patch features\nHybrid approaches\n\nThese different feature types offer various tradeoffs in terms of computation time, storage requirements, and predictive performance across tasks.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/00-features.html#section-outline",
    "href": "part-03-features/00-features.html#section-outline",
    "title": "Satellite features",
    "section": "Section outline",
    "text": "Section outline\nThe following chapters will guide you through key aspects of MOSAIKS features:\n\n\n\n\n\n\n\n\nChapter\nKey Topics\n\n\n\n12  Understanding features\nRandom convolutional features, implementation details\n\n\n13  API features\nPre-computed features, specifications, usage\n\n\n14  Computing features\nProcessing requirements, workflows, storage\n\n\n\n\n\nTable 1: Outline of the features section\n\n\nThese chapters provide both practical guidance for working with MOSAIKS features and deeper technical understanding of how the feature extraction process works.\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will attempt to provide some context and intuition for the random convolutional features (RCFs) that are at the core of MOSAIKS.",
    "crumbs": [
      "Satellite features"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html",
    "href": "part-03-features/01-features-rcf.html",
    "title": "12  Understanding features",
    "section": "",
    "text": "12.1 Kitchen sinks?\nMOSAIKS stands for Multi-task Observation using SAtellite Imagery & Kitchen Sinks. Whenever we present MOSAIKS, we get the question, “Where does ‘kitchen sinks’ come from?” The answer stems from the phrase “everything but the kitchen sink,” which means “almost everything imaginable.” In the context of MOSAIKS, everything but the kitchen sink emphasizes that we take a huge amount of information out of the raw imagery—though, of course, we’re not capturing every last pixel or every possible relationship. It’s as if we’re taking the most useful “ingredients” (i.e., features) from the imagery and leaving the rest behind.\nThis idea of leaving behind the raw imagery is key to MOSAIKS’s power. It means that most users never have to handle the massive amounts of satellite imagery directly. Instead, the MOSAIKS team takes on that burden—extracting a large set of random convolutional features—and then discards the imagery. End users do not need to see the raw imagery or interpret what each individual feature means; they can just use the numerical representation. As a result, users can simply download these features and apply them to their own predictive tasks.\nIn this section, however, we lift the hood to see what’s going on. We focus on how we extract the features from satellite imagery and attempt to provide some intuition for what these features represent.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#kitchen-sinks",
    "href": "part-03-features/01-features-rcf.html#kitchen-sinks",
    "title": "12  Understanding features",
    "section": "",
    "text": "Figure 12.1: Maddy (Madagascar; center) takes raw satellite images (left) and uses the kitchen sink method to produce random convolutional features (right). Art by Grace Lewin.\n\n\n\n\n\n\n\n\n\n\nIn this book, the terms random convolutional features, RCFs, features, satellite features, and MOSAIKS features are used interchangeably.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#turning-images-into-features",
    "href": "part-03-features/01-features-rcf.html#turning-images-into-features",
    "title": "12  Understanding features",
    "section": "\n12.2 Turning images into features",
    "text": "12.2 Turning images into features\n\n\n\n\n\n\nFigure 12.2: A collection of satellite images. Source: Microsoft Planetary Computer\n\n\n\n\n12.2.1 Overview\nThe MOSAIKS featurization process produces a fixed-length feature representation for each patch of satellite imagery. Practically, this means we end up with a numerical vector for each image. Because MOSAIKS uses satellite image, you can substitute the word image for location which means we end up with a numerical representation for each location.\n\n\n\n\n\nFigure 12.3: A closer look at the RCF processing. Illustration of the one-time unsupervised computation of random convolutional features. K patches are randomly sampled from across the N images. Each patch is convolved over each image, generating a nonlinear activation map for each patch. Activation maps are averaged over pixels to generate a single K-dimensional feature vector for each image. Source: Rolf et al. 2021 Figure 1 C\n\n\nThe featurization process has three main steps: convolution, activation, and average pooling.\nIn the coming sub-sections, we will illustrate these steps. In our example, our input imagery will have the dimensions \\(3 x 256 x 256\\), where \\(3\\) is the number of color channels (red, green, and blue) and \\(256 x 256\\) is the image size (width x height) in pixels.\n\n12.2.2 Understanding convolutions\nIn simple terms, applying a convolution to an image can highlight certain patterns, like edges, textures, or colors. Convolutional filters “scan” across the image, computing element-wise products with small patches of the image. Many computer vision models stack multiple convolutional and other layers into a convolutional neural network (CNN). Deep neural networks can have many layers, which is why the approach is often called deep learning.\n\n\n\n\n\nFigure 12.4: A gif showing a convolutional layer with no padding and no strides. Source: A guide to convolution arithmetic for deep learning.\n\n\nMOSAIKS, on the other hand, uses a single convolutional layer. These layer weights are randomly initialized and remain fixed; they aren’t updated during training. This is why the features we get are essentially “random samples” of the spatial and spectral information in the images.\n\n\n\n\n\n\nThere are many great resources for visualizing convolutions, including Convolutional Neural Networks (CNNs) explained.\n\n\n\n\n12.2.2.1 Initializing filters\nFor the convolution step, we need to initialize a set of filters. Each filter is a 3-dimensional tensor with the same number of color channels as the images and a width and height size specified by a kernel size parameter. We can initialize these filters in two ways:\n\nGaussian initialization: We draw the filter weights from a normal distribution.\nEmpirical patches: We randomly sample patches from the image dataset and use these as filters.\n\nEither way, each filter has the same number of color channels as the original images and a specified kernel size (e.g., 3×3). So if our kernel size is 3, each filter might have shape (3, 3, 3). The number of filters is a hyperparameter that you can set when you define your model.\nTo perform the convolution, we compute the dot product of each filter with every portion of the image as the filter slides across it. The result is a new image (a convolution output map) that highlights regions similar to that filter’s pattern.\n\n\n\n\n\n\nWe use pytorch’s torch.nn.functional.conv2d function to perform this operation.\n\n\n\n\n12.2.2.2 Whitening filters\nIf you decide to use empirical patches in the convolution step, it is best practice to whiten the patches. We use a process called Zero-phase Component Analysis (ZCA) whitening, which preserves the spatial structure of the data while removing correlations among pixels.\nThe key steps are:\n\nSubtract the mean of each feature from the data.\nCompute the covariance matrix \\(\\Sigma\\) and perform its eigendecomposition.\nUse the eigenvectors \\(U\\) and eigenvalues \\(D\\), along with a small constant \\(\\varepsilon\\), to form the ZCA transform \\(W\\).\nMultiply the original data by \\(W\\) to obtain the whitened data \\(\\widetilde{X}\\).\n\nMathematically, assuming \\(X\\) is your zero-mean data matrix of shape \\((N \\times d)\\):\n\\[\n\\Sigma = \\frac{1}{N} X^\\top X\n\\quad\\quad\\text{and}\\quad\\quad\n\\Sigma = U \\, D \\, U^\\top,\n\\]\n\\[\nW = U \\Bigl(D + \\varepsilon I\\Bigr)^{-\\tfrac{1}{2}} U^\\top,\n\\]\n\\[\n\\widetilde{X} = X \\, W.\n\\]\n\n\n\n\n\n\nSee the ZCA whitening implementation in torchgeo for more details.\n\n\n\n\n12.2.3 Activation\nNext, we apply a non-linear activation function—ReLU (Rectified Linear Unit)—to the convolution output map. ReLU outputs max(0, x), meaning it zeroes out negative values. This step helps capture non-linearities in the data.\nThe 2-for-1 activation special\nWhen we define our model, one of the parameters that needs to be specified is the number of features. This number needs to be a positive value because we generate 2 features for each filter.\nWe do this by applying the activation function twice:\n\n\nFeature A – ReLU activation on the convolution output.\n\n\nFeature B – ReLU activation on the inverted convolution output (i.e., multiplication by –1, or “negative” of the same filter output).\n\nSo if you specify 200 features, you are actually drawing 100 weights and getting 200 features back. This improves computational efficiency and ensures that each filter has the potential to capture both positive and negative cues.\n\n12.2.4 Average pooling\nWe then apply an adaptive average pooling layer to each activation map, collapsing the 2D spatial grid into a single number per filter. Essentially, this “global average” is a single summary value for how strongly that filter responded to the image.\n\n\n\n\n\n\nWe use pytorch’s torch.nn.functional.adaptive_avg_pool2d function to perform this operation.\n\n\n\n\n12.2.5 Putting it all together\nWe repeat the 3 steps (convolution, activation, and pooling) for all filters. If we have K filters, we end up with a K-dimensional feature vector.\n\n\n\n\n\nFigure 12.5: Input image, selected patches, whitened patches, convolution output, and activation maps",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#intuition-for-rcfs",
    "href": "part-03-features/01-features-rcf.html#intuition-for-rcfs",
    "title": "12  Understanding features",
    "section": "\n12.3 Intuition for RCFs",
    "text": "12.3 Intuition for RCFs\nBelow is a cartoon that illustrates what a random convolutional feature vector might be looking for. Suppose we randomly draw three 3×3 patches from different parts of an image: a forest patch, a road patch, and a river patch.\n\n\n\n\n\nFigure 12.6: A single image with 3 random convolutional patches, highlighting 3 different feature activations of the imagery.\n\n\nWhen we convolve the forest patch across the entire image, the resulting map “lights up” places that look like forest. Likewise, the road patch lights up roads, and the river patch lights up the river. After the ReLU and average-pooling steps, we get a set of summary values—one for each filter.\n\n\n\n\n\nFigure 12.7: Two cartoon images with the same patches used in the convolution step. We see that we have several labels for each image and that we can model each outcome with the same set of features.\n\n\nIf Image 1 has more trees than Image 2, the “forest” feature’s value is higher in Image 1. If Image 2 has more roads, its “road” feature will be higher. Because these features each capture a different (random) spatial pattern, we can then combine them for many downstream prediction tasks—whether it’s predicting tree cover, paved roads, or even something else that correlates with these visual patterns.\n\n\n\n\n\nFigure 12.8: A cartoon image projected into random feature space. If we look at just 2 dimensions, how grey on the x-axis and how green on the y-axis, we can find the vector through this feature space which represents our outcome.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#why-use-rcfs",
    "href": "part-03-features/01-features-rcf.html#why-use-rcfs",
    "title": "12  Understanding features",
    "section": "\n12.4 Why use RCFs?",
    "text": "12.4 Why use RCFs?\n\n12.4.1 Traditional convolutional neural networks (CNNs)\nTo appreciate RCFs, let’s contrast them with traditional CNNs. In a standard CNN, filters are learned via backpropagation. The network sees many examples, calculates errors, and updates the filter weights so that over time they become good at extracting task-specific features. This makes CNNs powerful, but only for what they have been trained to do.\n\n\n\n\n\nFigure 12.9: A simplified diagram showing a typical convolutional neural network model architecture.\n\n\n\n12.4.2 Replacing minimization with randomization in learning\nMOSAIKS takes a radically simpler approach: random filters are used to sample the imagery’s information. Because they are random, they are not “tailored” to any one task. This sounds counterintuitive at first—surely you’d want to learn your filters! But the virtue of randomization is speed and broad applicability. Since no training is required to set these filters, we can produce features at planet-scale very quickly. These features can then be shared with many users, each of whom can apply them to their own predictive tasks (e.g., estimating forest cover, housing density, crop yields, etc.).\n\n\n\n\n\nFigure 12.10: Rolf et al. 2021 Figure 1: MOSAIKS is designed to solve an unlimited number of tasks at planet-scale quickly. After a one-time unsupervised image featurization using random convolutional features, MOSAIKS centrally stores and distributes task-agnostic features to users, each of whom generates predictions in a new context.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/01-features-rcf.html#summary",
    "href": "part-03-features/01-features-rcf.html#summary",
    "title": "12  Understanding features",
    "section": "\n12.5 Summary",
    "text": "12.5 Summary\nRandom Convolutional Features (RCFs) form the backbone of MOSAIKS’s ability to handle massive amounts of satellite imagery at scale. Instead of learning tailored filters via backpropagation (like a traditional convolutional neural network), MOSAIKS uses randomly initialized filters that remain fixed. This single-layer approach may seem counterintuitive, but it has several advantages:\n\n\nLightweight and Task-Agnostic\n\nBecause the filters are not tied to any particular outcome, the same feature set can be applied to countless downstream tasks. This drastically reduces the need to reprocess raw imagery for every new prediction goal.\n\n\n\nPlanet-Scale Featurization\n\nMassive amounts of imagery can be processed quickly because no iterative training is needed to refine filters. After a one-time generation of RCFs, they can be stored, shared, and reused—removing the bottleneck of handling petabytes of raw data.\n\n\n\nBroad Feature Capture\n\nRandom filters effectively “sample” a wide variety of spatial and spectral patterns, capturing edges, textures, colors, and more. The “2-for-1” feature approach ensures that each filter captures both positive and negative cues, doubling the dimensionality (and potential information) without doubling compute time.\n\n\n\nEasily Distributed\n\nThe resulting feature vectors (rather than raw images) are small enough to download and manipulate on standard hardware. Researchers and practitioners can thus apply these features to varied applications, from ecological monitoring to socioeconomic modeling.\n\n\n\nBy converting raw imagery into a rich but compact numerical representation, RCFs offer a robust, flexible, and scalable gateway to satellite-based insights—without the headache of storing or manually interpreting vast image archives.\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section, we’ll look at what is publicly available in the MOSAIKS API and how you can access these features without featurizing data yourself.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Understanding features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html",
    "href": "part-03-features/02-features-api.html",
    "title": "13  API features",
    "section": "",
    "text": "13.1 Overview\nIn this chapter, we focus on the publicly available MOSAIKS features that can be accessed via the MOSAIKS API. These features offer a quick and straightforward way to incorporate satellite-based predictors into your analyses without having to manually process satellite imagery.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#overview",
    "href": "part-03-features/02-features-api.html#overview",
    "title": "13  API features",
    "section": "",
    "text": "Accessing features on the API is covered in Chapter 3. This chapter provides additional details on the features and how they were generated. To generate your own features, see Chapter 14.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#input-imagery",
    "href": "part-03-features/02-features-api.html#input-imagery",
    "title": "13  API features",
    "section": "\n13.2 Input imagery",
    "text": "13.2 Input imagery\n\n\nSourcePlanet Labs Visual Basemap (Global Quarterly 2019, Q3).\n\nTemporal Coverage\nPredominantly images captured between July and September 2019 (Q3), though exact capture dates vary by region.\n\nSpatial Coverage\nGlobal land areas, excluding most large water bodies.\n\nPotential Artifacts\nCloud cover, haze, or shadows may affect the quality of imagery in regions with significant cloud coverage during Q3 2019.\n\n\n\n\n\n\nFigure 13.1: Planet Labs Basemap imagery thumbnail image.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#deeper-look-how-the-api-features-were-generated",
    "href": "part-03-features/02-features-api.html#deeper-look-how-the-api-features-were-generated",
    "title": "13  API features",
    "section": "\n13.3 Deeper Look: How the API Features Were Generated",
    "text": "13.3 Deeper Look: How the API Features Were Generated\nThe MOSAIKS team performed a single featurization pass over the Planet Labs 2019 Q3 Visual Basemap images. The process is similar to the Random Convolutional Features (RCFs) method described earlier, but here are the specific parameters:\n\n\nPatch Collection\n\nRandom patches (filters) were drawn from real satellite images (Planet Labs 2019 Q3).\n\nEach patch was whitened: the raw pixel values were mean-centered and decorrelated so that each filter highlights distinct visual patterns.\n\n\n\nKernel Sizes\n\n\n2,000 features from a 4×4 patch shape\n\n\n2,000 features from a 6×6 patch shape\n\nAll patches maintain 3 color channels (R, G, B).\n\n\n\nBias & Activation\n\nA bias of –1 is added to each filter’s convolution output, allowing more nuanced activation levels.\n\nA ReLU activation (max(0, x)) is then applied to keep the model non-linear and remove negative values.\n\n\n\nPooling\n\nAfter convolution + ReLU, the response is average-pooled over each 0.01° patch (i.e., the local 256×256 pixel area, if we approximate each degree of latitude or longitude as ~100 km—though the exact pixel count can vary by latitude).\n\nThis results in a single numeric value per filter.\n\n\n\nFinal Feature Vector\n\nCombining all filters yields a 4,000-dimensional vector at each 0.01° grid cell.\n\nThis entire process was run once, creating a global 0.01° “feature layer.”\n\n\n\n\n\n\n\n\nFigure 13.2: Three randomly generated feature activation maps plotted from the Planet Labs Basemap thumbnail image.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#derived-aggregations",
    "href": "part-03-features/02-features-api.html#derived-aggregations",
    "title": "13  API features",
    "section": "\n13.4 Derived aggregations",
    "text": "13.4 Derived aggregations\nAlthough the API stores and provides these features at 0.01° resolution, it also offers pre-aggregated versions:\n\n\n\n\nResolution\nDescription\nWeighting\nDownload\nAccess\n\n\n\n0.01°\nNative resolution (~1km²)\nUnweighted\nMap or File Query\nAPI Portal\n\n\n0.1°\nAggregated grid (~10km²)\nArea or population\nChunked files\nGlobal Grids\n\n\n1°\nAggregated grid (~100km²)\nArea or population\nSingle file\nGlobal Grids\n\n\nADM2\nCounty or district level\nArea or population\nSingle file\nPrecomputed Files\n\n\nADM1\nState or province level\nArea or population\nSingle file\nPrecomputed Files\n\n\nADM0\nCountry level\nArea or population\nSingle file\nPrecomputed Files\n\n\n\n\n\nTable 13.1: The MOSAIKS API offers features derived from Planet Labs imagery (2019 Q3) at various resolutions for the entire globe. All features share the same 4,000 feature columns, with the only difference being the resolution/aggregation level. At each aggregation level above the native resolution, we offer both area-weighted and population-weighted versions. Population weights come from the Gridded Population of the World (GPWv4) population density dataset.\n\n\nIt is nice to have options, but sometimes it is hard to visualize what these aggregations mean. Figure 13.3 shows how these features might look when aggregated to different levels.\n\n\n\n\n\nFigure 13.3: Example showing of 3 representative random convolutional features (rows). Features are downloaded from the MOSAIKS API at 0.01° resolution (the native resolution) and aggregated to 3 levels, including (A) larger grid cells (0.1°), (B) counties, and (C) states.\n\n\n\n13.4.1 Area vs. Population Weighting\nEach aggregated level comes in two weighting flavors: - Area Weighting: Larger grid cells or polygons with more land area receive more weight in the aggregation.\n- Population Weighting: Uses population density (GPWv4) to weight cells more heavily where more people live.\nnote maybe a population map here",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#when-to-use-these-features",
    "href": "part-03-features/02-features-api.html#when-to-use-these-features",
    "title": "13  API features",
    "section": "\n13.5 When to use these features",
    "text": "13.5 When to use these features\n\nReadily Available: If your labels are from ~2019 or a period that hasn’t changed drastically since 2019, start with these API features—they’re the fastest and easiest to incorporate into your analyses.\nCoarse-Resolution or Aggregated Labels: If your data is aggregated (e.g., a country-level statistic), consider downloading the aggregated features to match your label resolution.\nResource Constraints: If you’re storage- or compute-limited, using the API’s pre-aggregated files can save time and processing overhead.\nHigh-Resolution or Household Data: For fine-grained tasks (e.g., household surveys), you may still benefit from the 0.01° resolution features—especially if you want to capture local variation. Or you can aggregate to match your label geometry and then use high-resolution features for prediction later (see Chapter 17).\n\n\n\n\n\n\nFigure 13.4: Global Human Development Index (HDI) data at the first sub-national level of administrative division (ADM1). Source: Smits & Permanyer 2019.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#key-takeaways",
    "href": "part-03-features/02-features-api.html#key-takeaways",
    "title": "13  API features",
    "section": "\n13.6 Key takeaways",
    "text": "13.6 Key takeaways\n\n\nSingle Source Imagery\n\n\nPlanet Labs Visual Basemap (2019 Q3)\n\nGlobal coverage of land areas at approximately 0.01° (~1 km) resolution\n\nThree color channels (Red, Green, Blue)\n\n\n\nSingle Featurization Pass\n\nAll features are computed once at the native 0.01° resolution.\n\nAny aggregated features (e.g., 0.1°, 1°, administrative boundaries) are derivative and use exactly the same underlying 0.01° features.\n\n\n\nRandom Convolutional Features (RCFs)\n\n\n4,000 total features\n\n\nKernel sizes: 2,000 features from a 4×4 kernel, and 2,000 features from a 6×6 kernel\n\n\nEmpirical patch whitening: random patches are drawn from the image set and then whitened (mean-centered, decorrelated)\n\n\nBias: –1 (applied to each filter output before activation)\n\n\nActivation: ReLU (Rectified Linear Unit)\n\n\n3 color channels from the original RGB imagery\n\n\n\nFlexible Resolutions\n\n\nHigh resolution (0.01°): Download via File Query or Map Query\n\n\nAggregations: 0.1°, 1°, and ADM0/ADM1/ADM2 boundaries, available as precomputed downloads\n\n\n\nUse Cases\n\nIdeal for tasks with label data from the same (or close) time period (2019 or neighboring years)\n\nQuick, straightforward integration into machine learning models\n\nGlobal scale analysis or smaller local/regional analysis",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/02-features-api.html#future-directions",
    "href": "part-03-features/02-features-api.html#future-directions",
    "title": "13  API features",
    "section": "\n13.7 Future directions",
    "text": "13.7 Future directions\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next section, we will",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>API features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html",
    "href": "part-03-features/03-features-computing.html",
    "title": "14  Computing features",
    "section": "",
    "text": "14.1 Overview\nWhile the MOSAIKS API provides pre-computed features for many applications, some use cases require computing custom features. This chapter covers the technical details of generating MOSAIKS features from satellite imagery.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#requirements",
    "href": "part-03-features/03-features-computing.html#requirements",
    "title": "14  Computing features",
    "section": "\n14.2 Requirements",
    "text": "14.2 Requirements\nTo compute MOSAIKS features, you’ll need:\n\nSatellite imagery (see Chapter 10)\nGPU-enabled computing environment (recommended)\nPython with deep learning libraries (pytorch recommended)\nSufficient storage for features",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#implementation",
    "href": "part-03-features/03-features-computing.html#implementation",
    "title": "14  Computing features",
    "section": "\n14.3 Implementation",
    "text": "14.3 Implementation\nThere are several ways to implement MOSAIKS feature extraction:\n\n14.3.1 torchgeo implementation\nThe torchgeo library provides a PyTorch implementation of random convolutional features:\nimport torch\nfrom torchgeo.models import RCF\n\n# Define model parameters\npatch_size = 3  # Size of random patches\nin_channels = 4  # Number of input image channels\nnum_filters = 4000  # Number of features to generate\n\n# When empirical, supply a custom pytorch dataset class that returns \n# a dictionary with 'image' key. This samples the dataset for model \n# weights. If gaussian do not supply a dataset class.\n\n# Initialize RCF model\nmodel = RCF(\n    in_channels=in_channels, \n    features=num_filters, \n    kernel_size=3, \n    bias=-1.0, \n    seed=42, \n    mode='empirical',\n    dataset=CustomDataset,\n)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#feature-parameters",
    "href": "part-03-features/03-features-computing.html#feature-parameters",
    "title": "14  Computing features",
    "section": "\n14.4 Feature parameters",
    "text": "14.4 Feature parameters\nSeveral key parameters influence feature extraction:\n\n14.4.1 Number of features (K)\n\nControls feature vector dimensionality\nMore features capture more information\nIncreases computation and storage needs\nTypical range: 1,000-8,192\nDiminishing returns above ~4,000\n\n14.4.2 Patch size\n\nDetermines spatial context captured\nLarger patches see more context\nBut increase computation\nTypical size: 3x3 or 5x5 pixels\nMatch to imagery resolution\n\n14.4.3 Input channels\n\nDepends on available spectral bands\nRGB = 3 channels\nCan use additional bands\nMore bands = richer spectral info\nBut increases computation",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#practical-considerations",
    "href": "part-03-features/03-features-computing.html#practical-considerations",
    "title": "14  Computing features",
    "section": "\n14.5 Practical considerations",
    "text": "14.5 Practical considerations\n\n14.5.1 Memory management\nWhen processing large imagery datasets:\n\n14.5.2 Storage formats\nEfficient formats for large feature matrices:\n\n14.5.3 Parallel processing\nFor large-scale feature extraction:",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#quality-control",
    "href": "part-03-features/03-features-computing.html#quality-control",
    "title": "14  Computing features",
    "section": "\n14.6 Quality control",
    "text": "14.6 Quality control\nImportant checks during feature extraction:\n\n\nInput validation\n\nImage dimensions\nValue ranges\nMissing data\nBand ordering\n\n\n\nFeature statistics\n\nDistribution checks\nZero/missing values\nCorrelation analysis\nFeature importance\n\n\n\nPerformance monitoring\n\nMemory usage\nProcessing speed\nGPU utilization\nStorage efficiency",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/03-features-computing.html#best-practices",
    "href": "part-03-features/03-features-computing.html#best-practices",
    "title": "14  Computing features",
    "section": "\n14.7 Best practices",
    "text": "14.7 Best practices\n\n\nDocumentation\n\nRecord all parameters\nTrack data sources\nDocument processing steps\nNote any issues\n\n\n\nTesting\n\nUnit tests for functions\nIntegration tests\nPerformance benchmarks\nValidation checks\n\n\n\nVersion control\n\nCode versioning\nFeature versioning\nParameter tracking\nResult logging\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll work through a complete example of computing custom MOSAIKS features.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Computing features</span>"
    ]
  },
  {
    "objectID": "part-03-features/04-features-demo.html",
    "href": "part-03-features/04-features-demo.html",
    "title": "15  Featurization demo",
    "section": "",
    "text": "15.1 Overview\nThis demonstration will show you a few key concepts",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Featurization demo</span>"
    ]
  },
  {
    "objectID": "part-03-features/04-features-demo.html#demonstration-code",
    "href": "part-03-features/04-features-demo.html#demonstration-code",
    "title": "15  Featurization demo",
    "section": "\n15.2 Demonstration code",
    "text": "15.2 Demonstration code\nBelow is a link to a Jupyter notebook intended to demonstrate practical featurization of satellite data for use in MOSAIKS. The notebook will guide you through the process of preparing imagery, including:\n\nBuilding a pytorch model from scratch\nUsing an out of the box solution with torchgeo\nBuilding a torch dataset and dataloader\nFeaturizing satellite imagery\nSaving features to disk\nVisualizing features\n\n\n\n\n\n\n\nClick the badge to run the demonstration!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Remember to click File -&gt; Save a copy in Drive to save any changes you make.\n\nOr to view a static version of the code on GitHub, click the badge below.\n\n\nFor instructions and tips on using Google Colab, please refer to Chapter 1.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Featurization demo</span>"
    ]
  },
  {
    "objectID": "part-03-features/04-features-demo.html#whats-next",
    "href": "part-03-features/04-features-demo.html#whats-next",
    "title": "15  Featurization demo",
    "section": "\n15.3 What’s next?",
    "text": "15.3 What’s next?\nNow that we have covered the basics of featurizing satellite data, we will move on to the next section, where we will discuss modeling.\n\n\n\n\n\n\nLooking forward\n\n\n\nThis is the end of the feature section. Next, we will move on to the modeling labels with satellite features.",
    "crumbs": [
      "Satellite features",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Featurization demo</span>"
    ]
  },
  {
    "objectID": "part-04-models/00-model.html",
    "href": "part-04-models/00-model.html",
    "title": "Task modeling",
    "section": "",
    "text": "Section outline\nThe following chapters will guide you through key aspects of modeling tasks within the MOSAIKS framework:\nThese chapters provide",
    "crumbs": [
      "Task modeling"
    ]
  },
  {
    "objectID": "part-04-models/00-model.html#section-outline",
    "href": "part-04-models/00-model.html#section-outline",
    "title": "Task modeling",
    "section": "",
    "text": "Outline of the modeling section\n\n\n\n\n\nChapter\nKey Topics\n\n\n\n16  Build, evaluate, deploy\nAlgorithm selection, hyperparameter tuning\n\n\n17  Going over space\nSpatial cross-validation, geographic generalization\n\n\n18  Going over time\nTime series analysis, temporal alignment\n\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will",
    "crumbs": [
      "Task modeling"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html",
    "href": "part-04-models/01-model-choice.html",
    "title": "16  Build, evaluate, deploy",
    "section": "",
    "text": "16.1 Overview\nWhen using MOSAIKS (Multi-task Observation using SAtellite Imagery & Kitchen Sinks), model selection largely depends on your label data type. While more complex methods are certainly possible, experience shows that linear models often perform remarkably well. This is because the random convolutional features in MOSAIKS already capture and encode non-linear information from the underlying satellite imagery. This chapter will outline modeling approaches for different types of label data, and offer guidance on best practices for model evaluation.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#trainvalidation-and-test-splits",
    "href": "part-04-models/01-model-choice.html#trainvalidation-and-test-splits",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.2 Train/validation and test splits",
    "text": "16.2 Train/validation and test splits\nBefore jumping into the details, it is important to emphasize that all modeling tasks benefit from a systematic approach to training, validation, and testing:\n\n\nTrain/validation split (80% of the data)\n\n\nTest split (20% of the data)\n\nThis ensures that you have separate, unbiased datasets for both model tuning and final performance evaluation.\n\n\n\n\n\nFigure 16.1: Train/validation and test splits shown for a standard 5-fold cross validation procedure.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#continuous-labels",
    "href": "part-04-models/01-model-choice.html#continuous-labels",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.3 Continuous labels",
    "text": "16.3 Continuous labels\nMany MOSAIKS applications involve predicting continuous outcomes such as forest cover percentage, population density, average income, crop yields, or building height. In these cases, penalized linear regression approaches (especially ridge regression) work well. These methods offer simplicity, computational efficiency, and interpretability, while mitigating overfitting through the use of regularization.\n\n16.3.1 Ridge regression\nRidge regression (L2 regularization) is often the default choice in MOSAIKS applications because it effectively handles the large number of potentially correlated features produced by the random convolution process. It achieves this via an L2 penalty on the regression coefficients, which shrinks coefficients towards zero and reduces variance, thereby improving generalization.\nThe ridge regression objective function:\n\\[\n\\min_{\\beta} \\|y - X\\beta\\|^2_2 + \\lambda\\|\\beta\\|^2_2\n\\]\nWhere:\n\n\n\\(\\lambda\\) controls the strength of the regularization,\n\n\\(\\beta\\) are the regression coefficients,\n\n\\(y\\) is the vector of observed labels,\n\n\\(X\\) is the feature matrix produced by the MOSAIKS pipeline.\n\n16.3.2 Lasso regression\nLasso regression (L1 regularization) can actually set coefficients to exactly zero, effectively performing feature selection. The L1 penalty helps enforce sparsity, which can be valuable when interpretability or identification of key features is a priority. This property makes Lasso particularly valuable when you want to identify which MOSAIKS features contribute most strongly to predictions.\nThe lasso objective function:\n\\[\n\\min_{\\beta} \\|y - X\\beta\\|^2_2 + \\lambda\\|\\beta\\|_1\n\\]\nWhere:\n\n\n\\(\\lambda\\) again controls the strength of the penalty,\n\nThe \\(\\|\\beta\\|_1\\) term encourages some coefficients to be exactly zero.\n\nBoth can be easily implemented using scikit-learn:\nfrom sklearn.linear_model import Ridge, Lasso\n\n# Ridge regression\nridge = Ridge(alpha=1.0)  # alpha is the regularization strength (λ)\nridge.fit(X_train, y_train)\n\n# Lasso regression\nlasso = Lasso(alpha=1.0)\nlasso.fit(X_train, y_train)\n\n16.3.3 Why linear models work well\nThough the models themselves are linear, the features are not. MOSAIKS uses random convolutions, non-linear activation functions (ReLU), and average pooling to transform raw satellite imagery into highly expressive features. Because these transformations capture a broad range of non-linear spatial patterns, traditional linear methods can then perform well with minimal additional complexity.\n\n16.3.4 Evaluation metrics\nFor continuous outcomes, key evaluation metrics include:\n\n\nR² (coefficient of determination): Measures the proportion of variance in the label data explained by the model.\n\n\nRMSE (root mean squared error): Quantifies the average magnitude of the prediction errors.\n\n\nMAE (mean absolute error): Average absolute difference between predictions and true values, less sensitive to large outliers than RMSE.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#binary-classification",
    "href": "part-04-models/01-model-choice.html#binary-classification",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.4 Binary classification",
    "text": "16.4 Binary classification\nIn some MOSAIKS applications, your labels may be binary (e.g., building presence vs. absence, land use change vs. no change). Here, logistic regression often serves as a straightforward choice.\n\n16.4.1 Logistic regression\nLogistic regression models the probability that a given example belongs to the “positive” class:\n\\[\n\\text{logit}(p) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n\n\\]\nwhere \\(p\\) is the probability of belonging to the positive class. Despite its simplicity, logistic regression provides robust and interpretable performance for many binary classification tasks with MOSAIKS features.\n\n16.4.2 Evaluation metrics\nFor binary classification, common metrics include:\n\n\nAUC-ROC: The area under the ROC curve, assessing the trade-off between true positive rate and false positive rate across different threshold settings.\n\n\nAccuracy: The proportion of correct predictions.\n\n\nPrecision: Of the positive predictions made, how many were correct?\n\n\nRecall: Of the actual positives in the dataset, how many did we correctly identify?\n\n\nF1 score: The harmonic mean of precision and recall, often used in imbalanced classification settings.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#multi-class-classification",
    "href": "part-04-models/01-model-choice.html#multi-class-classification",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.5 Multi-class classification",
    "text": "16.5 Multi-class classification\nSome applications require predicting multiple categories (e.g., land cover types, crop variety, building categories). These are multi-class classification problems. Several approaches are possible:\n\n\nOne-vs-Rest: Train a binary classifier for each class; each classifier distinguishes one class from “all others.”\n\n\nMultinomial (softmax) regression: A single model to predict probabilities across all classes simultaneously.\n\n\nOrdinal regression: For predicting ordered categories (e.g., mild, moderate, severe damage).\n\n\n\n\n\n\nFigure 16.2: Example of multiclass classification\n\n\n\n16.5.1 Evaluation metrics\nFor multi-class problems, consider:\n\n\nOverall accuracy: Percentage of examples correctly classified.\n\n\nPer-class accuracy: Accuracy within each class, useful if class sizes are imbalanced.\n\n\nConfusion matrix: Provides a detailed breakdown of predictions vs. actual classes.\n\n\nWeighted F1 score: Averages F1 across classes, weighting by class frequency.\n\n\nCohen’s kappa: Measures agreement between predicted and actual labels, adjusting for chance agreement.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#applying-the-model-to-make-predictions",
    "href": "part-04-models/01-model-choice.html#applying-the-model-to-make-predictions",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.6 Applying the model to make predictions",
    "text": "16.6 Applying the model to make predictions\nOnce you have selected a model and tuned it (e.g., via cross-validation), you can apply the model to new, unseen data. This step is often referred to as “inference” or “scoring”:\n\nPrepare features: The features for the new areas or times should be extracted using the same RCF model that was used to extract the original features. This means that a model made with custom made could not be able to features on the API. The features may be randomly created, but after the model is initialized, the weights are fixed.\nLoad the trained model: Ensure the model (along with any hyperparameters and preprocessing steps) is loaded exactly as it was trained. This consistency is critical for maintaining predictive accuracy.\n\nPredict: Pass the new feature matrix (X_new) into the trained model’s predict method to obtain predictions. For example, using scikit-learn:\ny_pred = ridge.predict(X_new)\nIf your task is classification, use predict_proba to get predicted probabilities:\np_pred = logistic_regression.predict_proba(X_new)\n\nInterpret and store results: Save the predictions in a structured format (e.g., CSV, GeoTIFF) for downstream analysis. Consider including metadata about the date and version of your model, the MOSAIKS feature extraction process, and any relevant notes on data quality.\n\nBy following these steps, you ensure that your MOSAIKS models are deployed effectively and consistently. From here, you can visualize the predictions, integrate them into downstream analyses, or inform policy and decision-making based on the model’s output.\n\n16.6.1 Addressing domain shift\nWhen applying your model to new geographic regions or under different conditions, be aware that the underlying data distribution (satellite imagery patterns, socioeconomic factors, land-use/land-cover types) may differ from your training set. This “domain shift” can lead to reduced predictive performance if the model has not been exposed to similar examples during training. To mitigate this, consider collecting additional representative training data from these new regions, adopting transfer learning or domain adaptation techniques, and quantifying predictive uncertainty so you can flag areas where the model may perform poorly. Periodic performance audits and retraining—especially when new data becomes available—help ensure robust generalization across diverse geographies.\nPredicting at higher resolution than the labels",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#quantifying-uncertainty",
    "href": "part-04-models/01-model-choice.html#quantifying-uncertainty",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.7 Quantifying uncertainty",
    "text": "16.7 Quantifying uncertainty",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#model-selection-workflow",
    "href": "part-04-models/01-model-choice.html#model-selection-workflow",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.8 Model selection workflow",
    "text": "16.8 Model selection workflow\n\n\nIdentify label type\n\nContinuous → Ridge or Lasso regression\n\nBinary → Logistic regression\n\nMulti-class → One-vs-Rest or Multinomial\n\n\n\nCross-validation\n\nSplit data into train, validation, and test sets, respecting spatial structure where possible (e.g., leave-cluster-out).\n\nSelect appropriate evaluation metrics (e.g., R² for continuous, ROC-AUC for binary).\n\nTune hyperparameters (\\(\\lambda\\) in ridge/lasso, regularization in logistic) using the validation set.\n\n\n\nModel deployment\n\nRetrain on the entire training set using the chosen hyperparameters.\n\nGenerate predictions on the test set (or new data).\n\nQuantify uncertainty (e.g., confidence intervals, out-of-sample error estimates).",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/01-model-choice.html#summary",
    "href": "part-04-models/01-model-choice.html#summary",
    "title": "16  Build, evaluate, deploy",
    "section": "\n16.9 Summary",
    "text": "16.9 Summary\n\n\nStart with simple linear models: Let MOSAIKS features do the heavy lifting of extracting non-linear patterns.\n\n\nMatch the metric to the task: R² or RMSE for continuous labels, AUC-ROC or F1 score for classification, etc.\n\n\nUse cross-validation: Always separate training and testing to maintain unbiased estimates of model performance.\n\n\nConsider spatial structure: When dealing with spatial data, standard random splits may lead to overly optimistic estimates of performance.\n\nThe key elements for successful model building include:\n\n\nAlgorithm Selection\n\nChoose based on label type\nConsider computational needs\nBalance complexity vs performance\n\n\n\nCross Validation\n\nUse spatial awareness in splits\nSelect appropriate metrics\nTune model parameters\n\n\n\nModel Evaluation\n\nTest out-of-sample performance\nValidate across space\nQuantify uncertainties\n\n\n\nDeployment\n\nTrain final model\nGenerate predictions\nDocument process\nMonitor performance\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter, we’ll discuss strategies for accounting for spatial dependencies in your modeling workflow, including methods for spatially stratified cross-validation, spatial interpolation, and spatial extrapolation.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Build, evaluate, deploy</span>"
    ]
  },
  {
    "objectID": "part-04-models/02-model-spatial.html",
    "href": "part-04-models/02-model-spatial.html",
    "title": "17  Going over space",
    "section": "",
    "text": "17.1 Label super-resolution",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Going over space</span>"
    ]
  },
  {
    "objectID": "part-04-models/02-model-spatial.html#label-super-resolution",
    "href": "part-04-models/02-model-spatial.html#label-super-resolution",
    "title": "17  Going over space",
    "section": "",
    "text": "HDI example",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Going over space</span>"
    ]
  },
  {
    "objectID": "part-04-models/02-model-spatial.html#spatial-extrapolation",
    "href": "part-04-models/02-model-spatial.html#spatial-extrapolation",
    "title": "17  Going over space",
    "section": "\n17.2 Spatial extrapolation",
    "text": "17.2 Spatial extrapolation\n\nASM example (predicting mines in new countries)\nGo over feature space and show reduced dimension\n\nconvergence of countries vs divergence in labels\n\n\n\n\n\n\n\n\n\nClick the badge to run the demo in Google Colab!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓\nRemember to click File -&gt; Save a copy in Drive to save any changes you make.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Going over space</span>"
    ]
  },
  {
    "objectID": "part-04-models/02-model-spatial.html#spatial-interpolation",
    "href": "part-04-models/02-model-spatial.html#spatial-interpolation",
    "title": "17  Going over space",
    "section": "\n17.3 Spatial interpolation",
    "text": "17.3 Spatial interpolation\n\nFilling in gaps from survey data. eg. you have population data for some areas but not all.\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter,",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Going over space</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html",
    "href": "part-04-models/03-model-temporal.html",
    "title": "18  Going over time",
    "section": "",
    "text": "18.1 Understanding temporal resolution\nTime series analysis in satellite-based ML applications requires careful attention to the temporal resolution of both your labels and imagery. In the context of MOSAIKS, you may opt for different strategies depending on the frequency of satellite acquisitions and how quickly your outcome of interest changes.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#understanding-temporal-resolution",
    "href": "part-04-models/03-model-temporal.html#understanding-temporal-resolution",
    "title": "18  Going over time",
    "section": "",
    "text": "Figure 18.1: ESRI visualization of pixel values for a given location changing over time.\n\n\n\n18.1.1 Temporal alignment\nWhen aligning data in time, consider:\n\nLabel frequency: How often are ground measurements collected? For instance, annual crop yields, monthly economic indicators, or even daily weather observations. The granularity of these data points will guide how to match them to your satellite-derived features.\nImagery availability: How frequently can you obtain clear satellite images of your area of interest? High revisit rates enable more frequent observations, but factors like cloud cover or sensor anomalies may reduce the actual number of usable images.\nChange detection: How quickly does your variable of interest change? Urban development may unfold over months or years, whereas flood events can occur within days. Matching the temporal granularity of your features to the dynamics of your phenomenon is crucial.\n\n18.1.2 Seasonal patterns\nBoth natural and human-driven processes often exhibit strong seasonal signals:\n\nNatural cycles: Vegetation growth, snow cover, and water extent are all influenced by seasons. For example, NDVI metrics might be more informative during peak growing seasons (Figure 18.2).\nHuman activities: Cropping cycles, holiday travel, and heating or cooling demand are just a few examples of how human behavior can vary throughout the year. These temporal rhythms can introduce systematic patterns in your data.\nFeature extraction: Because satellite observations reflect surface conditions, different times of the year may require distinct feature sets. For example, reflectance values change when vegetation is senescent vs. when it is fully grown. Similarly, atmospheric conditions (like haze) may be more prevalent in certain seasons.\n\n\n\n\n\n\nFigure 18.2: EOS Data Analytics time series visualization of the Normalized Difference Vegetation Index (NDVI) showing seasonal trends.\n\n\n\n18.1.3 Challenges in time series applications\nAlthough the potential benefits of time series analysis are significant, there are a few common pitfalls:\n\n18.1.3.1 Data gaps\n\nCloud cover: In regions with high cloud coverage, usable imagery can be sparse, leading to irregular time intervals between valid observations.\nSatellite maintenance or sensor dropout: Even short interruptions in satellite operations can reduce data availability.\nOrbital patterns: Some satellites have specific revisit schedules, meaning certain areas might not be imaged as frequently as others, leading to patchy time series data.\n\n\n\n\n\n\nFigure 18.3: Visualization of cloud cover over the Amazon rainforest.\n\n\n\n\n\n\n\n\nRecommended reading\n\n\n\nFor more information on cloud cover and its impact on satellite data, see Flores-Anderson et al. 2023.\n\n\n\n18.1.4 Pre-processed satellite products\nSeveral satellite providers offer pre-processed data products specifically designed for time series analysis. These products handle common challenges like cloud cover and normalization:\n\n18.1.4.1 MODIS vegetation indices\n\n16-day or monthly composites of vegetation indices (e.g., NDVI, EVI)\nAutomated cloud masking and quality control\nSurface reflectance values normalized for atmospheric effects\nGlobal coverage at 250m-1km resolution since 2000\nIdeal for monitoring seasonal vegetation dynamics\n\n18.1.4.2 Planet Basemap\n\nQuarterly visual composites from multiple PlanetScope satellites\nCloud-free mosaics using best available pixels\nColor-balanced and radiometrically calibrated\nGlobal coverage at ~4.7m resolution\nSuitable for tracking gradual land use changes\n\n18.1.4.3 Harmonized Landsat-Sentinel (HLS)\n\nCombined product using Landsat 8-9 and Sentinel-2 imagery\n2-3 day revisit frequency at 30m resolution\nAtmospherically corrected and co-registered\nConsistent surface reflectance values between sensors\nEnables dense time series from 2013-present\n\nThese products reduce the preprocessing burden for users but may not capture rapid changes that occur between compositing periods. The choice between using raw imagery or pre-processed products depends on the temporal resolution needed for your specific application. These data products and how to use them will be covered in greater detail in Chapter 9 and Chapter 10.\n\n18.1.4.4 Temporal consistency\n\nSensor drift: Over time, satellite sensors can degrade or drift, influencing the consistency of your data. Proper calibration can mitigate these issues.\nMulti-sensor calibration: If you are combining data from multiple satellites in a constellation, ensure that differences in sensor sensitivity or bands do not introduce spurious signals in your time series.\n\n\nVideo\nTime lapse imagery of North Platte, Nebraska. Imagery and visualization from planet Labs Inc.\n\n\n18.1.4.5 Storage and computation\n\nData volume: Each additional time step in your analysis increases the storage and processing requirements.\nTemporal correlation: Many time series phenomena exhibit autocorrelation, which can influence how you design and train models. Standard ML algorithms assume independence between samples, so specialized methods or features (such as lagged features) may be required to handle temporal dependencies.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#approaches-to-time-series-modeling",
    "href": "part-04-models/03-model-temporal.html#approaches-to-time-series-modeling",
    "title": "18  Going over time",
    "section": "\n18.2 Approaches to time series modeling",
    "text": "18.2 Approaches to time series modeling\nBelow are three common strategies for incorporating time series data within a MOSAIKS-like framework. Each approach has trade-offs in terms of complexity, computational cost, and interpretability.\n\n18.2.1 Feature stacking\nIn a feature-stacking approach, you generate MOSAIKS features for multiple time steps and then concatenate them into a single feature vector. This is straightforward but can lead to very large feature spaces if you have many time points.\n\nCompute features for each time period: Run your MOSAIKS feature extraction for each quarter, month, or year—whatever time granularity is relevant.\nConcatenate features: Combine them into a single vector, ensuring that naming conventions keep time steps distinguishable.\nModel training: Input the stacked features into your preferred machine learning algorithm (e.g., linear regression, random forest, or neural network).\n\nExample (quarterly stacking):\n# Create feature names for each quarter\nfeatures_Q1 = [f'X_{i}_Q1' for i in range(1000)]  # X_0_Q1, X_1_Q1, ..., X_999_Q1\nfeatures_Q2 = [f'X_{i}_Q2' for i in range(1000)]  # X_0_Q2, X_1_Q2, ..., X_999_Q2\nfeatures_Q3 = [f'X_{i}_Q3' for i in range(1000)]  # X_0_Q3, X_1_Q3, ..., X_999_Q3\nfeatures_Q4 = [f'X_{i}_Q4' for i in range(1000)]  # X_0_Q4, X_1_Q4, ..., X_999_Q4\n\n# Combine features names for all quarters\nfeatures_annual = features_Q1 + features_Q2 + features_Q3 + features_Q4\n\n# Total length = 4,000 features for four quarters\nfeatures_df = pd.DataFrame(data=features, columns=features_annual)\nThis approach is typically suitable for annual or seasonal data where the number of time steps remains manageable. However, it may be less practical if you have daily or weekly observations over several years.\n\n18.2.2 Temporal aggregation\nIn temporal aggregation, you compute features at a higher frequency but then summarize them over a time window:\n\nExtract features at a high frequency: This provides a rich temporal view.\nAggregate: Compute statistical summaries such as the mean, max, or variance of each feature across the chosen time window. Common windows include daily, weekly, monthly, and quarterly aggregations.\nModel with aggregated features: The aggregated features can represent dynamic processes while controlling the dimensionality of your dataset.\n\nThis method captures general trends and reduces noise from cloud cover or other transient factors. However, important temporal nuances (like specific short-lived events) might be lost in the aggregation.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#demeaning",
    "href": "part-04-models/03-model-temporal.html#demeaning",
    "title": "18  Going over time",
    "section": "\n18.3 Demeaning",
    "text": "18.3 Demeaning\nEvaluation over time\n\n18.3.1 Sequential modeling\nFor phenomena where temporal order is crucial and frequent observations exist, sequential modeling can be more powerful:\n\nFeature extraction: Maintain the time dimension in your feature matrix (e.g., one matrix per location, with time as rows and features as columns).\nApply time series modeling: Techniques like LSTM (Long Short-Term Memory) networks, temporal convolutional networks, or classical state-space models (e.g., ARIMA) can handle temporal dependencies explicitly.\nLagged relationships: Incorporate features from previous time steps to capture delayed effects (e.g., precipitation from last month affecting vegetation today).\n\nAlthough these methods provide a more nuanced view of temporal processes, they require additional modeling expertise and computational resources.\n\n\n\n\n\n\nStart with a simpler approach like feature stacking or temporal aggregation. If you find that your phenomenon has rapid or complex temporal dynamics that aren’t well-captured by these methods, then explore more advanced sequential models.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#hands-on-with-time-series-data",
    "href": "part-04-models/03-model-temporal.html#hands-on-with-time-series-data",
    "title": "18  Going over time",
    "section": "\n18.4 Hands on with time series data",
    "text": "18.4 Hands on with time series data\nIn lieu of time series MOSAIKS features, we will instead demonstrate similar examples using MODIS NDVI data. This will allow us to explore the challenges and opportunities of time series analysis without the need for custom feature extraction.\nNOTE: UPDATE ME with new notebook link.\n\n\n\n\n\n\nClick the badge to run the demo in Google Colab!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓\nRemember to click File -&gt; Save a copy in Drive to save any changes you make.",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#filling-temporal-gaps",
    "href": "part-04-models/03-model-temporal.html#filling-temporal-gaps",
    "title": "18  Going over time",
    "section": "\n18.5 Filling temporal gaps",
    "text": "18.5 Filling temporal gaps\nWhen working with time series data, you may encounter missing values due to cloud cover, sensor issues, or other factors. Here are some common strategies for filling these gaps:",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/03-model-temporal.html#summary",
    "href": "part-04-models/03-model-temporal.html#summary",
    "title": "18  Going over time",
    "section": "\n18.6 Summary",
    "text": "18.6 Summary\nHandling time series in satellite-based ML workflows requires balancing data volume, temporal alignment, and modeling complexity. While feature stacking can be effective for low-frequency or seasonal processes, more sophisticated techniques may be needed to capture high-frequency changes or long-range temporal dependencies. Ultimately, the “best” approach depends on the nature of your target variable, data availability, and the resources at your disposal.\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will look at",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Going over time</span>"
    ]
  },
  {
    "objectID": "part-04-models/04-model-demo.html",
    "href": "part-04-models/04-model-demo.html",
    "title": "19  Build a model",
    "section": "",
    "text": "Ce chapitre est en version préliminaire et peut être incomplet.\n\n\n\n\n\n\n\n\n\n\nClick the badge to run the demonstration!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Remember to click File -&gt; Save a copy in Drive to save any changes you make.\n\nOr to view a static version of the code on GitHub, click the badge below.\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nIn the next chapter we will look at",
    "crumbs": [
      "Task modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Build a model</span>"
    ]
  },
  {
    "objectID": "part-05-responsible/00-responsible.html",
    "href": "part-05-responsible/00-responsible.html",
    "title": "Responsible SIML",
    "section": "",
    "text": "Section outline\nThe following chapters will discuss important aspects of model uncertainty and bias, as well as the ethical considerations in the context of MOSAIKS:\nThese chapters provide",
    "crumbs": [
      "Responsible SIML"
    ]
  },
  {
    "objectID": "part-05-responsible/00-responsible.html#section-outline",
    "href": "part-05-responsible/00-responsible.html#section-outline",
    "title": "Responsible SIML",
    "section": "",
    "text": "Chapter\nKey Topics\n\n\n\n20  Bias, equity, & ethics\nResponsible use, communication, limitations\n\n\n21  Quantifying uncertainty\nError sources, confidence intervals, validation\n\n\n22  Transparency (demo)\nDemonstrating how MOSAIKS can be used to characterize uncertainty\n\n\n\n\n\nTable 1: Outline of the uncertainty section\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will",
    "crumbs": [
      "Responsible SIML"
    ]
  },
  {
    "objectID": "part-05-responsible/01-responsible-ethics.html",
    "href": "part-05-responsible/01-responsible-ethics.html",
    "title": "20  Bias, equity, & ethics",
    "section": "",
    "text": "Ce chapitre est en version préliminaire et peut être incomplet.\n\n\n\n\n\nhttps://www.earthcube.org/fair-training-materials\nhttps://www.fatml.org/\n\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will\n\n\nCould include concepts from this paper: The politics of pixels: A review and agenda for critical remote sensing\nSociopolitical factors impact who collects remotely sensed data, how it is processed, and who benefits from the results. This paper discuses the politics of remote sensing and how it can be used to promote social justice and equity.",
    "crumbs": [
      "Responsible SIML",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Bias, equity, & ethics</span>"
    ]
  },
  {
    "objectID": "part-05-responsible/02-responsible-uncertainty.html",
    "href": "part-05-responsible/02-responsible-uncertainty.html",
    "title": "21  Quantifying uncertainty",
    "section": "",
    "text": "Ce chapitre est en version préliminaire et peut être incomplet.\n\n\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nThe next chapter will",
    "crumbs": [
      "Responsible SIML",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantifying uncertainty</span>"
    ]
  },
  {
    "objectID": "part-05-responsible/03-responsible-demo.html",
    "href": "part-05-responsible/03-responsible-demo.html",
    "title": "22  Transparency (demo)",
    "section": "",
    "text": "Ce chapitre est en version préliminaire et peut être incomplet.\n\n\n\n\n\n\n\n\n\n\nClick the badge to run the demonstration!\n\n\n\n↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ Remember to click File -&gt; Save a copy in Drive to save any changes you make.\n\nOr to view a static version of the code on GitHub, click the badge below.\n\n\n\n\n\n\n\n\nLooking forward\n\n\n\nThe end! The only thing left is references.",
    "crumbs": [
      "Responsible SIML",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Transparency (demo)</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ce chapitre est en version préliminaire et peut être incomplet.",
    "crumbs": [
      "References"
    ]
  }
]